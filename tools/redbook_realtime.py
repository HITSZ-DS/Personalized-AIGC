#!/usr/bin/env python3
"""
å°çº¢ä¹¦å®æ—¶çˆ¬å–è„šæœ¬ V2
é‡‡ç”¨ HTML + Selenium ç­–ç•¥ï¼Œç¡®ä¿ç¬”è®° URL åŒ…å« xsec_tokenï¼Œé˜²æ­¢è¢«å°
æ”¯æŒä¸‹è½½å›¾ç‰‡ã€è§†é¢‘åŠæ–‡æœ¬å†…å®¹
"""

import os
import time
import json
import random
import re
import asyncio
import aiohttp
import requests
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from json5 import loads as json5_loads
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

try:
    import dateutil.parser
except ImportError:
    print("è­¦å‘Š: dateutilæœªå®‰è£…ï¼Œæ—¶é—´è§£æåŠŸèƒ½å¯èƒ½å—é™")
    dateutil = None


# ===================== é…ç½®åŒºåŸŸ =====================

# è·¯å¾„é…ç½®
DATASET_PATH = "dataset/redbook"
REALTIME_DATASET_PATH = "dataset/redbook_realtime"
DOWNLOAD_PATH = "download"
PROGRESS_FILE = "download/redbook/realtime_progress.json"  # æ–­ç‚¹ç»­ä¼ è¿›åº¦æ–‡ä»¶

# Cookieï¼ˆéœ€è¦æ‰‹åŠ¨æ›´æ–°ï¼‰
COOKIE_STR = "abRequestId=a9bac428-fbf4-5b70-ae9d-ae6e82bdf264; a1=19ab03e0dd71s5v6v33vw0myyf60nk2m7pu6v2t3d50000423372; webId=137269e03e381ab4a60bb862e4a8e3a8; gid=yj0D8qdJYdW8yj0D8qd8fUJ9fWyM2hKhqqhE7qf86vviKK288ITJ6h8884JqqWJ82fYyf8Sy; acw_tc=0a00d93117683708775673137eeb31c0233e6cfbe594960a47b13671d228c9; webBuild=5.7.0; websectiga=82e85efc5500b609ac1166aaf086ff8aa4261153a448ef0be5b17417e4512f28; sec_poison_id=60138e5a-9a57-45d5-92f2-67f52cb2a7ed; web_session=040069b77f21a94aaeedce70523b4b48831700; id_token=VjEAANeK14HCDRX4yJALANNegYAiBmeb5Bssr0GYghCCG34SH+W6r4dURI+Iqec8ANlCzWJTHFkzc2eHfHXwEkjRJJWJLkkpnwz8C7KEW+Yc2ehZ5NE6mvWtFzzPtzqcQF9j/t5G; xsecappid=xhs-pc-web; unread={%22ub%22:%226960d8d9000000000a02b5ef%22%2C%22ue%22:%22696461a2000000002103c341%22%2C%22uc%22:34}; loadts=1768372432540"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.xiaohongshu.com/",
    "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
}

# ä¸‹è½½é…ç½®
MAX_IMAGES_PER_NOTE = 5  # æ¯ä¸ªç¬”è®°æœ€å¤šä¸‹è½½å›¾ç‰‡æ•°
MAX_NOTES_PER_USER = 3   # æ¯ä¸ªç”¨æˆ·æœ€å¤šè·å–æ–°ç¬”è®°æ•°
DOWNLOAD_TIMEOUT = 30
REQUEST_DELAY = (2, 4)   # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

# Selenium é…ç½®
USE_SELENIUM = True
HEADLESS = True
MAX_SELENIUM_PAGES = 4   # ä½œè€…ä¸»é¡µæœ€å¤šç¿»é¡µæ•°
SELENIUM_TIMEOUT = 60    # Selenium æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# =====================================================


def sleep_random():
    """éšæœºå»¶è¿Ÿ"""
    time.sleep(random.uniform(*REQUEST_DELAY))


def normalize_title(title: str) -> str:
    """æ ‡å‡†åŒ–æ ‡é¢˜ï¼šå»ç©ºæ ¼ã€è½¬å°å†™"""
    if not title:
        return ""
    return re.sub(r"\s+", "", title).lower().strip()


class RedBookRealtime:
    def __init__(self, cookies: str = None):
        """
        Initialize the RedBookRealtime crawler
        
        Args:
            cookies: Optional cookies for authenticated requests
        """
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        
        # Use provided cookies or default
        cookie_str = cookies or COOKIE_STR
        if cookie_str:
            self._add_cookies(cookie_str)
        
        self.cookies = cookie_str
        
        # Paths configuration
        self.dataset_path = DATASET_PATH
        self.realtime_dataset_path = REALTIME_DATASET_PATH
        self.download_path = DOWNLOAD_PATH
        self.progress_file = PROGRESS_FILE
        
        # Statistics
        self.stats = {
            'users_processed': 0,
            'new_notes_found': 0,
            'notes_downloaded': 0,
            'images_downloaded': 0,
            'videos_downloaded': 0,
            'failed_downloads': 0,
            'skipped_notes': 0  # è·³è¿‡çš„å·²ä¸‹è½½ç¬”è®°æ•°
        }
        
        # æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½è¿›åº¦ï¼ˆæ–°æ ¼å¼ï¼šæŒ‰ç”¨æˆ·ç»„ç»‡ï¼‰
        self.progress = self._load_progress()  # {user_id: {completed, notes: {...}}}
        self.progress_changed = False
        
        # é‡å®šå‘è®¡æ•°å™¨ï¼šç”¨äºæ™ºèƒ½åˆ‡æ¢åˆ°Seleniumæ¨¡å¼
        self.redirect_count = 0
        self.use_selenium_fallback = False
    
    def _add_cookies(self, cookie_str):
        """æ·»åŠ  Cookies"""
        for item in cookie_str.split(';'):
            if '=' in item:
                k, v = item.strip().split('=', 1)
                self.session.cookies.set(k, v)
    
    def _load_progress(self) -> dict:
        """åŠ è½½æ–­ç‚¹ç»­ä¼ è¿›åº¦ï¼ˆæ–°æ ¼å¼ï¼šæŒ‰ç”¨æˆ·ç»„ç»‡ï¼‰"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # æ–°æ ¼å¼ï¼š{user_id: {completed, notes: {note_id: {...}}}}
                    if isinstance(data, dict):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ ¼å¼ï¼ˆåŒ…å«ç”¨æˆ·IDä½œä¸ºkeyï¼‰
                        is_new_format = False
                        for key, value in data.items():
                            if isinstance(value, dict) and 'notes' in value:
                                is_new_format = True
                                break
                        
                        if is_new_format:
                            # æ–°æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                            total_users = len(data)
                            completed_users = sum(1 for u in data.values() if u.get('completed', False))
                            total_notes = sum(len(u.get('notes', {})) for u in data.values())
                            print(f"âœ“ å·²åŠ è½½æ–­ç‚¹ç»­ä¼ è¿›åº¦: {total_users} ä¸ªç”¨æˆ·ï¼Œ{total_notes} ä¸ªç¬”è®°ï¼Œ{completed_users} ä¸ªç”¨æˆ·å·²å®Œæˆ")
                            return data
                        else:
                            # æ—§æ ¼å¼ï¼ˆ{note_id: {user_id, ...}}ï¼‰ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                            print(f"âš ï¸  æ£€æµ‹åˆ°æ—§æ ¼å¼è¿›åº¦æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢...")
                            new_data = {}
                            for note_id, info in data.items():
                                user_id = info.get('user_id', 'unknown')
                                if user_id not in new_data:
                                    new_data[user_id] = {
                                        'completed': False,
                                        'last_update': info.get('download_time', ''),
                                        'timestamp': info.get('timestamp', 0),
                                        'notes': {}
                                    }
                                new_data[user_id]['notes'][note_id] = {
                                    'status': info.get('status', 'success'),
                                    'timestamp': info.get('timestamp', 0),
                                    'download_time': info.get('download_time', '')
                                }
                            print(f"âœ“ å·²è½¬æ¢: {len(new_data)} ä¸ªç”¨æˆ·")
                            return new_data
                    else:
                        print(f"âš ï¸  æ— æ³•è¯†åˆ«çš„è¿›åº¦æ–‡ä»¶æ ¼å¼ï¼Œå°†ä»å¤´å¼€å§‹")
                        return {}
            except Exception as e:
                print(f"âš ï¸  åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†ä»å¤´å¼€å§‹")
                return {}
        else:
            print(f"â„¹ï¸  è¿›åº¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„è¿›åº¦è®°å½•")
            return {}
    
    def _save_progress(self):
        """ä¿å­˜æ–­ç‚¹ç»­ä¼ è¿›åº¦"""
        if not self.progress_changed:
            return
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.progress_file), exist_ok=True)
            
            # ä¿å­˜è¯¦ç»†ä¿¡æ¯ï¼ˆæ–°æ ¼å¼ï¼‰
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, ensure_ascii=False, indent=2)
            
            self.progress_changed = False
            print(f"    âœ“ è¿›åº¦å·²ä¿å­˜åˆ°: {self.progress_file}")
        except Exception as e:
            print(f"    âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def _is_user_completed(self, user_id: str) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²å®Œæˆï¼ˆæ‰¾åˆ°æ‰€æœ‰æœ€æ–°ç¬”è®°ï¼‰"""
        if user_id not in self.progress:
            return False
        return self.progress[user_id].get('completed', False)
    
    def _is_note_downloaded(self, user_id: str, note_id: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šç”¨æˆ·çš„ç¬”è®°æ˜¯å¦å·²ä¸‹è½½"""
        if user_id not in self.progress:
            return False
        notes = self.progress[user_id].get('notes', {})
        return note_id in notes
    
    def _mark_note_downloaded(self, note_id: str, user_id: str, status: str = "success"):
        """æ ‡è®°ç¬”è®°ä¸ºå·²ä¸‹è½½"""
        # ç¡®ä¿ç”¨æˆ·å­˜åœ¨
        if user_id not in self.progress:
            self.progress[user_id] = {
                'completed': False,
                'last_update': time.strftime('%Y-%m-%d %H:%M:%S'),
                'timestamp': int(time.time()),
                'notes': {}
            }
        
        # æ·»åŠ ç¬”è®°
        if note_id not in self.progress[user_id]['notes']:
            self.progress[user_id]['notes'][note_id] = {
                'status': status,
                'timestamp': int(time.time()),
                'download_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            self.progress[user_id]['last_update'] = time.strftime('%Y-%m-%d %H:%M:%S')
            self.progress[user_id]['timestamp'] = int(time.time())
            self.progress_changed = True
    
    def _mark_user_completed(self, user_id: str, completed: bool = True):
        """æ ‡è®°ç”¨æˆ·ä¸ºå·²å®Œæˆï¼ˆå·²æ‰¾åˆ°æ‰€æœ‰æœ€æ–°ç¬”è®°ï¼‰"""
        if user_id not in self.progress:
            self.progress[user_id] = {
                'completed': completed,
                'last_update': time.strftime('%Y-%m-%d %H:%M:%S'),
                'timestamp': int(time.time()),
                'notes': {}
            }
        else:
            self.progress[user_id]['completed'] = completed
            self.progress[user_id]['last_update'] = time.strftime('%Y-%m-%d %H:%M:%S')
            self.progress[user_id]['timestamp'] = int(time.time())
        
        self.progress_changed = True
    
    def clear_progress(self, user_id: str = None):
        """æ¸…ç©ºè¿›åº¦æ–‡ä»¶ï¼ˆç”¨äºé‡æ–°å¼€å§‹ï¼‰
        
        Args:
            user_id: å¦‚æœæŒ‡å®šï¼Œåªæ¸…ç©ºè¯¥ç”¨æˆ·çš„è¿›åº¦ï¼›å¦åˆ™æ¸…ç©ºæ‰€æœ‰è¿›åº¦
        """
        try:
            if user_id:
                # åªæ¸…ç©ºæŒ‡å®šç”¨æˆ·
                if user_id in self.progress:
                    del self.progress[user_id]
                    self.progress_changed = True
                    self._save_progress()
                    print(f"âœ“ å·²æ¸…ç©ºç”¨æˆ· {user_id} çš„è¿›åº¦")
            else:
                # æ¸…ç©ºæ‰€æœ‰
                if os.path.exists(self.progress_file):
                    os.remove(self.progress_file)
                    print(f"âœ“ å·²æ¸…ç©ºè¿›åº¦æ–‡ä»¶: {self.progress_file}")
                self.progress = {}
                self.progress_changed = False
        except Exception as e:
            print(f"âŒ æ¸…ç©ºè¿›åº¦å¤±è´¥: {e}")
    
    def get_progress_stats(self) -> dict:
        """è·å–è¿›åº¦ç»Ÿè®¡ä¿¡æ¯"""
        if not hasattr(self, 'progress') or not self.progress:
            return {'total_users': 0, 'total_notes': 0}
        
        stats = {
            'total_users': len(self.progress),
            'completed_users': 0,
            'incomplete_users': 0,
            'total_notes': 0,
            'by_user': {},
            'by_status': {'success': 0, 'failed': 0}
        }
        
        for user_id, user_data in self.progress.items():
            is_completed = user_data.get('completed', False)
            notes = user_data.get('notes', {})
            note_count = len(notes)
            
            # ç»Ÿè®¡å®ŒæˆçŠ¶æ€
            if is_completed:
                stats['completed_users'] += 1
            else:
                stats['incomplete_users'] += 1
            
            # ç»Ÿè®¡ç¬”è®°æ•°
            stats['total_notes'] += note_count
            stats['by_user'][user_id] = {
                'note_count': note_count,
                'completed': is_completed,
                'last_update': user_data.get('last_update', '')
            }
            
            # ç»Ÿè®¡çŠ¶æ€
            for note_id, note_info in notes.items():
                status = note_info.get('status', 'success')
                if status in stats['by_status']:
                    stats['by_status'][status] += 1
        
        return stats
    
    def __call__(self, user_id: str, save_data: bool = True, download_media: bool = True) -> List[Dict]:
        """
        Get latest notes for a specific user (åŒæ­¥ç‰ˆæœ¬)
        
        Args:
            user_id: The real xiaohongshu user ID
            save_data: Whether to automatically save the fetched data
            download_media: Whether to download the actual media files
        
        Returns:
            List of latest notes (max 3)
        """
        print(f"\n{'='*60}")
        print(f"å¤„ç†ç”¨æˆ·: {user_id}")
        print(f"{'='*60}")
        
        # Get latest timestamp from existing data
        latest_timestamp = self._get_latest_timestamp(user_id)
        
        # Fetch latest notes with HTML + Selenium
        new_notes = self._fetch_latest_notes(user_id, latest_timestamp)
        
        if not new_notes:
            print(f"ç”¨æˆ· {user_id} æ²¡æœ‰æ‰¾åˆ°æ–°ç¬”è®°")
            return []
        
        print(f"\nâœ“ æ‰¾åˆ° {len(new_notes)} ä¸ªæ–°ç¬”è®°")
        
        # Automatically save data if requested
        if save_data and new_notes:
            self._save_realtime_data(new_notes, user_id)
        
        # Download media files if requested
        if download_media and new_notes:
            print(f"\nå¼€å§‹ä¸‹è½½ç”¨æˆ· {user_id} çš„å®æ—¶åª’ä½“æ–‡ä»¶...")
            asyncio.run(self._download_realtime_media(new_notes, user_id))
        
        self.stats['users_processed'] += 1
        self.stats['new_notes_found'] += len(new_notes)
        
        # æ ‡è®°ç”¨æˆ·ä¸ºå·²å®Œæˆï¼ˆå·²æ‰¾åˆ°æ‰€æœ‰æœ€æ–°ç¬”è®°ï¼‰
        self._mark_user_completed(user_id, completed=True)
        self._save_progress()
        print(f"\nâœ“ ç”¨æˆ· {user_id} å·²æ ‡è®°ä¸ºå®Œæˆ")
        
        # é‡ç½®é‡å®šå‘è®¡æ•°å™¨ï¼Œä¸ºä¸‹ä¸€ä¸ªç”¨æˆ·å‡†å¤‡
        self.redirect_count = 0
        self.use_selenium_fallback = False
        
        return new_notes
    
    async def process_user_async(self, user_id: str, save_data: bool = True, download_media: bool = True) -> List[Dict]:
        """
        Get latest notes for a specific user (å¼‚æ­¥ç‰ˆæœ¬ï¼Œç”¨äºå¹¶è¡Œå¤„ç†)
        
        Args:
            user_id: The real xiaohongshu user ID
            save_data: Whether to automatically save the fetched data
            download_media: Whether to download the actual media files
        
        Returns:
            List of latest notes (max 3)
        """
        print(f"\n{'='*60}")
        print(f"[å¼‚æ­¥] å¤„ç†ç”¨æˆ·: {user_id}")
        print(f"{'='*60}")
        
        # Get latest timestamp from existing data (åŒæ­¥æ“ä½œï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ)
        loop = asyncio.get_event_loop()
        latest_timestamp = await loop.run_in_executor(None, self._get_latest_timestamp, user_id)
        
        # Fetch latest notes with HTML + Selenium (åŒæ­¥æ“ä½œï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ)
        new_notes = await loop.run_in_executor(None, self._fetch_latest_notes, user_id, latest_timestamp)
        
        if not new_notes:
            print(f"[å¼‚æ­¥] ç”¨æˆ· {user_id} æ²¡æœ‰æ‰¾åˆ°æ–°ç¬”è®°")
            return []
        
        print(f"\n[å¼‚æ­¥] âœ“ ç”¨æˆ· {user_id} æ‰¾åˆ° {len(new_notes)} ä¸ªæ–°ç¬”è®°")
        
        # Automatically save data if requested
        if save_data and new_notes:
            await loop.run_in_executor(None, self._save_realtime_data, new_notes, user_id)
        
        # Download media files if requested
        if download_media and new_notes:
            print(f"\n[å¼‚æ­¥] å¼€å§‹ä¸‹è½½ç”¨æˆ· {user_id} çš„å®æ—¶åª’ä½“æ–‡ä»¶...")
            await self._download_realtime_media(new_notes, user_id)
        
        self.stats['users_processed'] += 1
        self.stats['new_notes_found'] += len(new_notes)
        
        # æ ‡è®°ç”¨æˆ·ä¸ºå·²å®Œæˆï¼ˆå·²æ‰¾åˆ°æ‰€æœ‰æœ€æ–°ç¬”è®°ï¼‰
        await loop.run_in_executor(None, self._mark_user_completed, user_id, True)
        await loop.run_in_executor(None, self._save_progress)
        print(f"\n[å¼‚æ­¥] âœ“ ç”¨æˆ· {user_id} å·²æ ‡è®°ä¸ºå®Œæˆ")
        
        # é‡ç½®é‡å®šå‘è®¡æ•°å™¨ï¼Œä¸ºä¸‹ä¸€ä¸ªç”¨æˆ·å‡†å¤‡
        self.redirect_count = 0
        self.use_selenium_fallback = False
        
        return new_notes
    
    def _get_latest_timestamp(self, user_id: str) -> int:
        """Get the latest fav_time timestamp from user's existing data"""
        print(f"\nâ†’ æ£€æŸ¥ç”¨æˆ· {user_id} çš„æœ€æ–°æ—¶é—´æˆ³...")
        
        user_dirs = []
        
        # Check main dataset path
        main_user_dir = os.path.join(self.dataset_path, user_id)
        if os.path.exists(main_user_dir):
            user_dirs.append(main_user_dir)
        
        # Check realtime dataset path
        realtime_user_dir = os.path.join(self.realtime_dataset_path, user_id)
        if os.path.exists(realtime_user_dir):
            user_dirs.append(realtime_user_dir)
        
        if not user_dirs:
            print(f"  ç”¨æˆ·æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œå°†è·å–æ‰€æœ‰ç¬”è®°")
            return 0
        
        latest_timestamp = 0
        try:
            for user_dir in user_dirs:
                # Find all CSV files
                csv_files = [f for f in os.listdir(user_dir) if f.endswith('.csv')]
                
                for csv_file in csv_files:
                    csv_path = os.path.join(user_dir, csv_file)
                    try:
                        df = pd.read_csv(csv_path)
                        if 'fav_time' in df.columns and not df.empty:
                            for fav_time in df['fav_time']:
                                if pd.notna(fav_time):
                                    timestamp = self._convert_to_timestamp(fav_time)
                                    if timestamp:
                                        latest_timestamp = max(latest_timestamp, timestamp)
                    except Exception as e:
                        print(f"  è¯»å–CSVå¤±è´¥ {csv_path}: {e}")
                        continue
        
        except Exception as e:
            print(f"  æ‰«æç›®å½•å¤±è´¥: {e}")
        
        print(f"  æœ€æ–°æ—¶é—´æˆ³: {latest_timestamp}")
        if latest_timestamp > 0:
            time_str = datetime.fromtimestamp(latest_timestamp).strftime('%Y-%m-%d %H:%M:%S')
            print(f"  å¯¹åº”æ—¶é—´: {time_str}")
        
        return latest_timestamp
    
    def _convert_to_timestamp(self, time_value) -> int:
        """Convert various time formats to timestamp"""
        if isinstance(time_value, (int, float)):
            return int(time_value)
        
        if isinstance(time_value, str):
            try:
                if time_value.isdigit():
                    return int(time_value)
                
                if dateutil:
                    dt = dateutil.parser.parse(time_value)
                    return int(dt.timestamp())
            except:
                return 0
        
        return 0
    
    # ========== HTML + Selenium çˆ¬å–é€»è¾‘ ==========
    
    def _get_author_profile_url(self, user_id):
        """è·å–ä½œè€…ä¸»é¡µ URLï¼ˆä¸éœ€è¦ tokenï¼‰"""
        return f"https://www.xiaohongshu.com/user/profile/{user_id}"
    
    def parse_notes_from_json(self, json_data):
        """ä» JSON æ•°æ®è§£æç¬”è®°ä¿¡æ¯"""
        notes = []
        if not json_data:
            return notes
        
        data_list = []
        if isinstance(json_data, list):
            if len(json_data) > 0 and isinstance(json_data[0], list):
                data_list = json_data[0]
            else:
                data_list = json_data
        elif isinstance(json_data, dict):
            if "notes" in json_data:
                return self.parse_notes_from_json(json_data["notes"])
            if "data" in json_data and "notes" in json_data["data"]:
                return self.parse_notes_from_json(json_data["data"]["notes"])
        
        for item in data_list:
            if not isinstance(item, dict):
                continue
            
            note_card = item.get("noteCard", {})
            note_id = item.get("id") or note_card.get("noteId")
            title = (note_card.get("displayTitle") or 
                    note_card.get("title") or 
                    item.get("displayTitle") or "")
            
            # æå–ç¬”è®°ç±»å‹
            note_type = note_card.get("type", "")
            
            # æå–äº’åŠ¨æ•°æ®
            interact_info = note_card.get("interactInfo", {})
            liked_count = interact_info.get("likedCount", 0)
            collected_count = interact_info.get("collectedCount", 0)
            comment_count = interact_info.get("commentCount", 0)
            
            # æå–å°é¢
            cover_url = ""
            if "cover" in note_card:
                cover = note_card["cover"]
                if isinstance(cover, dict) and "urlDefault" in cover:
                    cover_url = cover["urlDefault"]
                elif isinstance(cover, str):
                    cover_url = cover
            
            # æå– URLï¼ˆåŒ…å« xsec_tokenï¼‰
            url = None
            
            # æ–¹æ³•1: æ£€æŸ¥ç°æˆçš„ URL å­—æ®µ
            if isinstance(note_card, dict):
                for key in note_card.keys():
                    if 'url' in key.lower() or 'link' in key.lower() or 'href' in key.lower():
                        url_value = note_card[key]
                        if url_value and isinstance(url_value, str):
                            if 'xsec_token' in url_value or '/explore/' in url_value or '/user/profile/' in url_value:
                                url = url_value
                                break
            
            if not url:
                for key in item.keys():
                    if 'url' in key.lower() or 'link' in key.lower() or 'href' in key.lower():
                        url_value = item[key]
                        if url_value and isinstance(url_value, str):
                            if 'xsec_token' in url_value or '/explore/' in url_value or '/user/profile/' in url_value:
                                url = url_value
                                break
            
            # æ–¹æ³•2: ä½¿ç”¨ xsecToken æ„å»ºå®Œæ•´ URL
            if not url and note_id:
                xsec_token = None
                if isinstance(note_card, dict) and 'xsecToken' in note_card:
                    xsec_token = note_card['xsecToken']
                elif 'xsecToken' in item:
                    xsec_token = item['xsecToken']
                
                if xsec_token:
                    url = f"https://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}"
            
            if note_id:
                notes.append({
                    "id": note_id,
                    "title": title,
                    "title_norm": normalize_title(title),
                    "url": url,
                    "type": note_type,
                    "cover_url": cover_url,
                    "liked_count": liked_count,
                    "collected_count": collected_count,
                    "comment_count": comment_count
                })
        
        return notes
    
    def fetch_notes_from_html(self, user_id):
        """æ–¹æ³•1: HTML è¯·æ±‚è·å–ç¬”è®°åˆ—è¡¨"""
        url = self._get_author_profile_url(user_id)
        print(f"  â†’ HTML è¯·æ±‚: {url}")
        
        sleep_random()
        
        try:
            resp = self.session.get(url, timeout=20)
            
            if resp.status_code != 200:
                print(f"     âš ï¸  çŠ¶æ€ç : {resp.status_code}")
                return []
            
            # æå– JSON
            match = re.search(r'"notes":\s*(\[\[.*?\]\])', resp.text, re.DOTALL)
            if match:
                try:
                    raw_json = json5_loads(match.group(1))
                    notes = self.parse_notes_from_json(raw_json)
                    print(f"     âœ“ HTML è§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(notes)} ä¸ªç¬”è®°")
                    return notes
                except Exception as e:
                    print(f"     âš ï¸  JSON è§£æå¤±è´¥: {e}")
            else:
                print(f"     âš ï¸  æœªæ‰¾åˆ° notes JSON æ•°æ®")
            
            return []
        
        except Exception as e:
            print(f"     âŒ HTML è¯·æ±‚å¤±è´¥: {e}")
            return []
    
    def _create_driver(self):
        """åˆ›å»º Selenium WebDriver"""
        options = Options()
        if HEADLESS:
            options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument(f'user-agent={HEADERS["User-Agent"]}')
        
        driver = webdriver.Chrome(options=options)
        driver.set_window_size(1920, 1080)
        driver.implicitly_wait(5)
        driver.set_page_load_timeout(30)
        
        return driver
    
    def _add_cookies_to_driver(self, driver):
        """å‘ WebDriver æ·»åŠ  Cookies"""
        driver.get("https://www.xiaohongshu.com/404")
        time.sleep(1)
        
        for item in self.cookies.split(';'):
            if '=' in item:
                name, value = item.strip().split('=', 1)
                try:
                    driver.add_cookie({
                        'name': name,
                        'value': value,
                        'domain': '.xiaohongshu.com'
                    })
                except:
                    pass
    
    def _extract_notes_from_page(self, driver):
        """ä» Selenium é¡µé¢æå–ç¬”è®°"""
        notes = []
        
        # å°è¯• JSON
        try:
            html = driver.page_source
            match = re.search(r'"notes":\s*(\[\[.*?\]\])', html, re.DOTALL)
            if match:
                raw = json5_loads(match.group(1))
                notes.extend(self.parse_notes_from_json(raw))
        except:
            pass
        
        # DOM è§£æ
        try:
            items = driver.find_elements(By.CSS_SELECTOR, "section.note-item")
            
            if len(items) == 0:
                alt_selectors = [".note-item", "a.cover", "[class*='note']", "[class*='Note']"]
                is_direct_link = False
                for selector in alt_selectors:
                    alt_items = driver.find_elements(By.CSS_SELECTOR, selector)
                    if alt_items:
                        items = alt_items
                        if selector == "a.cover":
                            is_direct_link = True
                        break
            else:
                is_direct_link = False
            
            for item in items:
                try:
                    if is_direct_link:
                        link_elem = item
                        href = item.get_attribute("href")
                    else:
                        link_elem = item.find_element(By.CSS_SELECTOR, "a.cover")
                        href = link_elem.get_attribute("href")
                    
                    if not href:
                        continue
                    
                    # æå– note_id
                    note_id = ""
                    match_profile = re.search(r'/user/profile/[^/]+/([a-z0-9]+)', href)
                    match_explore = re.search(r'/explore/([a-z0-9]+)', href)
                    
                    if match_profile:
                        note_id = match_profile.group(1)
                    elif match_explore:
                        note_id = match_explore.group(1)
                    
                    if not note_id:
                        continue
                    
                    # æå–æ ‡é¢˜
                    title = ""
                    if not is_direct_link:
                        try:
                            title_elem = item.find_element(By.CSS_SELECTOR, ".footer .title span")
                            title = title_elem.text.strip()
                        except:
                            title = link_elem.get_attribute("title") or ""
                    else:
                        title = link_elem.get_attribute("title") or ""
                    
                    # å®Œæ•´URLï¼ˆåŒ…å«xsec_tokenï¼‰
                    full_url = href if href.startswith("http") else "https://www.xiaohongshu.com" + href
                    
                    # è½¬æ¢URLæ ¼å¼ï¼š/user/profile/{user_id}/{note_id}?params â†’ /explore/{note_id}?params
                    if "/user/profile/" in full_url:
                        params = ""
                        if "?" in full_url:
                            params = "?" + full_url.split("?", 1)[1]
                        full_url = f"https://www.xiaohongshu.com/explore/{note_id}{params}"
                    
                    # å»é‡
                    existing_note = None
                    for n in notes:
                        if n['id'] == note_id:
                            existing_note = n
                            break
                    
                    if existing_note:
                        if not existing_note.get('url') and full_url:
                            existing_note['url'] = full_url
                            if title:
                                existing_note['title'] = title
                                existing_note['title_norm'] = normalize_title(title)
                    else:
                        notes.append({
                            "id": note_id,
                            "title": title,
                            "title_norm": normalize_title(title),
                            "url": full_url,
                            "type": "",
                            "cover_url": "",
                            "liked_count": 0,
                            "collected_count": 0,
                            "comment_count": 0
                        })
                
                except Exception as e:
                    continue
        
        except Exception as e:
            pass
        
        return notes
    
    def fetch_notes_with_selenium(self, user_id, max_notes: int = MAX_NOTES_PER_USER * 3):
        """æ–¹æ³•2: Selenium è·å–ç¬”è®°åˆ—è¡¨"""
        start_time = time.time()
        
        print(f"  â†’ Selenium è®¿é—®ä½œè€…ä¸»é¡µ...")
        print(f"     ç›®æ ‡: è·å–æœ€è¿‘ {max_notes} ä¸ªç¬”è®°")
        
        driver = None
        
        try:
            driver = self._create_driver()
            
            if time.time() - start_time > SELENIUM_TIMEOUT:
                print(f"     âš ï¸  å¯åŠ¨è¶…æ—¶")
                return []
            
            self._add_cookies_to_driver(driver)
            
            url = self._get_author_profile_url(user_id)
            print(f"     è®¿é—®: {url}")
            
            try:
                driver.get(url)
            except Exception as e:
                print(f"     âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶: {e}")
                return []
            
            time.sleep(5)
            
            # æ£€æŸ¥é¡µé¢
            current_url = driver.current_url
            if '/login' in current_url or '/404' in current_url:
                print(f"     âš ï¸  é¡µé¢è¢«é‡å®šå‘: {current_url}")
                return []
            
            all_notes = {}
            no_new_count = 0
            
            for page in range(MAX_SELENIUM_PAGES):
                elapsed = time.time() - start_time
                if elapsed > SELENIUM_TIMEOUT:
                    print(f"     âš ï¸  æ€»è¶…æ—¶ ({elapsed:.1f}ç§’)ï¼Œåœæ­¢ç¿»é¡µ")
                    break
                
                print(f"     ç¿»é¡µ {page + 1}/{MAX_SELENIUM_PAGES}...", end=" ", flush=True)
                
                try:
                    current_notes = self._extract_notes_from_page(driver)
                except Exception as e:
                    print(f"æå–å¤±è´¥: {e}")
                    current_notes = []
                
                new_count = 0
                for note in current_notes:
                    if note['id'] not in all_notes:
                        all_notes[note['id']] = note
                        new_count += 1
                
                print(f"æ–°å¢ {new_count} ä¸ª")
                
                # æ£€æŸ¥æ˜¯å¦è¶³å¤Ÿ
                if len(all_notes) >= max_notes:
                    print(f"     âœ“ å·²è·å–è¶³å¤Ÿç¬”è®° ({len(all_notes)} ä¸ª)")
                    break
                
                # è¿ç»­æ— æ–°ç¬”è®°ï¼Œé€€å‡º
                if new_count == 0:
                    no_new_count += 1
                    if no_new_count >= 2:
                        print(f"     â„¹ï¸  è¿ç»­ {no_new_count} é¡µæ— æ–°ç¬”è®°ï¼Œåœæ­¢ç¿»é¡µ")
                        break
                else:
                    no_new_count = 0
                
                # æ»šåŠ¨åŠ è½½
                if page < MAX_SELENIUM_PAGES - 1:
                    try:
                        last_height = driver.execute_script("return document.body.scrollHeight")
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(3)
                        
                        new_height = driver.execute_script("return document.body.scrollHeight")
                        if new_height == last_height:
                            print(f"     å·²åˆ°åº•éƒ¨")
                            break
                    except Exception as e:
                        print(f"     æ»šåŠ¨å¤±è´¥: {e}")
                        break
            
            print(f"     âœ“ Selenium å®Œæˆï¼Œå…±è·å– {len(all_notes)} ä¸ªç¬”è®°")
            
            return list(all_notes.values())
        
        except Exception as e:
            print(f"     âŒ Selenium å¤±è´¥: {e}")
            return []
        
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    
    def _fetch_latest_notes(self, user_id: str, latest_timestamp: int) -> List[Dict]:
        """
        Fetch latest notes that are newer than latest_timestamp
        ä½¿ç”¨ HTML + Selenium ç­–ç•¥ï¼Œç¡®ä¿ URL åŒ…å« xsec_token
        """
        print(f"\nâ†’ è·å–ç”¨æˆ· {user_id} çš„æœ€æ–°ç¬”è®°...")
        
        all_notes = []
        
        # é˜¶æ®µ1: HTML è§£æ
        print(f"\n[é˜¶æ®µ1] HTML è§£æ...")
        html_notes = self.fetch_notes_from_html(user_id)
        
        if html_notes:
            print(f"  âœ“ HTML è·å–åˆ° {len(html_notes)} ä¸ªç¬”è®°")
            
            # æ£€æŸ¥æœ‰å¤šå°‘ç¬”è®°æœ‰å®Œæ•´ URL
            notes_with_url = sum(1 for n in html_notes if n.get('url'))
            print(f"  å…¶ä¸­ {notes_with_url} ä¸ªç¬”è®°æœ‰å®Œæ•´ URLï¼ˆå« tokenï¼‰")
            
            if notes_with_url < len(html_notes):
                print(f"  âš ï¸  {len(html_notes) - notes_with_url} ä¸ªç¬”è®°ç¼ºå°‘ URLï¼Œä½¿ç”¨ Selenium è·å–")
                selenium_notes = self.fetch_notes_with_selenium(user_id)
                
                if selenium_notes:
                    # ç”¨ Selenium çš„ç¬”è®°è¡¥å…… HTML çš„ç¬”è®°
                    selenium_by_id = {n['id']: n for n in selenium_notes if n.get('url')}
                    
                    for i, note in enumerate(html_notes):
                        if not note.get('url') and note['id'] in selenium_by_id:
                            html_notes[i] = selenium_by_id[note['id']]
                    
                    # æ·»åŠ  Selenium ä¸­æ–°å‘ç°çš„ç¬”è®°
                    existing_ids = {n['id'] for n in html_notes}
                    for note in selenium_notes:
                        if note['id'] not in existing_ids and note.get('url'):
                            html_notes.append(note)
            
            all_notes = html_notes
        else:
            # HTML å¤±è´¥ï¼Œç›´æ¥ç”¨ Selenium
            print(f"  âš ï¸  HTML è§£æå¤±è´¥ï¼Œä½¿ç”¨ Selenium")
            selenium_notes = self.fetch_notes_with_selenium(user_id)
            
            if selenium_notes:
                print(f"  âœ“ Selenium è·å–åˆ° {len(selenium_notes)} ä¸ªç¬”è®°")
                all_notes = selenium_notes
            else:
                print(f"  âŒ Selenium ä¹Ÿå¤±è´¥")
                return []
        
        if not all_notes:
            return []
        
        # éªŒè¯ URL
        notes_with_valid_url = []
        for note in all_notes:
            if note.get('url'):
                notes_with_valid_url.append(note)
            else:
                # å¦‚æœæ²¡æœ‰ URLï¼Œå°è¯•æ„å»ºåŸºç¡€ URL
                note['url'] = f"https://www.xiaohongshu.com/explore/{note['id']}"
                print(f"  âš ï¸  ç¬”è®° {note['id']} ç¼ºå°‘ tokenï¼Œä½¿ç”¨åŸºç¡€ URL")
                notes_with_valid_url.append(note)
        
        print(f"\nâ†’ è·å–ç¬”è®°è¯¦ç»†ä¿¡æ¯...")
        
        # ä¸ºæ‰€æœ‰ç¬”è®°è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬åˆ›å»ºæ—¶é—´ï¼‰
        enriched_notes = []
        selenium_driver = None  # Selenium driverï¼Œä»…åœ¨éœ€è¦æ—¶åˆ›å»º
        
        for i, note in enumerate(notes_with_valid_url, 1):
            print(f"  [{i}/{len(notes_with_valid_url)}] {note['title'][:30]}...")
            
            # æ™ºèƒ½é€‰æ‹©ï¼šå¦‚æœHTMLæ¨¡å¼è¿ç»­å¤±è´¥4æ¬¡ï¼Œåˆ‡æ¢åˆ°Seleniumæ¨¡å¼
            if self.use_selenium_fallback:
                if selenium_driver is None:
                    print(f"    ğŸš€ åˆå§‹åŒ–Seleniumæµè§ˆå™¨...")
                    selenium_driver = self._init_driver()
                
                note_detail = self._get_note_detail_with_selenium(selenium_driver, note)
            else:
                # é»˜è®¤ä½¿ç”¨HTMLæ¨¡å¼
                note_detail = self._get_note_detail(note)
            
            if note_detail:
                enriched_notes.append(note_detail)
            
            # é™åˆ¶æ•°é‡ï¼ˆè·å–æ¯”éœ€è¦çš„å¤šä¸€äº›ï¼Œå› ä¸ºæœ‰äº›å¯èƒ½ä¸æ˜¯æ–°çš„ï¼‰
            if len(enriched_notes) >= MAX_NOTES_PER_USER * 2:
                break
            
            # å»¶è¿Ÿ
            if i < len(notes_with_valid_url):
                time.sleep(random.uniform(1, 2))
        
        # æ¸…ç†Selenium driver
        if selenium_driver:
            try:
                selenium_driver.quit()
                print(f"    âœ… å·²å…³é—­Seleniumæµè§ˆå™¨")
            except:
                pass
        
        # ç­›é€‰æ–°ç¬”è®°
        new_notes = []
        for note in enriched_notes:
            note_timestamp = note.get('fav_time', 0)
            if note_timestamp > latest_timestamp:
                new_notes.append(note)
                if len(new_notes) >= MAX_NOTES_PER_USER:
                    break
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        new_notes.sort(key=lambda x: x.get('fav_time', 0), reverse=True)
        
        return new_notes
    
    def _get_note_detail(self, note: Dict) -> Optional[Dict]:
        """è·å–ç¬”è®°è¯¦ç»†ä¿¡æ¯ï¼ˆHTMLæ¨¡å¼ï¼‰"""
        note_url = note.get('url', '')
        note_id = note.get('id', '')
        
        if not note_url:
            return None
        
        sleep_random()
        
        try:
            resp = self.session.get(note_url, timeout=20)
            
            # æ£€æŸ¥é‡å®šå‘
            if '/404' in resp.url or '/login' in resp.url:
                self.redirect_count += 1
                print(f"    âš ï¸  è¢«é‡å®šå‘ (è¿ç»­ç¬¬{self.redirect_count}æ¬¡): {resp.url[:80]}...")
                
                # å½“è¿ç»­é‡å®šå‘è¶…è¿‡4æ¬¡æ—¶ï¼Œè§¦å‘åˆ‡æ¢åˆ°Seleniumæ¨¡å¼
                if self.redirect_count >= 4 and not self.use_selenium_fallback:
                    self.use_selenium_fallback = True
                    print(f"    ğŸ”„ è¿ç»­é‡å®šå‘{self.redirect_count}æ¬¡ï¼Œåˆ‡æ¢åˆ°Seleniumæ¨¡å¼...")
                
                return None
            
            if resp.status_code != 200:
                print(f"    âš ï¸  çŠ¶æ€ç : {resp.status_code}")
                return None
            
            # æˆåŠŸè·å–ï¼Œé‡ç½®é‡å®šå‘è®¡æ•°å™¨
            if self.redirect_count > 0:
                print(f"    âœ… æˆåŠŸè·å–è¯¦æƒ…ï¼Œé‡ç½®é‡å®šå‘è®¡æ•°å™¨")
                self.redirect_count = 0
                self.use_selenium_fallback = False
            
            # è§£æè¯¦æƒ…
            detail = self._parse_note_detail(resp.text, note)
            return detail
        
        except Exception as e:
            print(f"    âŒ è·å–è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _get_note_detail_with_selenium(self, driver, note: Dict) -> Optional[Dict]:
        """ä½¿ç”¨Seleniumè·å–ç¬”è®°è¯¦ç»†ä¿¡æ¯"""
        note_url = note.get('url', '')
        note_id = note.get('id', '')
        
        if not note_url:
            return None
        
        try:
            print(f"    ğŸŒ ä½¿ç”¨Seleniumè®¿é—®: {note_url[:60]}...")
            driver.get(note_url)
            time.sleep(random.uniform(3, 5))
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘
            current_url = driver.current_url
            if '/login' in current_url or '/404' in current_url:
                print(f"    âš ï¸  Seleniumä¹Ÿè¢«é‡å®šå‘: {current_url[:80]}...")
                return None
            
            # è·å–é¡µé¢HTML
            html = driver.page_source
            
            # è§£æè¯¦æƒ…
            detail = self._parse_note_detail(html, note)
            
            # æˆåŠŸè·å–ï¼Œé‡ç½®è®¡æ•°å™¨
            if detail:
                self.redirect_count = 0
                print(f"    âœ… SeleniumæˆåŠŸè·å–è¯¦æƒ…")
            
            return detail
        
        except Exception as e:
            print(f"    âŒ Seleniumè·å–è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _parse_note_detail(self, html: str, base_note: Dict) -> Dict:
        """è§£æç¬”è®°è¯¦æƒ…é¡µ"""
        soup = BeautifulSoup(html, "html.parser")
        
        # æå–åˆ›å»ºæ—¶é—´
        create_time = ""
        timestamp = int(time.time())
        
        # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„ä½ç½®æå–æ—¶é—´
        time_patterns = [
            r'"time":\s*"([^"]+)"',
            r'"createTime":\s*"([^"]+)"',
            r'"publishTime":\s*"([^"]+)"',
            r'"updateTime":\s*(\d+)',
            r'å‘å¸ƒäº\s*(\d{4}-\d{2}-\d{2})',
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, html)
            if match:
                time_str = match.group(1)
                try:
                    if time_str.isdigit():
                        timestamp = int(time_str)
                        if timestamp > 10000000000:  # æ¯«ç§’è½¬ç§’
                            timestamp = timestamp // 1000
                    elif dateutil:
                        dt = dateutil.parser.parse(time_str)
                        timestamp = int(dt.timestamp())
                    create_time = time_str
                    break
                except:
                    continue
        
        # æå–æè¿°/å†…å®¹
        description = ""
        
        # æ–¹æ³•1: meta description
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc:
            description = meta_desc.get("content", "")
        
        # æ–¹æ³•2: ä» JSON ä¸­æå–
        if not description:
            desc_match = re.search(r'"desc":\s*"([^"]+)"', html)
            if desc_match:
                description = desc_match.group(1)
        
        # æå–å›¾ç‰‡URLs
        image_urls = self._extract_images_from_note_page(html)
        
        # æå–è§†é¢‘URLï¼ˆå°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µï¼‰
        video_url = ""
        video_patterns = [
            r'"videoUrl":\s*"([^"]+)"',
            r'"video":\s*{\s*"[^"]*url[^"]*":\s*"([^"]+)"',
            r'"streamUrl":\s*"([^"]+)"',
            r'"playUrl":\s*"([^"]+)"',
            r'"originVideoKey":\s*"([^"]+)"'
        ]
        
        for pattern in video_patterns:
            video_match = re.search(pattern, html)
            if video_match:
                video_url = video_match.group(1)
                # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„è§†é¢‘ URL
                if video_url and ('video' in video_url.lower() or 'stream' in video_url.lower()):
                    break
        
        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä» JSON æ•°æ®ä¸­é€’å½’æŸ¥æ‰¾
        if not video_url:
            video_url = self._find_video_in_html(html)
        
        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»HTMLä¸­ç›´æ¥æœç´¢å®Œæ•´çš„è§†é¢‘URL
        if not video_url:
            # æœç´¢å¸¸è§çš„è§†é¢‘URLæ ¼å¼
            video_url_patterns = [
                r'(https?://[^"\s]*sns-video[^"\s]*\.mp4)',
                r'(https?://[^"\s]*stream[^"\s]*\.mp4)',
                r'(https?://[^"\s]*video[^"\s]*\.mp4)',
            ]
            for pattern in video_url_patterns:
                video_match = re.search(pattern, html)
                if video_match:
                    video_url = video_match.group(1)
                    break
        
        # æ„å»ºå®Œæ•´ç¬”è®°ä¿¡æ¯
        return {
            'redbookID': base_note.get('id', ''),
            'title': base_note.get('title', ''),
            'note_url': base_note.get('url', ''),
            'content': description,
            'fav_time': timestamp,
            'user_id': '',  # ä»URLæˆ–å…¶ä»–åœ°æ–¹æå–
            'type': base_note.get('type', ''),
            'user_nickname': '',
            'liked_count': base_note.get('liked_count', 0),
            'comment_count': base_note.get('comment_count', 0),
            'collect_count': base_note.get('collected_count', 0),
            'tags': '',
            'cover_url': base_note.get('cover_url', ''),
            'images': '|'.join(image_urls),
            'video_url': video_url,
            'xsec_token': self._extract_xsec_token(base_note.get('url', '')),
            'create_time': create_time,
            'timestamp': int(time.time())
        }
    
    def _extract_xsec_token(self, url: str) -> str:
        """ä» URL ä¸­æå– xsec_token"""
        if not url:
            return ""
        
        match = re.search(r'xsec_token=([^&]+)', url)
        if match:
            return match.group(1)
        
        return ""
    
    def _extract_images_from_note_page(self, html: str) -> List[str]:
        """ä»ç¬”è®°è¯¦æƒ…é¡µæå–å›¾ç‰‡URLï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        soup = BeautifulSoup(html, "html.parser")
        image_urls = []
        
        # æ–¹æ³•1: ä» script æ ‡ç­¾ä¸­çš„ JSON æå–
        script_images = self._extract_images_from_script_tags(html)
        image_urls.extend(script_images)
        
        # æ–¹æ³•2: og:image meta æ ‡ç­¾
        for meta in soup.find_all("meta"):
            name_attr = meta.get("name")
            prop_attr = meta.get("property")
            if name_attr == "og:image" or prop_attr == "og:image":
                content = meta.get("content")
                if content and self._is_valid_note_image(content):
                    if content not in image_urls:
                        image_urls.append(content)
        
        # æ–¹æ³•3: ä»æ•´ä¸ª HTML ä¸­æ­£åˆ™æœç´¢
        try:
            image_pattern = r'"(https?://[^"]*xhscdn\.com[^"]*)"'
            matches = re.findall(image_pattern, html)
            for url in matches:
                if self._is_valid_note_image(url) and url not in image_urls:
                    image_urls.append(url)
        except:
            pass
        
        # å¤„ç†URLå¹¶å»é‡ï¼Œæœ€åå†è¿‡æ»¤ä¸€æ¬¡ç¡®ä¿æ²¡æœ‰è§†é¢‘
        unique_urls = []
        seen = set()
        for url in image_urls:
            if url.startswith("//"):
                url = "https:" + url
            # æœ€ç»ˆè¿‡æ»¤ï¼šæ’é™¤è§†é¢‘URL
            if any(keyword in url.lower() for keyword in ['video', 'stream', '.mp4', '.m3u8']):
                continue
            if url not in seen:
                unique_urls.append(url)
                seen.add(url)
        
        return unique_urls
    
    def _extract_images_from_script_tags(self, html: str) -> List[str]:
        """ä» JavaScript æ•°æ®ä¸­æå–å›¾ç‰‡"""
        images = []
        
        patterns = [
            r'"imageList":\s*(\[.*?\])',
            r'"images":\s*(\[.*?\])',
            r'window\.__INITIAL_STATE__\s*=\s*({.*?})</script>'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html, re.DOTALL)
            for match in matches:
                try:
                    data = json.loads(match)
                    found_images = self._find_images_in_json_data(data)
                    images.extend(found_images)
                except:
                    continue
        
        return images
    
    def _find_images_in_json_data(self, data) -> List[str]:
        """é€’å½’æŸ¥æ‰¾ JSON æ•°æ®ä¸­çš„å›¾ç‰‡ URL"""
        images = []
        
        if isinstance(data, dict):
            for key, value in data.items():
                if key.lower() in ['url', 'image', 'cover', 'pic', 'urldefault', 'traceurl'] and isinstance(value, str):
                    if value.startswith('http') and 'xhscdn.com' in value and self._is_valid_note_image(value):
                        images.append(value)
                elif isinstance(value, (dict, list)):
                    images.extend(self._find_images_in_json_data(value))
        elif isinstance(data, list):
            for item in data:
                images.extend(self._find_images_in_json_data(item))
        
        return images
    
    def _find_video_in_html(self, html: str) -> str:
        """ä» HTML ä¸­æŸ¥æ‰¾è§†é¢‘ URL"""
        # å°è¯•ä» JSON æ•°æ®ä¸­é€’å½’æŸ¥æ‰¾
        try:
            # æŸ¥æ‰¾å¯èƒ½åŒ…å«è§†é¢‘æ•°æ®çš„ JSON
            json_pattern = r'window\.__INITIAL_STATE__\s*=\s*({.*?})</script>'
            match = re.search(json_pattern, html, re.DOTALL)
            if match:
                data = json.loads(match.group(1))
                video_url = self._find_video_in_json_data(data)
                if video_url:
                    return video_url
        except:
            pass
        
        return ""
    
    def _find_video_in_json_data(self, data) -> str:
        """é€’å½’æŸ¥æ‰¾ JSON æ•°æ®ä¸­çš„è§†é¢‘ URL"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key.lower() in ['videourl', 'streamurl', 'playurl', 'url', 'video', 'mp4url', 'h264url'] and isinstance(value, str):
                    # éªŒè¯æ˜¯è§†é¢‘URL
                    if value.startswith('http') and (
                        'video' in value.lower() or 
                        'stream' in value.lower() or 
                        '.mp4' in value.lower() or
                        '.m3u8' in value.lower()
                    ):
                        return value
                elif isinstance(value, (dict, list)):
                    result = self._find_video_in_json_data(value)
                    if result:
                        return result
        elif isinstance(data, list):
            for item in data:
                result = self._find_video_in_json_data(item)
                if result:
                    return result
        
        return ""
    
    def _is_valid_note_image(self, url: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ç¬”è®°å›¾ç‰‡"""
        if not url or url.startswith("data:image"):
            return False
        
        # æ’é™¤è§†é¢‘URL
        if any(keyword in url.lower() for keyword in ['video', 'stream', '.mp4', '.m3u8']):
            return False
        
        # æ’é™¤JS/CSSæ–‡ä»¶
        if any(ext in url.lower() for ext in ['.js', '.css', '.json']):
            return False
        
        # æ’é™¤çº¯åŸŸåï¼ˆæ²¡æœ‰è·¯å¾„çš„URLï¼‰
        if url.count('/') <= 3:  # https://domain.com/ åªæœ‰3ä¸ªæ–œæ 
            return False
        
        blacklist = ["logo", "icon", "avatar", "favicon", "default", "placeholder"]
        if any(b in url.lower() for b in blacklist):
            return False
        
        if any(p in url for p in ["/fe-platform/", "/fe-static/", "/static/", "/as/v1/", "/formula-static/"]):
            return False
        
        if "xhscdn.com" not in url:
            return False
        
        # ç¡®ä¿æ˜¯å›¾ç‰‡ç›¸å…³çš„åŸŸåæˆ–è·¯å¾„
        valid_patterns = ['webpic', 'image', '.jpg', '.png', '.webp', '.jpeg', 'nd_dft', 'nd_prv', 'nc_n']
        if not any(pattern in url.lower() for pattern in valid_patterns):
            return False
        
        return True
    
    # ========== æ•°æ®ä¿å­˜ ==========
    
    def _save_realtime_data(self, notes: List[Dict], user_id: str):
        """ä¿å­˜å®æ—¶æ•°æ®ä¸º CSV"""
        if not notes:
            return
        
        timestamp = int(time.time())
        user_dir = os.path.join(self.realtime_dataset_path, user_id)
        os.makedirs(user_dir, exist_ok=True)
        
        csv_filename = f"realtime_notes_{timestamp}.csv"
        csv_filepath = os.path.join(user_dir, csv_filename)
        
        self._save_as_csv(notes, csv_filepath)
        
        print(f"\nâœ“ æ•°æ®å·²ä¿å­˜: {csv_filepath}")
        print(f"  ç¬”è®°æ•°: {len(notes)}")
    
    def _save_as_csv(self, notes: List[Dict], filepath: str):
        """ä¿å­˜ä¸º CSV æ–‡ä»¶"""
        if not notes:
            return
        
        import csv
        
        fieldnames = [
            'åºå·', 'redbookID', 'title', 'ä½œè€…', 'ä½œè€…ID',
            'ç‚¹èµæ•°', 'è¯„è®ºæ•°', 'æ”¶è—æ•°', 'content', 'tag', 'å°é¢URL',
            'images', 'æœ¬åœ°å›¾ç‰‡è·¯å¾„åˆ—è¡¨', 'videos', 'æœ¬åœ°è§†é¢‘è·¯å¾„',
            'ç¬”è®°URL', 'xsec_token', 'fav_time', 'é‡‡é›†æ—¶é—´'
        ]
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for i, note in enumerate(notes, 1):
                csv_row = {
                    'åºå·': i,
                    'redbookID': note.get('redbookID', ''),
                    'title': note.get('title', ''),
                    'ä½œè€…': note.get('user_nickname', ''),
                    'ä½œè€…ID': note.get('user_id', ''),
                    'ç‚¹èµæ•°': note.get('liked_count', 0),
                    'è¯„è®ºæ•°': note.get('comment_count', 0),
                    'æ”¶è—æ•°': note.get('collect_count', 0),
                    'content': note.get('content', ''),
                    'tag': note.get('tags', ''),
                    'å°é¢URL': note.get('cover_url', ''),
                    'images': note.get('images', ''),
                    'æœ¬åœ°å›¾ç‰‡è·¯å¾„åˆ—è¡¨': '',
                    'videos': note.get('video_url', ''),
                    'æœ¬åœ°è§†é¢‘è·¯å¾„': '',
                    'ç¬”è®°URL': note.get('note_url', ''),
                    'xsec_token': note.get('xsec_token', ''),
                    'fav_time': note.get('fav_time', ''),
                    'é‡‡é›†æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(note.get('timestamp', time.time())))
                }
                writer.writerow(csv_row)
    
    # ========== åª’ä½“ä¸‹è½½ ==========
    
    async def _download_realtime_media(self, notes: List[Dict], user_id: str):
        """ä¸‹è½½å®æ—¶åª’ä½“æ–‡ä»¶ï¼ˆå¸¦æ–­ç‚¹ç»­ä¼ ï¼‰"""
        if not notes:
            return
        
        realtime_dir = os.path.join(self.download_path, "redbook", user_id, "realtime")
        os.makedirs(realtime_dir, exist_ok=True)
        
        print(f"\nä¸‹è½½ç›®å½•: {realtime_dir}")
        
        for i, note in enumerate(notes, 1):
            note_id = note.get('redbookID', f'note_{i}')
            note_title = note.get('title', 'æ— æ ‡é¢˜')
            
            # æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
            if self._is_note_downloaded(user_id, note_id):
                print(f"\n[{i}/{len(notes)}] â­ï¸  è·³è¿‡å·²ä¸‹è½½ç¬”è®°: {note_title[:50]}... (ID: {note_id})")
                self.stats['skipped_notes'] += 1
                continue
            
            print(f"\n[{i}/{len(notes)}] å¤„ç†ç¬”è®°: {note_title[:50]}...")
            
            note_folder = os.path.join(realtime_dir, note_id)
            os.makedirs(note_folder, exist_ok=True)
            
            try:
                # ä¿å­˜æ–‡æœ¬
                self._save_text_content(note_title, note.get('content', ''), note_folder)
                
                # ä¸‹è½½åª’ä½“
                await self._download_note_media(note, note_folder)
                
                # æ ‡è®°ä¸ºå·²ä¸‹è½½
                self._mark_note_downloaded(note_id, user_id, status="success")
                self._save_progress()
                
                self.stats['notes_downloaded'] += 1
                print(f"  âœ… ç¬”è®°å¤„ç†å®Œæˆï¼ˆå·²ä¿å­˜è¿›åº¦ï¼‰")
            
            except Exception as e:
                print(f"  âŒ ç¬”è®°å¤„ç†å¤±è´¥: {e}")
                # æ ‡è®°ä¸ºå¤±è´¥ï¼ˆä½†ä»ç„¶è®°å½•ï¼Œé¿å…é‡å¤å°è¯•ï¼‰
                self._mark_note_downloaded(note_id, user_id, status="failed")
                self._save_progress()
                self.stats['failed_downloads'] += 1
            
            if i < len(notes):
                await asyncio.sleep(random.uniform(1, 2))
    
    def _save_text_content(self, title: str, content: str, save_path: str) -> bool:
        """ä¿å­˜æ–‡æœ¬å†…å®¹"""
        try:
            content_clean = re.sub(r'#\w+', '', content)
            content_clean = re.sub(r'\s+', ' ', content_clean).strip()
            
            if not content_clean:
                content_clean = 'No content'
            
            clean_title = self._sanitize_filename(title)
            filename = f"{clean_title}.txt"
            filepath = os.path.join(save_path, filename)
            
            if len(content_clean) > 2000:
                content_clean = content_clean[:2000]
            
            full_content = f"Title: {title}\n\nContent: {content_clean}"
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(full_content)
            
            print(f"    âœ… æ–‡æœ¬ä¿å­˜æˆåŠŸ")
            return True
        
        except Exception as e:
            print(f"    âš ï¸  æ–‡æœ¬ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        if not filename or filename.strip() == '':
            return "æ— æ ‡é¢˜"
        
        filename = re.sub(r'[\uFEFF\u200B-\u200D\uFFFC]', '', filename)
        invalid_chars = r'<>:"/\\|?*#\r\n\t'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        
        filename = re.sub(r'_+', '_', filename)
        filename = filename.strip('_').strip()
        filename = re.sub(r'\.{2,}', '_', filename)
        
        return filename[:100] if filename else "æ— æ ‡é¢˜"
    
    async def _download_note_media(self, note: Dict, note_folder: str):
        """ä¸‹è½½ç¬”è®°çš„åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡å’Œè§†é¢‘ï¼‰"""
        video_url = note.get('video_url', '')
        
        # ä¸‹è½½å›¾ç‰‡
        image_urls_str = note.get('images', '')
        if image_urls_str:
            image_urls = image_urls_str.split('|')
            image_urls = [url for url in image_urls if url]
            
            if image_urls:
                # å¦‚æœæœ‰è§†é¢‘ï¼Œåªä¸‹è½½ç¬¬ä¸€å¼ å°é¢å›¾ï¼ˆé«˜æ¸…ï¼‰
                if video_url:
                    print(f"    ä¸‹è½½è§†é¢‘å°é¢ï¼ˆ1å¼ ï¼‰...")
                    await self._download_images([image_urls[0]], note_folder)
                else:
                    # æ²¡æœ‰è§†é¢‘ï¼Œä¸‹è½½æ‰€æœ‰å›¾ç‰‡
                    print(f"    ä¸‹è½½ {len(image_urls)} å¼ å›¾ç‰‡...")
                    await self._download_images(image_urls, note_folder)
        
        # ä¸‹è½½è§†é¢‘
        if video_url:
            print(f"    ä¸‹è½½è§†é¢‘...")
            await self._download_video(video_url, note_folder)
    
    async def _download_images(self, image_urls: List[str], save_path: str):
        """å¼‚æ­¥ä¸‹è½½å›¾ç‰‡ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰"""
        # é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¿æ¥è¿‡å¤š
        semaphore = asyncio.Semaphore(3)  # æœ€å¤šåŒæ—¶ä¸‹è½½3å¼ 
        
        async def download_with_limit(session, url, filepath, index):
            async with semaphore:
                return await self._download_file_async(session, url, filepath, "image")
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for i, img_url in enumerate(image_urls[:MAX_IMAGES_PER_NOTE]):
                ext = self._get_file_extension(img_url)
                filename = f"image_{i}{ext}"
                filepath = os.path.join(save_path, filename)
                
                task = download_with_limit(session, img_url, filepath, i)
                tasks.append(task)
            
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                success_count = sum(1 for r in results if r is True)
                print(f"      âœ“ æˆåŠŸä¸‹è½½ {success_count}/{len(tasks)} å¼ å›¾ç‰‡")
                self.stats['images_downloaded'] += success_count
    
    async def _download_video(self, video_url: str, save_path: str):
        """å¼‚æ­¥ä¸‹è½½è§†é¢‘"""
        async with aiohttp.ClientSession() as session:
            filename = "video_0.mp4"
            filepath = os.path.join(save_path, filename)
            
            success = await self._download_file_async(session, video_url, filepath, "video")
            if success:
                print(f"      âœ“ è§†é¢‘ä¸‹è½½æˆåŠŸ")
                self.stats['videos_downloaded'] += 1
            else:
                print(f"      âœ— è§†é¢‘ä¸‹è½½å¤±è´¥")
    
    async def _download_file_async(self, session: aiohttp.ClientSession, url: str, save_path: str, file_type: str = "file", max_retries: int = 3) -> bool:
        """å¼‚æ­¥ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        if os.path.exists(save_path):
            # æ£€æŸ¥å·²å­˜åœ¨æ–‡ä»¶çš„å¤§å°
            file_size = os.path.getsize(save_path)
            if file_size >= 1000:  # å¦‚æœæ–‡ä»¶æœ‰æ•ˆï¼Œè·³è¿‡
                return False
            else:  # å¦‚æœæ–‡ä»¶å¤ªå°ï¼Œåˆ é™¤å¹¶é‡æ–°ä¸‹è½½
                os.remove(save_path)
        
        for attempt in range(max_retries):
            try:
                await asyncio.sleep(random.uniform(0.5, 1.0))
                
                headers = {
                    'User-Agent': HEADERS['User-Agent'],
                    'Referer': 'https://www.xiaohongshu.com/',
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8' if file_type == "image" else '*/*',
                }
                
                async with session.get(url, headers=headers, timeout=DOWNLOAD_TIMEOUT) as resp:
                    if resp.status == 200:
                        content = await resp.read()
                        
                        # éªŒè¯æ–‡ä»¶å¤§å°
                        content_size = len(content)
                        if content_size < 1000:
                            if attempt < max_retries - 1:
                                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                                continue
                            return False
                        
                        # ä¿å­˜æ–‡ä»¶
                        os.makedirs(os.path.dirname(save_path), exist_ok=True)
                        with open(save_path, 'wb') as f:
                            f.write(content)
                        
                        # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼ˆå›¾ç‰‡ï¼‰
                        if file_type == "image":
                            if not self._verify_image(save_path):
                                os.remove(save_path)
                                if attempt < max_retries - 1:
                                    await asyncio.sleep(2 ** attempt)
                                    continue
                                return False
                        
                        return True
                    else:
                        # é200çŠ¶æ€ç ï¼Œé‡è¯•
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                            continue
            
            except asyncio.TimeoutError:
                # è¶…æ—¶ï¼Œé‡è¯•
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
            
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸ï¼Œé‡è¯•
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
        
        return False
    
    def _verify_image(self, filepath: str) -> bool:
        """éªŒè¯å›¾ç‰‡æ–‡ä»¶å®Œæ•´æ€§"""
        try:
            from PIL import Image
            with Image.open(filepath) as img:
                img.verify()
            return True
        except:
            # å¦‚æœ PIL ä¸å¯ç”¨æˆ–éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ–‡ä»¶å¤´æ£€æŸ¥
            try:
                with open(filepath, 'rb') as f:
                    header = f.read(12)
                    # æ£€æŸ¥å¸¸è§å›¾ç‰‡æ ¼å¼çš„é­”æ•°
                    if header[:3] == b'\xff\xd8\xff':  # JPEG
                        return True
                    elif header[:8] == b'\x89PNG\r\n\x1a\n':  # PNG
                        return True
                    elif header[:6] in (b'GIF87a', b'GIF89a'):  # GIF
                        return True
                    elif header[:4] == b'RIFF' and header[8:12] == b'WEBP':  # WEBP
                        return True
                return False
            except:
                return False
    
    def _get_file_extension(self, url: str) -> str:
        """è·å–æ–‡ä»¶æ‰©å±•å"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        path = parsed.path
        
        if '.' in path:
            ext = '.' + path.split('.')[-1].lower()
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:
                return ext
        
        return '.jpg'
    
    # ========== æ‰¹é‡å¤„ç† ==========
    
    def get_all_download_users(self) -> List[str]:
        """è·å–æ‰€æœ‰ä¸‹è½½ç›®å½•ä¸­çš„ç”¨æˆ·ID"""
        redbook_download_dir = os.path.join(self.download_path, "redbook")
        
        if not os.path.exists(redbook_download_dir):
            return []
        
        user_dirs = []
        try:
            for item in os.listdir(redbook_download_dir):
                item_path = os.path.join(redbook_download_dir, item)
                if os.path.isdir(item_path):
                    user_dirs.append(item)
            
            return user_dirs
        
        except Exception as e:
            print(f"æ‰«æä¸‹è½½ç›®å½•å¤±è´¥: {e}")
            return []
    
    def process_all_users(self, save_data: bool = True, download_media: bool = True, max_users: int = None, parallel: int = 3) -> Dict[str, List[Dict]]:
        """
        å¤„ç†æ‰€æœ‰ç”¨æˆ·çš„å®æ—¶æ•°æ®
        
        Args:
            save_data: æ˜¯å¦ä¿å­˜æ•°æ®
            download_media: æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶
            max_users: æœ€å¤šå¤„ç†ç”¨æˆ·æ•°ï¼ˆé™åˆ¶çˆ¬å–çš„ç”¨æˆ·æ•°é‡ï¼‰
            parallel: å¹¶å‘æ•°é‡ï¼ˆé»˜è®¤3ï¼ŒåŒæ—¶å¤„ç†3ä¸ªç”¨æˆ·ï¼‰
        """
        if parallel > 1:
            # ä½¿ç”¨å¼‚æ­¥å¹¶è¡Œå¤„ç†
            return asyncio.run(self._process_all_users_async(save_data, download_media, max_users, parallel))
        else:
            # ä½¿ç”¨åŸæœ‰çš„ä¸²è¡Œå¤„ç†
            return self._process_all_users_sync(save_data, download_media, max_users)
    
    def _process_all_users_sync(self, save_data: bool = True, download_media: bool = True, max_users: int = None) -> Dict[str, List[Dict]]:
        """ä¸²è¡Œå¤„ç†æ‰€æœ‰ç”¨æˆ·ï¼ˆåŸé€»è¾‘ï¼‰"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†ç”¨æˆ·å®æ—¶æ•°æ®ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰...")
        print("="*60)
        
        download_users = self.get_all_download_users()
        
        if not download_users:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç”¨æˆ·")
            return {}
        
        if max_users and len(download_users) > max_users:
            download_users = download_users[:max_users]
        
        print(f"å‡†å¤‡å¤„ç† {len(download_users)} ä¸ªç”¨æˆ·\n")
        
        results = {}
        
        for i, user_id in enumerate(download_users, 1):
            print(f"\n{'='*60}")
            print(f"[{i}/{len(download_users)}] å¤„ç†ç”¨æˆ·: {user_id}")
            print(f"{'='*60}")
            
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²å®Œæˆï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            if self._is_user_completed(user_id):
                print(f"â­ï¸  ç”¨æˆ· {user_id} å·²å®Œæˆï¼Œè·³è¿‡")
                results[user_id] = []
                continue
            
            try:
                new_notes = self(user_id, save_data=save_data, download_media=download_media)
                results[user_id] = new_notes
                
                if new_notes:
                    print(f"\nâœ… ç”¨æˆ· {user_id} å®Œæˆ: {len(new_notes)} ä¸ªæ–°ç¬”è®°")
                else:
                    print(f"\nğŸ“ ç”¨æˆ· {user_id} æ²¡æœ‰æ–°ç¬”è®°")
                
                # å»¶è¿Ÿ
                if i < len(download_users):
                    delay = random.uniform(4, 5)
                    print(f"\nâ³ ç­‰å¾… {delay:.1f} ç§’...")
                    time.sleep(delay)
            
            except Exception as e:
                print(f"\nâŒ å¤„ç†ç”¨æˆ· {user_id} å¤±è´¥: {e}")
                results[user_id] = []
                continue
        
        # æ˜¾ç¤ºç»Ÿè®¡
        self._show_stats()
        
        return results
    
    async def _process_all_users_async(self, save_data: bool = True, download_media: bool = True, max_users: int = None, parallel: int = 2) -> Dict[str, List[Dict]]:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰ç”¨æˆ·"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†ç”¨æˆ·å®æ—¶æ•°æ®ï¼ˆå¹¶è¡Œæ¨¡å¼ï¼š{parallel} ä¸ªå¹¶å‘ï¼‰...")
        print("="*60)
        
        download_users = self.get_all_download_users()
        
        if not download_users:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç”¨æˆ·")
            return {}
        
        if max_users and len(download_users) > max_users:
            download_users = download_users[:max_users]
        
        print(f"å‡†å¤‡å¤„ç† {len(download_users)} ä¸ªç”¨æˆ·\n")
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(parallel)
        results = {}
        
        async def process_with_limit(user_id: str, index: int, total: int):
            """å¸¦å¹¶å‘é™åˆ¶çš„ç”¨æˆ·å¤„ç†"""
            async with semaphore:
                print(f"\n{'='*60}")
                print(f"[{index}/{total}] ğŸ”„ å¼€å§‹å¤„ç†ç”¨æˆ·: {user_id}")
                print(f"{'='*60}")
                
                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²å®Œæˆï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                if self._is_user_completed(user_id):
                    print(f"â­ï¸  ç”¨æˆ· {user_id} å·²å®Œæˆï¼Œè·³è¿‡")
                    return []
                
                try:
                    new_notes = await self.process_user_async(user_id, save_data=save_data, download_media=download_media)
                    
                    if new_notes:
                        print(f"\nâœ… [{index}/{total}] ç”¨æˆ· {user_id} å®Œæˆ: {len(new_notes)} ä¸ªæ–°ç¬”è®°")
                    else:
                        print(f"\nğŸ“ [{index}/{total}] ç”¨æˆ· {user_id} æ²¡æœ‰æ–°ç¬”è®°")
                    
                    return user_id, new_notes
                
                except Exception as e:
                    print(f"\nâŒ [{index}/{total}] å¤„ç†ç”¨æˆ· {user_id} å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    return user_id, []
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            process_with_limit(user_id, i+1, len(download_users))
            for i, user_id in enumerate(download_users)
        ]
        
        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(tasks)} ä¸ªç”¨æˆ·ï¼ˆå¹¶å‘æ•°: {parallel}ï¼‰...\n")
        
        completed_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ•´ç†ç»“æœ
        for result in completed_results:
            if isinstance(result, Exception):
                print(f"âš ï¸  ä»»åŠ¡å¼‚å¸¸: {result}")
                continue
            
            user_id, notes = result
            results[user_id] = notes
        
        # æ˜¾ç¤ºç»Ÿè®¡
        self._show_stats()
        
        return results
    
    def _show_stats(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡")
        print("="*60)
        print(f"å¤„ç†ç”¨æˆ·æ•°:      {self.stats['users_processed']}")
        print(f"å‘ç°æ–°ç¬”è®°æ•°:    {self.stats['new_notes_found']}")
        print(f"ä¸‹è½½ç¬”è®°æ•°:      {self.stats['notes_downloaded']}")
        print(f"è·³è¿‡ç¬”è®°æ•°:      {self.stats['skipped_notes']} (å·²ä¸‹è½½)")
        print(f"å¤±è´¥ç¬”è®°æ•°:      {self.stats['failed_downloads']}")
        print(f"ä¸‹è½½å›¾ç‰‡æ•°:      {self.stats['images_downloaded']}")
        print(f"ä¸‹è½½è§†é¢‘æ•°:      {self.stats['videos_downloaded']}")
        print("="*60)
        
        # æ˜¾ç¤ºè¿›åº¦ç»Ÿè®¡
        if hasattr(self, 'progress') and len(self.progress) > 0:
            progress_stats = self.get_progress_stats()
            print(f"ğŸ“ æ–­ç‚¹ç»­ä¼ ç»Ÿè®¡")
            print(f"  æ€»ç”¨æˆ·æ•°:     {progress_stats['total_users']}")
            print(f"  å·²å®Œæˆç”¨æˆ·:   {progress_stats['completed_users']}")
            print(f"  æœªå®Œæˆç”¨æˆ·:   {progress_stats['incomplete_users']}")
            print(f"  æ€»å·²ä¸‹è½½ç¬”è®°: {progress_stats['total_notes']}")
            print(f"  æˆåŠŸ:         {progress_stats['by_status'].get('success', 0)}")
            print(f"  å¤±è´¥:         {progress_stats['by_status'].get('failed', 0)}")
            print(f"  è¿›åº¦æ–‡ä»¶:     {self.progress_file}")
            print("="*60)
        
        print("âœ… å®Œæˆï¼")


# =====================================================

if __name__ == "__main__":
    import sys
    
    crawler = RedBookRealtime()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºè¿›åº¦
    if '--clear-progress' in sys.argv:
        print("ğŸ—‘ï¸  æ¸…ç©ºæ–­ç‚¹ç»­ä¼ è¿›åº¦...")
        crawler.clear_progress()
        print("âœ“ è¿›åº¦å·²æ¸…ç©ºï¼Œå°†é‡æ–°ä¸‹è½½æ‰€æœ‰å†…å®¹")
        sys.exit(0)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æŸ¥çœ‹è¿›åº¦
    if '--show-progress' in sys.argv:
        print("ğŸ“Š æ–­ç‚¹ç»­ä¼ è¿›åº¦ç»Ÿè®¡:")
        stats = crawler.get_progress_stats()
        print(f"  æ€»ç”¨æˆ·æ•°:     {stats['total_users']}")
        print(f"  å·²å®Œæˆç”¨æˆ·:   {stats['completed_users']}")
        print(f"  æœªå®Œæˆç”¨æˆ·:   {stats['incomplete_users']}")
        print(f"  æ€»å·²ä¸‹è½½ç¬”è®°: {stats['total_notes']}")
        print(f"  æˆåŠŸ:         {stats['by_status'].get('success', 0)}")
        print(f"  å¤±è´¥:         {stats['by_status'].get('failed', 0)}")
        print(f"\næŒ‰ç”¨æˆ·ç»Ÿè®¡:")
        for user_id, user_info in stats['by_user'].items():
            status_icon = "âœ…" if user_info['completed'] else "ğŸ”„"
            print(f"    {status_icon} {user_id}: {user_info['note_count']} ä¸ªç¬”è®° (æœ€åæ›´æ–°: {user_info['last_update']})")
        print(f"\nè¿›åº¦æ–‡ä»¶: {crawler.progress_file}")
        sys.exit(0)
    
    if len(sys.argv) > 1 and not sys.argv[1].startswith('--'):
        # å¤„ç†æŒ‡å®šç”¨æˆ·
        user_id = sys.argv[1]
        crawler(user_id, save_data=True, download_media=True)
    else:
        # æ‰¹é‡å¤„ç†æ‰€æœ‰ç”¨æˆ·
        # ä½¿ç”¨æ–¹æ³•ï¼š
        # python tools/redbook_realtime.py                            # é»˜è®¤3ä¸ªå¹¶è¡Œ
        # python tools/redbook_realtime.py --parallel 2               # 2ä¸ªå¹¶è¡Œ
        # python tools/redbook_realtime.py --parallel 5               # 5ä¸ªå¹¶è¡Œ
        # python tools/redbook_realtime.py --max-users 200            # åªå¤„ç†å‰200ä¸ªç”¨æˆ·
        # python tools/redbook_realtime.py --parallel 3 --max-users 200  # 3ä¸ªå¹¶è¡Œï¼Œæœ€å¤š200ä¸ªç”¨æˆ·
        # python tools/redbook_realtime.py --clear-progress           # æ¸…ç©ºè¿›åº¦ï¼Œé‡æ–°å¼€å§‹
        # python tools/redbook_realtime.py --show-progress            # æŸ¥çœ‹è¿›åº¦ç»Ÿè®¡
        
        parallel = 3  # é»˜è®¤3ä¸ªå¹¶è¡Œ
        max_users = None  # é»˜è®¤å¤„ç†æ‰€æœ‰ç”¨æˆ·
        
        # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†å¹¶å‘æ•°
        if '--parallel' in sys.argv:
            try:
                idx = sys.argv.index('--parallel')
                if idx + 1 < len(sys.argv):
                    parallel = int(sys.argv[idx + 1])
                    print(f"âœ“ è®¾ç½®å¹¶å‘æ•°: {parallel}")
            except (ValueError, IndexError):
                print("âš ï¸  å¹¶å‘æ•°å‚æ•°é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼ 3")
        
        # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†ç”¨æˆ·ä¸Šé™
        if '--max-users' in sys.argv:
            try:
                idx = sys.argv.index('--max-users')
                if idx + 1 < len(sys.argv):
                    max_users = int(sys.argv[idx + 1])
                    print(f"âœ“ è®¾ç½®ç”¨æˆ·ä¸Šé™: {max_users}")
            except (ValueError, IndexError):
                print("âš ï¸  ç”¨æˆ·ä¸Šé™å‚æ•°é”™è¯¯ï¼Œå°†å¤„ç†æ‰€æœ‰ç”¨æˆ·")
        
        crawler.process_all_users(save_data=True, download_media=True, max_users=max_users, parallel=parallel)

