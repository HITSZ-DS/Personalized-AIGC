
import json
import os
import pandas as pd
import time
import csv
import asyncio
import random
import subprocess
import shutil
from typing import List, Dict, Optional
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from bilibli_spider import BilibiliFavoritesSpider

class BilibiliRealTime:
    def __init__(self, cookies: str = None):
        """
        Initialize the BilibiliRealTime crawler

        Args:
            cookies: Optional cookies for authenticated requests
        """
        self.spider = BilibiliFavoritesSpider(cookies)
        self.user_map_path = "SSLRec/datasets/general_cf/bilibili/user_map.json"
        self.dataset_path = "dataset/bilibili"
        self.realtime_dataset_path = "dataset/bilibili_realtime"
        self.download_path = "download"

        # Load user mapping
        self.user_map = self._load_user_map()

    def __call__(self, user_id: str, save_data: bool = True, download_videos: bool = True, is_real_uid: bool = False) -> List[Dict]:
        """
        Get latest favorites for a specific user

        Args:
            user_id: The user ID (can be internal ID or real bilibili UID)
            save_data: Whether to automatically save the fetched data
            download_videos: Whether to download the actual video files
            is_real_uid: If True, user_id is treated as real bilibili UID; if False, as internal ID

        Returns:
            List of latest favorite videos (max 3)
        """
        if is_real_uid:
            # user_id is already a real bilibili UID
            real_uid = user_id
            # Try to get internal ID for data path compatibility
            internal_user_id = self.get_internal_user_id(real_uid)
            if not internal_user_id:
                print(f"è­¦å‘Š: ç”¨æˆ· {real_uid} åœ¨user_mapä¸­æœªæ‰¾åˆ°å¯¹åº”çš„å†…éƒ¨IDï¼Œä½¿ç”¨real_uidä½œä¸ºæ ‡è¯†")
                internal_user_id = real_uid
        else:
            # user_id is an internal ID, convert to real UID
            real_uid = self._get_real_uid(user_id)
            if not real_uid:
                print(f"ç”¨æˆ·ID {user_id} æœªæ‰¾åˆ°å¯¹åº”çš„çœŸå®Bç«™UID")
                return []
            internal_user_id = user_id

        # Get latest timestamp from existing data
        latest_timestamp = self._get_latest_timestamp(internal_user_id)

        # Fetch latest favorites
        new_videos = self._fetch_latest_favorites(real_uid, internal_user_id, latest_timestamp)

        # Automatically save data if requested and data exists
        if save_data and new_videos:
            self._save_realtime_data(new_videos, internal_user_id, real_uid)

        # Download videos if requested and data exists
        if download_videos and new_videos:
            print(f"å¼€å§‹ä¸‹è½½ç”¨æˆ· {real_uid} çš„å®æ—¶è§†é¢‘...")
            asyncio.run(self._download_realtime_videos(new_videos, internal_user_id, real_uid, global_concurrent=False))

        return new_videos

    def _load_user_map(self) -> Dict[str, str]:
        """Load user mapping from JSON file"""
        try:
            with open(self.user_map_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"ç”¨æˆ·æ˜ å°„æ–‡ä»¶æœªæ‰¾åˆ°: {self.user_map_path}")
            return {}
        except json.JSONDecodeError:
            print(f"ç”¨æˆ·æ˜ å°„æ–‡ä»¶æ ¼å¼é”™è¯¯: {self.user_map_path}")
            return {}

    def _get_real_uid(self, user_id: str) -> Optional[str]:
        """Convert internal user_id to real bilibili UID"""
        return self.user_map.get(user_id)

    def _get_latest_timestamp(self, user_id: str) -> int:
        """Get the latest fav_time timestamp from user's existing data"""
        user_dir = os.path.join(self.dataset_path, self.user_map.get(user_id, ""))
        if not os.path.exists(user_dir):
            print(f"ç”¨æˆ·æ•°æ®ç›®å½•ä¸å­˜åœ¨: {user_dir}")
            return 0

        latest_timestamp = 0
        try:
            # Find all CSV files in user directory
            csv_files = [f for f in os.listdir(user_dir) if f.endswith('.csv')]

            for csv_file in csv_files:
                csv_path = os.path.join(user_dir, csv_file)
                try:
                    df = pd.read_csv(csv_path)
                    if 'fav_time' in df.columns and not df.empty:
                        max_time = df['fav_time'].max()
                        if pd.notna(max_time):
                            latest_timestamp = max(latest_timestamp, int(max_time))
                except Exception as e:
                    print(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥ {csv_path}: {e}")
                    continue

        except Exception as e:
            print(f"æ‰«æç”¨æˆ·ç›®å½•å¤±è´¥ {user_dir}: {e}")

        print(f"ç”¨æˆ· {user_id} çš„æœ€æ–°æ—¶é—´æˆ³: {latest_timestamp}")
        return latest_timestamp

    def _fetch_latest_favorites(self, real_uid: str, user_id: str, latest_timestamp: int) -> List[Dict]:
        """
        Fetch latest favorites that are newer than latest_timestamp

        Args:
            real_uid: Real bilibili UID
            user_id: Internal user ID
            latest_timestamp: Latest timestamp from existing data

        Returns:
            List of latest favorite videos (max 3)
        """
        try:
            # Get user's favorites list
            favorites = self.spider.get_user_favorites(real_uid)
            if not favorites:
                print(f"æ— æ³•è·å–ç”¨æˆ· {real_uid} çš„æ”¶è—å¤¹åˆ—è¡¨")
                return []

            print(f"è·å–åˆ°ç”¨æˆ· {real_uid} çš„ {len(favorites)} ä¸ªæ”¶è—å¤¹")

            # Collect all new videos from all favorites
            new_videos = []

            for fav in favorites:
                if len(new_videos) >= 3:  # Already have enough videos
                    break

                fav_id = fav['id']
                fav_title = fav['title']

                print(f"æ£€æŸ¥æ”¶è—å¤¹: {fav_title}")

                # Get contents of this favorite folder
                page = 1
                while len(new_videos) < 3:
                    videos = self.spider.get_favorite_contents(fav_id, fav_title, page)
                    if not videos:
                        break

                    for video in videos:
                        # Check if this video is newer than latest_timestamp
                        fav_time = video.get('fav_time', 0)
                        if isinstance(fav_time, (int, float)) and fav_time > latest_timestamp:
                            new_videos.append(video)
                            print(f"å‘ç°æ–°è§†é¢‘: {video.get('title', 'Unknown')} (fav_time: {fav_time})")

                            if len(new_videos) >= 3:
                                break

                    # If no more videos or we have enough, break
                    if len(videos) < 20 or len(new_videos) >= 3:
                        break

                    page += 1
                    time.sleep(1)  # Be polite to the API

                time.sleep(2)  # Delay between favorites

            # Sort by fav_time descending and take top 3
            new_videos.sort(key=lambda x: x.get('fav_time', 0), reverse=True)
            result = new_videos[:3]

            print(f"ç”¨æˆ· {real_uid} æ‰¾åˆ° {len(result)} ä¸ªæ–°æ”¶è—è§†é¢‘")
            return result

        except Exception as e:
            print(f"è·å–ç”¨æˆ· {real_uid} æœ€æ–°æ”¶è—å¤±è´¥: {e}")
            return []

    def _save_as_csv(self, videos: List[Dict], filepath: str):
        """
        Save videos data as CSV file

        Args:
            videos: List of video dictionaries
            filepath: Path to save the CSV file
        """
        if not videos:
            return

        # Get all possible fields
        fields = set()
        for video in videos:
            fields.update(video.keys())
        fields = sorted(fields)

        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        # Write CSV file
        with open(filepath, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fields)
            writer.writeheader()
            writer.writerows(videos)

    def _save_realtime_data(self, videos: List[Dict], user_id: str, real_uid: str):
        """
        Save realtime data following bilibili_spider.py's structure

        Args:
            videos: List of video dictionaries to save
            user_id: Internal user ID
            real_uid: Real bilibili UID
        """
        if not videos:
            print(f"æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜ (ç”¨æˆ·: {user_id})")
            return

        # Create timestamp for this batch
        timestamp = int(time.time())

        # Create user directory using real_uid
        user_dir = os.path.join(self.realtime_dataset_path, real_uid)
        os.makedirs(user_dir, exist_ok=True)

        # Group videos by favorite folder
        fav_contents = {}
        for video in videos:
            fav_id = video.get('fav_id', 'unknown')
            fav_title = video.get('fav_title', 'unknown')

            if fav_id not in fav_contents:
                fav_contents[fav_id] = {
                    'title': fav_title,
                    'videos': []
                }
            fav_contents[fav_id]['videos'].append(video)

        # Save each favorite folder as separate CSV file
        saved_files = []
        for fav_id, content in fav_contents.items():
            fav_title = content['title']
            fav_videos = content['videos']

            # Create safe filename (same logic as bilibili_spider.py)
            safe_title = "".join([c for c in fav_title if c.isalnum() or c in ' _-'])
            safe_title = safe_title[:50]  # Limit length

            # Create filename with timestamp to avoid conflicts
            filename = f"{safe_title}_{fav_id}_{timestamp}.csv"
            filepath = os.path.join(user_dir, filename)

            # Save CSV file
            self._save_as_csv(fav_videos, filepath)
            saved_files.append(filepath)

            print(f"ä¿å­˜æ”¶è—å¤¹æ•°æ®: {filepath} ({len(fav_videos)} ä¸ªè§†é¢‘)")

        # Create summary file for this batch
        if len(fav_contents) > 1:
            summary_filename = f"realtime_summary_{timestamp}.csv"
            summary_filepath = os.path.join(user_dir, summary_filename)
            self._save_as_csv(videos, summary_filepath)
            saved_files.append(summary_filepath)
            print(f"ä¿å­˜æ±‡æ€»æ•°æ®: {summary_filepath}")

        print(f"ç”¨æˆ· {real_uid} å®æ—¶æ•°æ®ä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {len(saved_files)} ä¸ªæ–‡ä»¶")
        return saved_files

    def get_user_stats(self, user_id: str) -> Dict:
        """Get statistics for a user"""
        real_uid = self._get_real_uid(user_id)
        if not real_uid:
            return {"error": "ç”¨æˆ·IDæœªæ‰¾åˆ°"}

        # Count existing videos
        user_dir = os.path.join(self.dataset_path, real_uid)
        total_videos = 0

        if os.path.exists(user_dir):
            for file in os.listdir(user_dir):
                if file.endswith('.csv'):
                    csv_path = os.path.join(user_dir, file)
                    try:
                        df = pd.read_csv(csv_path)
                        total_videos += len(df)
                    except Exception:
                        continue

        return {
            "user_id": user_id,
            "real_uid": real_uid,
            "total_videos": total_videos,
            "dataset_path": user_dir
        }

    async def _download_realtime_videos(self, videos: List[Dict], user_id: str, real_uid: str, global_concurrent: bool = False, max_concurrent_downloads: int = 10):
        """
        Download realtime videos to download/{user}/realtime directory

        Args:
            videos: List of video dictionaries to download
            user_id: Internal user ID
            real_uid: Real bilibili UID
            global_concurrent: Whether to use global concurrency (for single user, this is same as batch mode)
            max_concurrent_downloads: Maximum concurrent downloads
        """
        if not videos:
            print(f"æ²¡æœ‰è§†é¢‘éœ€è¦ä¸‹è½½ (ç”¨æˆ·: {user_id})")
            return

        # Check if yt-dlp is available
        if not self._ensure_ytdlp_available():
            print("âŒ yt-dlp ä¸å¯ç”¨ï¼Œè·³è¿‡è§†é¢‘ä¸‹è½½")
            return

        # Create realtime download directory
        realtime_dir = os.path.join(self.download_path, "bilibili", real_uid, "realtime")
        os.makedirs(realtime_dir, exist_ok=True)

        print(f"å®æ—¶è§†é¢‘ä¸‹è½½ç›®å½•: {realtime_dir}")
        print(f"å‡†å¤‡ä¸‹è½½ {len(videos)} ä¸ªå®æ—¶è§†é¢‘...")
        print(f"å¹¶å‘æ¨¡å¼: {'å…¨å±€å¹¶å‘' if global_concurrent else 'æ‰¹æ¬¡å¹¶å‘'}")

        if global_concurrent:
            # Use global concurrent mode for single user
            await self._global_concurrent_download_single_user(videos, real_uid, realtime_dir, max_concurrent_downloads)
        else:
            # Use original batch mode
            await self._batch_download_single_user(videos, real_uid, realtime_dir)

    async def _global_concurrent_download_single_user(self, videos: List[Dict], real_uid: str, realtime_dir: str, max_concurrent_downloads: int):
        """
        Global concurrent download for single user's videos
        """
        print(f"ğŸš€ å¯ç”¨å…¨å±€å¹¶å‘ä¸‹è½½æ¨¡å¼ (æœ€å¤§å¹¶å‘: {max_concurrent_downloads})...")

        # Create download tasks
        download_tasks = []
        video_infos = []

        for video in videos:
            bvid = video.get('bvid')
            if bvid:
                # Create individual folder for each video
                video_folder = os.path.join(realtime_dir, bvid)
                os.makedirs(video_folder, exist_ok=True)

                video_info = {
                    'bvid': bvid,
                    'video_folder': video_folder,
                    'real_uid': real_uid,
                    'title': video.get('title', bvid)[:50]
                }

                task = self._download_single_video(bvid, video_folder, video)
                download_tasks.append(task)
                video_infos.append(video_info)
            else:
                print(f"âš ï¸ è­¦å‘Š: è§†é¢‘ç¼ºå°‘bvidä¿¡æ¯: {video}")

        if not download_tasks:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘å¯ä»¥ä¸‹è½½")
            return

        # Use semaphore to limit global concurrency
        semaphore = asyncio.Semaphore(max_concurrent_downloads)
        successful_downloads = 0
        failed_downloads = 0

        async def limited_download(video_info, task):
            async with semaphore:
                try:
                    result = await task
                    if result:
                        print(f"âœ… {video_info['bvid']} ä¸‹è½½æˆåŠŸ: {video_info['title']}")
                        return True
                    else:
                        print(f"âŒ {video_info['bvid']} ä¸‹è½½å¤±è´¥: {video_info['title']}")
                        return False
                except Exception as e:
                    print(f"âŒ {video_info['bvid']} ä¸‹è½½å¼‚å¸¸: {e}")
                    return False

        # Create limited tasks
        limited_tasks = []
        for video_info, task in zip(video_infos, download_tasks):
            limited_task = limited_download(video_info, task)
            limited_tasks.append(limited_task)

        # Execute all downloads concurrently
        start_time = time.time()
        results = await asyncio.gather(*limited_tasks, return_exceptions=True)
        end_time = time.time()

        # Count results
        for result in results:
            if isinstance(result, Exception):
                failed_downloads += 1
                print(f"âŒ ä¸‹è½½å¼‚å¸¸: {result}")
            elif result:
                successful_downloads += 1
            else:
                failed_downloads += 1

        # Print summary
        print(f"\nğŸ‰ å…¨å±€å¹¶å‘ä¸‹è½½å®Œæˆ:")
        print(f"  âœ… æˆåŠŸä¸‹è½½: {successful_downloads}/{len(download_tasks)} ä¸ªè§†é¢‘")
        print(f"  âŒ å¤±è´¥ä¸‹è½½: {failed_downloads}/{len(download_tasks)} ä¸ªè§†é¢‘")
        print(f"  â±ï¸ æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
        if end_time - start_time > 0:
            print(f"  ğŸš€ å¹³å‡é€Ÿåº¦: {len(download_tasks)/(end_time - start_time):.2f} ä»»åŠ¡/ç§’")
        print(f"  ğŸ‘¤ ç”¨æˆ·: {real_uid}")
        print(f"  ğŸ“ ä¸‹è½½ç›®å½•: {realtime_dir}")

    async def _batch_download_single_user(self, videos: List[Dict], real_uid: str, realtime_dir: str):
        """
        Original batch download for single user's videos
        """
        print(f"ğŸ“± ä½¿ç”¨æ‰¹æ¬¡å¹¶å‘ä¸‹è½½æ¨¡å¼...")

        # Create download tasks
        download_tasks = []
        for video in videos:
            bvid = video.get('bvid')
            if bvid:
                # Create individual folder for each video
                video_folder = os.path.join(realtime_dir, bvid)
                os.makedirs(video_folder, exist_ok=True)

                task = self._download_single_video(bvid, video_folder, video)
                download_tasks.append(task)
            else:
                print(f"âš ï¸ è­¦å‘Š: è§†é¢‘ç¼ºå°‘bvidä¿¡æ¯: {video}")

        if not download_tasks:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘å¯ä»¥ä¸‹è½½")
            return

        # Execute downloads with limited concurrency
        successful_downloads = 0
        batch_size = 3  # Download 3 videos concurrently to avoid rate limiting

        for batch_idx in range(0, len(download_tasks), batch_size):
            batch_tasks = download_tasks[batch_idx:batch_idx + batch_size]
            print(f"ğŸ“¦ å¤„ç†ä¸‹è½½æ‰¹æ¬¡ {batch_idx//batch_size + 1}/{(len(download_tasks) + batch_size - 1)//batch_size} ({len(batch_tasks)} ä¸ªè§†é¢‘)")

            # Add delay between batches (except for the first batch)
            if batch_idx > 0:
                delay = random.uniform(2, 3)  # 2-3 seconds between batches
                print(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¼€å§‹ä¸‹ä¸€æ‰¹æ¬¡...")
                await asyncio.sleep(delay)

            # Execute current batch concurrently
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # Count successful downloads
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    print(f"âŒ ä¸‹è½½å¼‚å¸¸: {result}")
                elif result:
                    successful_downloads += 1

            print(f"âœ… æ‰¹æ¬¡å®Œæˆ: {sum(1 for r in batch_results if r is True)}/{len(batch_results)} ä¸ªè§†é¢‘ä¸‹è½½æˆåŠŸ")

        print(f"\nğŸ“Š å®æ—¶è§†é¢‘ä¸‹è½½æ€»ç»“:")
        print(f"  âœ… æˆåŠŸä¸‹è½½: {successful_downloads}/{len(download_tasks)} ä¸ªè§†é¢‘")
        print(f"  ğŸ‘¤ ç”¨æˆ·: {real_uid}")
        print(f"  ğŸ“ ä¸‹è½½ç›®å½•: {realtime_dir}")

    async def _download_single_video(self, bvid: str, save_path: str, video_info: Dict, max_retries: int = 3, max_duration_hours: int = 0.1, max_videos_per_playlist: int = 1) -> bool:
        """
        Download a single bilibili video with retry mechanism

        Args:
            bvid: Bilibili video ID
            save_path: Path to save the video
            video_info: Video information dictionary
            max_retries: Maximum retry attempts
            max_duration_hours: Maximum video duration in hours
            max_videos_per_playlist: Maximum videos to download from playlists

        Returns:
            True if successful, False otherwise
        """
        for attempt in range(max_retries):
            try:
                # Add random delay to avoid being detected as bot
                if attempt > 0:
                    delay = random.uniform(5, 10)  # 5-10 seconds delay
                    print(f"é‡è¯•ä¸‹è½½ {bvid}, ç­‰å¾… {delay:.1f} ç§’... (å°è¯• {attempt + 1}/{max_retries})")
                    await asyncio.sleep(delay)

                url = f"https://www.bilibili.com/video/{bvid}"
                video_title = video_info.get('title', bvid)[:50]  # Limit title length for display
                print(f"ä¸‹è½½å®æ—¶è§†é¢‘: {video_title} ({bvid}) åˆ° {save_path}")

                # Enhanced yt-dlp command with better options and duration limit
                cmd = [
                    "yt-dlp",
                    "--output", f"{save_path}/%(title)s.%(ext)s",
                    "--no-warnings",
                    "--retries", "3",
                    "--fragment-retries", "3",
                    "--skip-unavailable-fragments",
                    "--user-agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    "--referer", "https://www.bilibili.com/",
                ]

                # Add duration limit (convert hours to seconds for yt-dlp)
                max_duration_seconds = max_duration_hours * 3600
                cmd.extend(["--download-sections", f"*0-{max_duration_seconds}"])

                # Add playlist item limit
                cmd.extend(["--playlist-end", str(max_videos_per_playlist)])

                # Add the URL at the end
                cmd.append(url)

                # Run yt-dlp command asynchronously
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )

                stdout, stderr = await process.communicate()

                if process.returncode == 0:
                    print(f"âœ… å®æ—¶è§†é¢‘ä¸‹è½½æˆåŠŸ: {bvid}")
                    return True
                else:
                    try:
                        error_msg = stderr.decode('utf-8')
                    except UnicodeDecodeError:
                        error_msg = stderr.decode('gbk', errors='ignore')

                    # Check if it's a permanent error (no need to retry)
                    if any(keyword in error_msg.lower() for keyword in ['private', 'deleted', 'not available', 'geo-blocked']):
                        print(f"âŒ æ°¸ä¹…æ€§é”™è¯¯ï¼Œè·³è¿‡ {bvid}: {error_msg}")
                        return False

                    print(f"âš ï¸ ä¸‹è½½å°è¯• {attempt + 1} å¤±è´¥ {bvid}: {error_msg}")

                    if attempt == max_retries - 1:
                        print(f"âŒ æ‰€æœ‰ä¸‹è½½å°è¯•éƒ½å¤±è´¥ {bvid}")
                        return False

            except Exception as e:
                print(f"âŒ ä¸‹è½½å¼‚å¸¸ {bvid}: {e}")
                if attempt == max_retries - 1:
                    return False

        return False

    def _check_ytdlp_available(self) -> bool:
        """
        Check if yt-dlp is available in the system

        Returns:
            True if yt-dlp is available, False otherwise
        """
        return shutil.which("yt-dlp") is not None

    def _install_ytdlp(self) -> bool:
        """
        Try to install yt-dlp using pip

        Returns:
            True if installation successful, False otherwise
        """
        try:
            print("ğŸ”§ yt-dlp æœªæ‰¾åˆ°ï¼Œå°è¯•å®‰è£…...")
            result = subprocess.run([sys.executable, "-m", "pip", "install", "yt-dlp"],
                                  capture_output=True, text=True, timeout=300)

            if result.returncode == 0:
                print("âœ… yt-dlp å®‰è£…æˆåŠŸ")
                return True
            else:
                print(f"âŒ yt-dlp å®‰è£…å¤±è´¥: {result.stderr}")
                return False

        except Exception as e:
            print(f"âŒ å®‰è£… yt-dlp æ—¶å‡ºé”™: {e}")
            return False

    def _ensure_ytdlp_available(self) -> bool:
        """
        Ensure yt-dlp is available, install if necessary

        Returns:
            True if yt-dlp is available, False otherwise
        """
        if self._check_ytdlp_available():
            return True

        print("âš ï¸ æ£€æµ‹åˆ° yt-dlp æœªå®‰è£…ï¼Œè§†é¢‘ä¸‹è½½åŠŸèƒ½éœ€è¦ yt-dlp")

        # Ask user if they want to install
        try:
            choice = input("æ˜¯å¦è¦è‡ªåŠ¨å®‰è£… yt-dlp? (y/n): ").lower().strip()
            if choice == 'y':
                return self._install_ytdlp()
            else:
                print("âŒ ç”¨æˆ·é€‰æ‹©ä¸å®‰è£… yt-dlpï¼Œè·³è¿‡è§†é¢‘ä¸‹è½½")
                return False
        except:
            # If input fails (e.g., in non-interactive environment), try to install automatically
            print("ğŸ¤– éäº¤äº’ç¯å¢ƒï¼Œå°è¯•è‡ªåŠ¨å®‰è£… yt-dlp...")
            return self._install_ytdlp()

    def get_all_download_users(self) -> List[str]:
        """
        Get all user IDs that exist in the download/bilibili directory

        Returns:
            List of real user IDs (not internal IDs)
        """
        bilibili_download_dir = os.path.join(self.download_path, "bilibili")

        if not os.path.exists(bilibili_download_dir):
            print(f"Bilibiliä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {bilibili_download_dir}")
            return []

        # Get all subdirectories (these are real user IDs)
        user_dirs = []
        try:
            for item in os.listdir(bilibili_download_dir):
                item_path = os.path.join(bilibili_download_dir, item)
                if os.path.isdir(item_path):
                    user_dirs.append(item)

            print(f"å‘ç° {len(user_dirs)} ä¸ªç”¨æˆ·ç›®å½•: {user_dirs}")
            return user_dirs

        except Exception as e:
            print(f"æ‰«æä¸‹è½½ç›®å½•å¤±è´¥: {e}")
            return []

    def get_internal_user_id(self, real_uid: str) -> Optional[str]:
        """
        Convert real bilibili UID to internal user_id (reverse of _get_real_uid)

        Args:
            real_uid: Real bilibili UID

        Returns:
            Internal user ID if found, None otherwise
        """
        for internal_id, real_id in self.user_map.items():
            if real_id == real_uid:
                return internal_id
        return None

    def process_all_users(self, save_data: bool = True, download_videos: bool = True, global_concurrent: bool = True, max_concurrent_downloads: int = 15) -> Dict[str, List[Dict]]:
        """
        Process realtime data for all users in the download directory

        Args:
            save_data: Whether to save the fetched data
            download_videos: Whether to download the actual video files
            global_concurrent: Whether to use global concurrency across all users
            max_concurrent_downloads: Maximum concurrent downloads globally

        Returns:
            Dictionary mapping user_id to list of new videos
        """
        print("ğŸš€ å¼€å§‹å¤„ç†æ‰€æœ‰ç”¨æˆ·çš„å®æ—¶æ•°æ®...")
        print(f"ğŸ”§ å¹¶å‘æ¨¡å¼: {'å…¨å±€å¹¶å‘' if global_concurrent else 'æŒ‰ç”¨æˆ·ä¸²è¡Œ'}")
        if global_concurrent and download_videos:
            print(f"ğŸ¯ æœ€å¤§å¹¶å‘ä¸‹è½½æ•°: {max_concurrent_downloads}")
        print("=" * 60)

        # Get all users from download directory
        real_user_ids = self.get_all_download_users()

        if not real_user_ids:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç”¨æˆ·ç›®å½•")
            return {}

        if global_concurrent and download_videos:
            # Use global concurrent mode
            return asyncio.run(self._process_all_users_global_concurrent(real_user_ids, save_data, max_concurrent_downloads))
        else:
            # Use original sequential mode
            return self._process_all_users_sequential(real_user_ids, save_data, download_videos)

    def _process_all_users_sequential(self, real_user_ids: List[str], save_data: bool, download_videos: bool) -> Dict[str, List[Dict]]:
        """
        Original sequential processing of all users
        """
        print("ğŸ“± ä½¿ç”¨æŒ‰ç”¨æˆ·ä¸²è¡Œå¤„ç†æ¨¡å¼...")

        results = {}
        total_new_videos = 0

        for i, real_uid in enumerate(real_user_ids, 1):
            print(f"\nğŸ“‹ å¤„ç†ç”¨æˆ· {i}/{len(real_user_ids)}: {real_uid}")
            print("-" * 40)

            try:
                # Process this user's realtime data using real UID directly
                new_videos = self(real_uid, save_data=save_data, download_videos=download_videos, is_real_uid=True)
                results[real_uid] = new_videos
                total_new_videos += len(new_videos)

                print(f"âœ… ç”¨æˆ· {real_uid} å¤„ç†å®Œæˆ: {len(new_videos)} ä¸ªæ–°è§†é¢‘")

                # Add delay between users to be polite
                if i < len(real_user_ids):  # Don't delay after the last user
                    delay = random.uniform(2, 4)
                    print(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªç”¨æˆ·...")
                    time.sleep(delay)

            except Exception as e:
                print(f"âŒ å¤„ç†ç”¨æˆ· {real_uid} æ—¶å‡ºé”™: {e}")
                results[real_uid] = []
                continue

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰ç”¨æˆ·å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æ€»ç»“:")
        print(f"  - å¤„ç†ç”¨æˆ·æ•°: {len(real_user_ids)}")
        print(f"  - æ€»æ–°è§†é¢‘æ•°: {total_new_videos}")
        print(f"  - æˆåŠŸå¤„ç†: {len([r for r in results.values() if r])} ä¸ªç”¨æˆ·æœ‰æ–°å†…å®¹")
        print("=" * 60)

        return results

    async def _process_all_users_global_concurrent(self, real_user_ids: List[str], save_data: bool, max_concurrent_downloads: int) -> Dict[str, List[Dict]]:
        """
        Global concurrent processing of all users with cross-user video download concurrency
        """
        print("ğŸš€ ä½¿ç”¨å…¨å±€å¹¶å‘å¤„ç†æ¨¡å¼...")

        # Step 1: Collect all new videos from all users (without downloading)
        print("\nğŸ“Š ç¬¬ä¸€é˜¶æ®µ: æ”¶é›†æ‰€æœ‰ç”¨æˆ·çš„æ–°è§†é¢‘æ•°æ®...")
        all_user_videos = {}
        total_videos = 0

        for i, real_uid in enumerate(real_user_ids, 1):
            print(f"ğŸ“‹ æ”¶é›†ç”¨æˆ· {i}/{len(real_user_ids)}: {real_uid}")

            try:
                # Get new videos without downloading
                new_videos = self(real_uid, save_data=save_data, download_videos=False, is_real_uid=True)
                all_user_videos[real_uid] = new_videos
                total_videos += len(new_videos)

                print(f"âœ… ç”¨æˆ· {real_uid}: å‘ç° {len(new_videos)} ä¸ªæ–°è§†é¢‘")

                # Small delay between API calls
                if i < len(real_user_ids):
                    await asyncio.sleep(1)

            except Exception as e:
                print(f"âŒ æ”¶é›†ç”¨æˆ· {real_uid} æ•°æ®æ—¶å‡ºé”™: {e}")
                all_user_videos[real_uid] = []
                continue

        print(f"\nğŸ“ˆ æ•°æ®æ”¶é›†å®Œæˆ:")
        print(f"  ğŸ“Š æ€»ç”¨æˆ·æ•°: {len(real_user_ids)}")
        print(f"  ğŸ¬ æ€»è§†é¢‘æ•°: {total_videos}")
        print(f"  âœ… æœ‰æ–°å†…å®¹çš„ç”¨æˆ·: {len([v for v in all_user_videos.values() if v])}")

        if total_videos == 0:
            print("âŒ æ²¡æœ‰å‘ç°æ–°è§†é¢‘ï¼Œè·³è¿‡ä¸‹è½½é˜¶æ®µ")
            return all_user_videos

        # Step 2: Global concurrent download across all users
        print(f"\nğŸ¯ ç¬¬äºŒé˜¶æ®µ: å…¨å±€å¹¶å‘ä¸‹è½½æ‰€æœ‰è§†é¢‘ (æœ€å¤§å¹¶å‘: {max_concurrent_downloads})...")

        # Check if yt-dlp is available
        if not self._ensure_ytdlp_available():
            print("âŒ yt-dlp ä¸å¯ç”¨ï¼Œè·³è¿‡è§†é¢‘ä¸‹è½½")
            return all_user_videos

        # Create all download tasks
        all_download_tasks = []
        video_task_mapping = []  # Track which task belongs to which user/video

        for real_uid, videos in all_user_videos.items():
            if not videos:
                continue

            # Create realtime download directory for this user
            realtime_dir = os.path.join(self.download_path, "bilibili", real_uid, "realtime")
            os.makedirs(realtime_dir, exist_ok=True)

            for video in videos:
                bvid = video.get('bvid')
                if bvid:
                    # Create individual folder for each video
                    video_folder = os.path.join(realtime_dir, bvid)
                    os.makedirs(video_folder, exist_ok=True)

                    task_info = {
                        'bvid': bvid,
                        'real_uid': real_uid,
                        'video_folder': video_folder,
                        'title': video.get('title', bvid)[:50]
                    }

                    task = self._download_single_video(bvid, video_folder, video)
                    all_download_tasks.append(task)
                    video_task_mapping.append(task_info)

        if not all_download_tasks:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘å¯ä»¥ä¸‹è½½")
            return all_user_videos

        print(f"ğŸ“¦ å‡†å¤‡ä¸‹è½½ {len(all_download_tasks)} ä¸ªè§†é¢‘...")

        # Use semaphore to limit global concurrency
        semaphore = asyncio.Semaphore(max_concurrent_downloads)
        successful_downloads = 0
        failed_downloads = 0
        user_success_count = {uid: 0 for uid in all_user_videos.keys()}

        async def limited_download(task_info, task):
            async with semaphore:
                try:
                    result = await task
                    if result:
                        user_success_count[task_info['real_uid']] += 1
                        print(f"âœ… {task_info['bvid']} ä¸‹è½½æˆåŠŸ ({task_info['real_uid']}): {task_info['title']}")
                        return True
                    else:
                        print(f"âŒ {task_info['bvid']} ä¸‹è½½å¤±è´¥ ({task_info['real_uid']}): {task_info['title']}")
                        return False
                except Exception as e:
                    print(f"âŒ {task_info['bvid']} ä¸‹è½½å¼‚å¸¸ ({task_info['real_uid']}): {e}")
                    return False

        # Create limited tasks
        limited_tasks = []
        for task_info, task in zip(video_task_mapping, all_download_tasks):
            limited_task = limited_download(task_info, task)
            limited_tasks.append(limited_task)

        # Execute all downloads concurrently
        start_time = time.time()
        results = await asyncio.gather(*limited_tasks, return_exceptions=True)
        end_time = time.time()

        # Count results
        for result in results:
            if isinstance(result, Exception):
                failed_downloads += 1
                print(f"âŒ ä¸‹è½½å¼‚å¸¸: {result}")
            elif result:
                successful_downloads += 1
            else:
                failed_downloads += 1

        # Print summary
        print(f"\nğŸ‰ å…¨å±€å¹¶å‘ä¸‹è½½å®Œæˆ:")
        print(f"  âœ… æˆåŠŸä¸‹è½½: {successful_downloads}/{len(all_download_tasks)} ä¸ªè§†é¢‘")
        print(f"  âŒ å¤±è´¥ä¸‹è½½: {failed_downloads}/{len(all_download_tasks)} ä¸ªè§†é¢‘")
        print(f"  â±ï¸ æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
        if end_time - start_time > 0:
            print(f"  ğŸš€ å¹³å‡é€Ÿåº¦: {len(all_download_tasks)/(end_time - start_time):.2f} ä»»åŠ¡/ç§’")

        # Print per-user summary
        print(f"\nğŸ‘¥ å„ç”¨æˆ·ä¸‹è½½ç»Ÿè®¡:")
        for real_uid, videos in all_user_videos.items():
            if videos:
                success_count = user_success_count[real_uid]
                total_count = len(videos)
                print(f"  ğŸ“‹ {real_uid}: {success_count}/{total_count} æˆåŠŸ")

        print("=" * 60)

        return all_user_videos
