"""
Baseline Post Quality Evaluator

ËØÑ‰º∞ÁîüÊàêÁöÑ baseline ÂõæÊñáÂ∏ñÂ≠êÔºàJPG Ê†ºÂºèÔºâÁöÑË¥®Èáè
ËØÑÂàÜÁª¥Â∫¶ÔºöÈÄªËæëÊÄß„ÄÅËßÜËßâÂëàÁé∞„ÄÅÊãü‰∫∫Á®ãÂ∫¶
"""

import os
import json
import sys
import base64
import time
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

# Load environment variables from .env file
load_dotenv()

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


@dataclass
class DimensionScore:
    """Score for a single evaluation dimension"""
    dimension: str
    score: float  # 0-10


@dataclass
class BaselineEvaluationResult:
    """Complete evaluation result for a baseline post"""
    image_path: str
    overall_score: float  # Weighted average
    dimension_scores: List[DimensionScore]


class BaselineQualityEvaluator:
    """Evaluates baseline post quality using AI"""
    
    # Dimension weights for Redbook (sum to 1.0) - 3 dimensions for baseline
    DIMENSION_WEIGHTS_REDBOOK = {
        'logic': 0.30,              # ÈÄªËæëÊÄß
        'visual_presentation': 0.35, # ËßÜËßâÂëàÁé∞
        'human_likeness': 0.35,      # Êãü‰∫∫Á®ãÂ∫¶
    }
    
    # Dimension weights for Hupu (sum to 1.0) - 3 dimensions for baseline
    DIMENSION_WEIGHTS_HUPU = {
        'logic': 0.35,              # ÈÄªËæëÊÄß
        'visual_presentation': 0.35, # ËßÜËßâÂëàÁé∞
        'human_likeness': 0.30,      # Êãü‰∫∫Á®ãÂ∫¶
    }
    
    # Evaluation dimensions - 3 dimensions for baseline
    DIMENSIONS = {
        'logic': {
            'name': 'ÈÄªËæëÊÄß',
            'description': 'ÂÜÖÂÆπÁªìÊûÑÊòØÂê¶Ê∏ÖÊô∞ÔºåÈÄªËæëÊòØÂê¶ËøûË¥ØÔºå‰ø°ÊÅØÊòØÂê¶Êúâ‰ª∑ÂÄºÔºåÊñáÂ≠ó‰∏éÂõæÁâáÊòØÂê¶ÂåπÈÖçÔºàHupuÊ®°ÂºèÔºöÊñáÂ≠ó‰∏éÂ∏ÉÂ±ÄÊòØÂê¶ÂåπÈÖçÔºâ'
        },
        'visual_presentation': {
            'name': 'ËßÜËßâÂëàÁé∞',
            'description': 'ÂõæÁâáË¥®ÈáèÊòØÂê¶È´òÔºåÊéíÁâàÊòØÂê¶ÂêàÁêÜÁæéËßÇÔºåËßÜËßâÂÖÉÁ¥†ÊòØÂê¶ÊÅ∞ÂΩìÔºåÊï¥‰ΩìËßÜËßâÊïàÊûúÊòØÂê¶Âê∏Âºï‰∫∫ÔºàHupuÊ®°ÂºèÔºöÊéíÁâàÊòØÂê¶ÂêàÁêÜÁæéËßÇÔºåÊñáÂ≠óÂ∏ÉÂ±ÄÊòØÂê¶Ê∏ÖÊô∞Ôºâ'
        },
        'human_likeness': {
            'name': 'Êãü‰∫∫Á®ãÂ∫¶',
            'description': 'ËØ≠Ë®ÄÊòØÂê¶Ëá™ÁÑ∂ÁúüÂÆûÔºåÊòØÂê¶Êúâ‰∏™‰∫∫ÂåñË°®ËææÔºåÊÉÖÊÑüË°®ËææÊòØÂê¶ÊÅ∞ÂΩìÔºåÊòØÂê¶ÂÉèÁúü‰∫∫ÂÜôÁöÑ'
        }
    }
    
    def __init__(self, use_vision_model: bool = True, post_type: str = 'redbook'):
        """
        Initialize evaluator
        
        Args:
            use_vision_model: Whether to use vision-capable model (can see images)
            post_type: 'redbook' or 'hupu'
        """
        self.use_vision_model = use_vision_model
        self.post_type = post_type.lower()
        
        # Select dimension weights based on post type
        if self.post_type == 'hupu':
            self.DIMENSION_WEIGHTS = self.DIMENSION_WEIGHTS_HUPU
        else:
            self.DIMENSION_WEIGHTS = self.DIMENSION_WEIGHTS_REDBOOK
        
        # Initialize OpenAI client with .env configuration
        self.client = OpenAI(
            api_key=os.getenv("CHAT_API_KEY"),
            base_url=os.getenv("CHAT_BASE_URL"),
        )
        self.model = os.getenv("CHAT_MODEL", "claude-sonnet-4-5-20250929")
    
    def image_to_base64(self, image_path: str) -> Optional[str]:
        """Convert image to base64 string for API"""
        try:
            with open(image_path, 'rb') as f:
                image_data = f.read()
                base64_str = base64.b64encode(image_data).decode('utf-8')
                
                # Determine MIME type from extension
                ext = Path(image_path).suffix.lower()
                mime_type = {
                    '.jpg': 'image/jpeg',
                    '.jpeg': 'image/jpeg',
                    '.png': 'image/png',
                    '.webp': 'image/webp'
                }.get(ext, 'image/jpeg')
                
                return f"data:{mime_type};base64,{base64_str}"
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to convert image to base64: {e}")
            return None
    
    def create_evaluation_prompt(self) -> str:
        """Create prompt for AI evaluation"""
        prompt_parts = []
        
        # Detect platform name
        platform_name = "Â∞èÁ∫¢‰π¶" if self.post_type == 'redbook' else "ËôéÊâëËÆ∫Âùõ"
        content_type = "ÂõæÊñáÂ∏ñÂ≠ê" if self.post_type == 'redbook' else "ËÆ®ËÆ∫Â∏ñÂ≠ê"
        
        prompt_parts.append(f"‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑÁ§æ‰∫§Â™í‰ΩìÂÜÖÂÆπËØÑ‰ª∑‰∏ìÂÆ∂„ÄÇËØ∑ÂØπ‰ª•‰∏ãAIÁîüÊàêÁöÑ{platform_name}È£éÊ†º{content_type}ËøõË°åÂÖ®Èù¢ËØÑ‰ª∑„ÄÇ")
        prompt_parts.append("\n" + "=" * 60)
        prompt_parts.append("ËØÑ‰ª∑Áª¥Â∫¶ÔºàÊØè‰∏™Áª¥Â∫¶0-10ÂàÜÔºâÔºö")
        prompt_parts.append("=" * 60)
        
        for dim_id, dim_info in self.DIMENSIONS.items():
            prompt_parts.append(f"\n{dim_info['name']} ({dim_id}):")
            prompt_parts.append(f"  {dim_info['description']}")
        
        prompt_parts.append("\n" + "=" * 60)
        prompt_parts.append("Â∏ñÂ≠êÂÜÖÂÆπÔºö")
        prompt_parts.append("=" * 60)
        
        if self.post_type == 'redbook':
            prompt_parts.append("\nËøôÊòØ‰∏ÄÂº†ÂÆåÊï¥ÁöÑÂ∞èÁ∫¢‰π¶Â∏ñÂ≠êÊà™ÂõæÔºåÂåÖÂê´ÊñáÂ≠óÂÜÖÂÆπÂíåÂõæÁâá„ÄÇËØ∑‰ªîÁªÜÊü•ÁúãÂõæÁâá‰∏≠ÁöÑÊâÄÊúâÂÜÖÂÆπÔºàÂåÖÊã¨ÊñáÂ≠ó„ÄÅÂõæÁâá„ÄÅÂ∏ÉÂ±ÄÁ≠âÔºâËøõË°åËØÑ‰ª∑„ÄÇ")
        else:
            prompt_parts.append("\nËøôÊòØ‰∏ÄÂº†ÂÆåÊï¥ÁöÑËôéÊâëËÆ∫ÂùõËÆ®ËÆ∫Â∏ñÂ≠êÊà™ÂõæÔºå‰∏ªË¶ÅÂåÖÂê´ÊñáÂ≠óÂÜÖÂÆπÔºàÊó†ÂõæÁâáÔºâ„ÄÇËØ∑‰ªîÁªÜÊü•ÁúãÂõæÁâá‰∏≠ÁöÑÊâÄÊúâÂÜÖÂÆπÔºàÂåÖÊã¨ÊñáÂ≠ó„ÄÅÂ∏ÉÂ±Ä„ÄÅÊéíÁâàÁ≠âÔºâËøõË°åËØÑ‰ª∑„ÄÇ")
        
        # Add evaluation instructions
        prompt_parts.append("\n" + "=" * 60)
        prompt_parts.append("ËØÑ‰ª∑Ë¶ÅÊ±ÇÔºö")
        prompt_parts.append("=" * 60)
        

# ËØÑÂàÜË¶ÅÊ±Ç‚Äî‚Äî‰∏•ÔºàËØ∑‰∏•Ê†ºÊâßË°åÔºâÔºö
# 1. **ËØÑÂàÜÊ†áÂáÜÔºà0-10ÂàÜÔºå‰øùÁïô1‰ΩçÂ∞èÊï∞Ôºâ**Ôºö
#    - 9.0-10.0ÂàÜÔºöÊé•ËøëÂÆåÁæéÔºåÂá†‰πéÊó†ÂèØÊåëÂâîÔºàÊûÅÂ∞ëÊï∞‰ΩúÂìÅÔºâ
#    - 8.0-8.9ÂàÜÔºö‰ºòÁßÄÊ∞¥Âπ≥ÔºåÊúâÊòéÊòæ‰∫ÆÁÇπ‰ΩÜ‰ªçÊúâÂ∞èÁëïÁñµ
#    - 7.0-7.9ÂàÜÔºöËâØÂ•ΩÊ∞¥Âπ≥ÔºåÂü∫Êú¨ÂêàÊ†º‰ΩÜÊúâÊîπËøõÁ©∫Èó¥
#    - 6.0-6.9ÂàÜÔºö‰∏≠Á≠âÂÅè‰∏ãÔºåÂ≠òÂú®ÊòéÊòæÈóÆÈ¢ò
#    - 5.0ÂàÜ‰ª•‰∏ãÔºöËæÉÂ∑ÆÔºåÊúâ‰∏•ÈáçÈóÆÈ¢ò

# 2. **Êâ£ÂàÜÂèÇËÄÉÊ†áÂáÜÔºàÂèØÂºπÊÄßÊâßË°åÔºâ**Ôºö
#    - ÈÄªËæëÊÄßÔºöÂÜÖÂÆπÁªìÊûÑÊ∑∑‰π±-0.8ÂàÜÔºåÈÄªËæë‰∏çËøûË¥Ø-0.8ÂàÜÔºå‰ø°ÊÅØ‰ª∑ÂÄº‰Ωé-0.5ÂàÜÔºåÂõæÊñá‰∏çÂåπÈÖç-0.8ÂàÜ
#    - ËßÜËßâÂëàÁé∞ÔºöÂõæÁâáË¥®ÈáèÂ∑Æ-0.5ÂàÜÔºåÊéíÁâà‰∏çÁæéËßÇ-0.5ÂàÜÔºåÂÖÉÁ¥†‰∏çÂçèË∞É-0.5ÂàÜÔºåËßÜËßâÂê∏ÂºïÂäõÂº±-0.5ÂàÜ
#    - Êãü‰∫∫Á®ãÂ∫¶ÔºöËØ≠Ë®ÄÁîüÁ°¨-0.5ÂàÜÔºåÁº∫Â∞ë‰∏™ÊÄßÂåñ-0.5ÂàÜÔºåÊÉÖÊÑüË°®Ëææ‰∏çËá™ÁÑ∂-0.5ÂàÜÔºåAIÁóïËøπÊòéÊòæ-0.5ÂàÜ

# 3. **ËØÑÂàÜÂéüÂàô**Ôºö
#    - ‰ªé‰∏•ËØÑÂàÜÔºå‰∏çË¶ÅËøá‰∫éÂÆΩÂÆπ
#    - ÂÖ≥Ê≥®ÁªÜËäÇÂíåË¥®ÈáèÔºå‰∏çË¶ÅÂè™ÁúãÂ§ß‰ΩìÂÆåÊï¥ÊÄß
#    - Âè™ÊúâÁúüÊ≠£Âá∫Ëâ≤ÁöÑ‰ΩúÂìÅÊâçÂ∫îÁªôÂà∞8ÂàÜ‰ª•‰∏ä

#  ---------------------------------------------------------

# ËØÑÂàÜË¶ÅÊ±Ç‚Äî‚Äî‰∏≠Ôºö
# 1. ÂØπÊØè‰∏™Áª¥Â∫¶ËøõË°åËØÑÂàÜÔºà0-10ÂàÜÔºå‰øùÁïô1‰ΩçÂ∞èÊï∞ÔºâÔºåËØ∑‰ª•È´òÊ†áÂáÜËØÑ‰ª∑ÂÜÖÂÆπÁöÑÂÆåÊï¥ÊÄßÂíåË¥®Èáè
# 2. ÁâπÂà´ÂÖ≥Ê≥®ÂÜÖÂÆπÊòØÂê¶Â≠òÂú®ÈúÄË¶ÅÊîπËøõÁöÑÂú∞ÊñπÔºåËØÑÂàÜÊó∂ËØ∑‰∏•Ê†ºÊääÂÖ≥
#    - ÂØπ‰∫éÈÄªËæëÊÄßÈóÆÈ¢ò„ÄÅËßÜËßâÁº∫Èô∑„ÄÅËØ≠Ë®Ä‰∏çËá™ÁÑ∂Á≠âÈóÆÈ¢òÔºåÂ∫îÁõ∏Â∫îÊâ£ÂàÜ
#    - ‰∏çË¶ÅÂõ†‰∏∫ÂÜÖÂÆπÂü∫Êú¨ÂÆåÊï¥Â∞±Áªô‰∫àÈ´òÂàÜÔºåË¶ÅÂÖ≥Ê≥®ÁªÜËäÇÂíåË¥®Èáè
#    - ËØÑÂàÜÂ∫îÂèçÊò†ÂÜÖÂÆπÁöÑÁúüÂÆûË¥®ÈáèÊ∞¥Âπ≥ÔºåÈÅøÂÖçËøá‰∫éÂÆΩÊùæ
# 3. ËÆ°ÁÆóÂä†ÊùÉÂπ≥ÂùáÂàÜ‰Ωú‰∏∫ÊÄª‰ΩìÂæóÂàÜ

#  ---------------------------------------------------------

# ËØÑÂàÜË¶ÅÊ±ÇÔºàËØ∑‰øùÊåÅÂêàÁêÜÂÆΩÂÆπÔºâ‚Äî‚ÄîÂÆΩÔºö

# 1. **ËØÑÂàÜÊ†áÂáÜÔºà0-10ÂàÜÔºå‰øùÁïô1‰ΩçÂ∞èÊï∞Ôºâ**Ôºö
#    - 8.0-10.0ÂàÜÔºö‰ºòÁßÄÂà∞ÂÆåÁæé
#    - 7.0-7.9ÂàÜÔºöËâØÂ•ΩÔºåÂ§ßÂ§öÊï∞ÂÜÖÂÆπÂ∫îÂú®Ê≠§Âå∫Èó¥
#    - 6.0-6.9ÂàÜÔºöÂü∫Êú¨ÂêàÊ†º
#    - 5.0ÂàÜ‰ª•‰∏ãÔºöÂ≠òÂú®ÊòéÊòæ‰∏•ÈáçÈóÆÈ¢ò

# 2. **ÂêÑÁª¥Â∫¶ËØÑÂàÜÊåáÂºï**Ôºö
#    - **ÈÄªËæëÊÄß**Ôºà6-8ÂàÜ‰∏∫‰∏ªÔºâÔºö
#      * ÂÜÖÂÆπÂü∫Êú¨ËøûË¥Ø„ÄÅÊúâÂü∫Êú¨‰ø°ÊÅØ ‚Üí 6.5-7.0ÂàÜ
#      * ÂÜÖÂÆπÁªìÊûÑÊ∏ÖÊô∞„ÄÅ‰ø°ÊÅØÂÆåÊï¥ ‚Üí 7.5-8.0ÂàÜ
#      * Âè™ÊúâÂÜÖÂÆπ‰∏•ÈáçÊ∑∑‰π±„ÄÅÊó†Ê≥ïÁêÜËß£Êâç‰Ωé‰∫é6ÂàÜ
   
#    - **Êãü‰∫∫Á®ãÂ∫¶**Ôºà6-8ÂàÜ‰∏∫‰∏ªÔºâÔºö
#      * ËØ≠Ë®ÄËæÉ‰∏∫Ëá™ÁÑ∂„ÄÅË°®ËææÊµÅÁïÖ ‚Üí 6.5-7.0ÂàÜ
#      * Êúâ‰∏™ÊÄßÂåñË°®Ëææ„ÄÅÊÉÖÊÑüËá™ÁÑ∂ ‚Üí 7.5-8.0ÂàÜ
#      * Âè™ÊúâËØ≠Ë®ÄÊûÅÂ∫¶ÁîüÁ°¨„ÄÅÊòéÊòæÊú∫Âô®ÁóïËøπÊâç‰Ωé‰∫é6ÂàÜ
   
#    - **ËßÜËßâÂëàÁé∞**Ôºà7-8ÂàÜ‰∏∫‰∏ªÔºâÔºö
#      * ÂõæÁâáÊ∏ÖÊô∞„ÄÅÊéíÁâàÂêàÁêÜ ‚Üí 7.0-7.5ÂàÜ
#      * ËßÜËßâÂê∏Âºï„ÄÅËÆæËÆ°Á≤æÁæé ‚Üí 7.5-8.5ÂàÜ

# 3. **ËØÑÂàÜÂéüÂàô**Ôºö
#    - ‰øùÊåÅÂêàÁêÜÂÆΩÂÆπÔºå‰∏çË¶ÅËøáÂàÜËãõÂàª
#    - ÂÖ≥Ê≥®Êï¥‰ΩìÂÆåÊàêÂ∫¶ÔºåËΩªÂæÆÁëïÁñµÂèØ‰ª•Êé•Âèó
#    - Â§ßÂ§öÊï∞ÂÜÖÂÆπÂ∫îÂú®6.5-8.0ÂàÜÂå∫Èó¥

        evaluation_instructions = """
ËØÑÂàÜË¶ÅÊ±ÇÔºàËØ∑‰øùÊåÅÂêàÁêÜÂÆΩÂÆπÔºâÔºö

1. **ËØÑÂàÜÊ†áÂáÜÔºà0-10ÂàÜÔºå‰øùÁïô1‰ΩçÂ∞èÊï∞Ôºâ**Ôºö
   - 8.0-10.0ÂàÜÔºö‰ºòÁßÄÂà∞ÂÆåÁæé
   - 7.0-7.9ÂàÜÔºöËâØÂ•ΩÔºåÂ§ßÂ§öÊï∞ÂÜÖÂÆπÂ∫îÂú®Ê≠§Âå∫Èó¥
   - 6.0-6.9ÂàÜÔºöÂü∫Êú¨ÂêàÊ†º
   - 5.0ÂàÜ‰ª•‰∏ãÔºöÂ≠òÂú®ÊòéÊòæ‰∏•ÈáçÈóÆÈ¢ò

2. **ÂêÑÁª¥Â∫¶ËØÑÂàÜÊåáÂºï**Ôºö
   - **ÈÄªËæëÊÄß**Ôºà6-8ÂàÜ‰∏∫‰∏ªÔºâÔºö
     * ÂÜÖÂÆπÂü∫Êú¨ËøûË¥Ø„ÄÅÊúâÂü∫Êú¨‰ø°ÊÅØ ‚Üí 6.5-7.0ÂàÜ
     * ÂÜÖÂÆπÁªìÊûÑÊ∏ÖÊô∞„ÄÅ‰ø°ÊÅØÂÆåÊï¥ ‚Üí 7.5-8.0ÂàÜ
     * Âè™ÊúâÂÜÖÂÆπ‰∏•ÈáçÊ∑∑‰π±„ÄÅÊó†Ê≥ïÁêÜËß£Êâç‰Ωé‰∫é6ÂàÜ
   
   - **Êãü‰∫∫Á®ãÂ∫¶**Ôºà6-8ÂàÜ‰∏∫‰∏ªÔºâÔºö
     * ËØ≠Ë®ÄËæÉ‰∏∫Ëá™ÁÑ∂„ÄÅË°®ËææÊµÅÁïÖ ‚Üí 6.5-7.0ÂàÜ
     * Êúâ‰∏™ÊÄßÂåñË°®Ëææ„ÄÅÊÉÖÊÑüËá™ÁÑ∂ ‚Üí 7.5-8.0ÂàÜ
     * Âè™ÊúâËØ≠Ë®ÄÊûÅÂ∫¶ÁîüÁ°¨„ÄÅÊòéÊòæÊú∫Âô®ÁóïËøπÊâç‰Ωé‰∫é6ÂàÜ
   
   - **ËßÜËßâÂëàÁé∞**Ôºà7-8ÂàÜ‰∏∫‰∏ªÔºâÔºö
     * ÂõæÁâáÊ∏ÖÊô∞„ÄÅÊéíÁâàÂêàÁêÜ ‚Üí 7.0-7.5ÂàÜ
     * ËßÜËßâÂê∏Âºï„ÄÅËÆæËÆ°Á≤æÁæé ‚Üí 7.5-8.5ÂàÜ

3. **ËØÑÂàÜÂéüÂàô**Ôºö
   - ‰øùÊåÅÂêàÁêÜÂÆΩÂÆπÔºå‰∏çË¶ÅËøáÂàÜËãõÂàª
   - ÂÖ≥Ê≥®Êï¥‰ΩìÂÆåÊàêÂ∫¶ÔºåËΩªÂæÆÁëïÁñµÂèØ‰ª•Êé•Âèó
   - Â§ßÂ§öÊï∞ÂÜÖÂÆπÂ∫îÂú®6.5-8.0ÂàÜÂå∫Èó¥
    
ËØ∑‰ª•JSONÊ†ºÂºèËæìÂá∫ÁªìÊûúÔºåÊ†ºÂºèÂ¶Ç‰∏ãÔºö
{
    "dimension_scores": [
        {
            "dimension": "logic",
            "score": 7.5
        },
        {
            "dimension": "visual_presentation",
            "score": 7.8
        },
        {
            "dimension": "human_likeness",
            "score": 7.2
        }
    ],
    "overall_score": 7.5
}

Ê≥®ÊÑèÔºöÂè™ÈúÄË¶ÅËæìÂá∫ËØÑÂàÜÔºå‰∏çÈúÄË¶ÅÁêÜÁî±„ÄÅÂª∫ËÆÆÊàñÂÖ∂‰ªñÊñáÂ≠óËØ¥Êòé„ÄÇ
"""
        
        prompt_parts.append(evaluation_instructions)
        
        return "\n".join(prompt_parts)
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def evaluate_with_ai(self, image_path: str) -> BaselineEvaluationResult:
        """
        Evaluate baseline post using AI
        
        Args:
            image_path: Path to JPG image file
        
        Returns:
            BaselineEvaluationResult object
        """
        # Create prompt
        prompt = self.create_evaluation_prompt()
        
        # Prepare content for API
        messages = [{"role": "user", "content": prompt}]
        
        # Add image if using vision model
        if self.use_vision_model:
            image_base64 = self.image_to_base64(image_path)
            if image_base64:
                messages[0]["content"] = [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": image_base64}
                    }
                ]
        
        # Call AI API
        print("=" * 60)
        print("Calling AI API for evaluation...")
        print(f"Model: {self.model}")
        print(f"Using vision: {self.use_vision_model}")
        if isinstance(messages[0]["content"], list):
            has_image = any(c.get('type') == 'image_url' for c in messages[0]['content'])
            if has_image:
                print(f"Content: Text + Image")
            else:
                print(f"Content: Text only")
        print("=" * 60)
        
        response_text = None
        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            response_text = completion.choices[0].message.content
            
            # Clean response (remove markdown code blocks if any)
            cleaned_response = response_text.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response[7:]
            elif cleaned_response.startswith("```"):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response[:-3]
            cleaned_response = cleaned_response.strip()
            
            # Parse JSON response
            result_dict = json.loads(cleaned_response)
            
            # Convert to BaselineEvaluationResult
            dimension_scores = []
            for dim_data in result_dict.get("dimension_scores", []):
                dimension_scores.append(DimensionScore(
                    dimension=dim_data.get("dimension", ""),
                    score=float(dim_data.get("score", 0.0))
                ))
            
            # Calculate weighted score if not provided
            overall_score = result_dict.get("overall_score")
            if overall_score is None:
                overall_score = self.calculate_weighted_score(dimension_scores)
            
            result = BaselineEvaluationResult(
                image_path=image_path,
                overall_score=float(overall_score),
                dimension_scores=dimension_scores
            )
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"‚ùå JSONËß£ÊûêÈîôËØØ: {e}")
            if response_text:
                print(f"ÂìçÂ∫îÂÜÖÂÆπ: {response_text[:500]}")
            raise
        except Exception as e:
            print(f"‚ùå APIË∞ÉÁî®ÈîôËØØ: {e}")
            if response_text:
                print(f"ÂìçÂ∫îÂÜÖÂÆπ: {response_text[:500] if len(response_text) > 500 else response_text}")
            raise
    
    def calculate_weighted_score(self, dimension_scores: List[DimensionScore]) -> float:
        """Calculate weighted overall score using current dimension weights"""
        total_score = 0.0
        total_weight = 0.0
        
        for dim_score in dimension_scores:
            weight = self.DIMENSION_WEIGHTS.get(dim_score.dimension, 0.0)
            total_score += dim_score.score * weight
            total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0.0


def detect_post_type_from_path(path: str) -> str:
    """
    Detect post type from file path or directory name
    
    Args:
        path: File or directory path
    
    Returns:
        'redbook' or 'hupu'
    """
    path_str = str(path).lower()
    if 'hupu' in path_str or 'discussion_post' in path_str:
        return 'hupu'
    else:
        return 'redbook'


def evaluate_single_baseline(image_path: str, 
                             use_vision: bool = True,
                             post_type: Optional[str] = None) -> Dict:
    """
    Evaluate a single baseline JPG image
    
    Args:
        image_path: Path to JPG image file
        use_vision: Whether to use vision model
        post_type: 'redbook' or 'hupu'. If None, auto-detect from path
    
    Returns:
        Evaluation result dictionary
    """
    image_path_obj = Path(image_path)
    
    if not image_path_obj.exists():
        raise FileNotFoundError(f"Image not found: {image_path}")
    
    # Auto-detect post type if not provided
    if post_type is None:
        post_type = detect_post_type_from_path(str(image_path_obj))
    
    # Create evaluator
    evaluator = BaselineQualityEvaluator(use_vision_model=use_vision, post_type=post_type)
    
    # Evaluate with AI
    print(f"\n{'='*60}")
    print(f"Evaluating: {image_path_obj.name}")
    print(f"{'='*60}\n")
    
    result = evaluator.evaluate_with_ai(str(image_path_obj))
    
    # Convert to dict for saving
    result_dict = {
        'image_path': str(image_path_obj),
        'image_filename': image_path_obj.name,
        'post_type': post_type,
        'overall_score': result.overall_score,
        'dimension_scores': [
            {
                'dimension': ds.dimension,
                'dimension_name': evaluator.DIMENSIONS.get(ds.dimension, {}).get('name', ds.dimension),
                'score': ds.score
            }
            for ds in result.dimension_scores
        ]
    }
    
    print(f"ÊÄª‰ΩìÂæóÂàÜ: {result.overall_score:.2f}/10.0")
    
    return result_dict


def find_baseline_images(product_dir: Path, post_type: str = 'redbook') -> List[Path]:
    """
    Find baseline JPG images in a product directory
    
    Args:
        product_dir: Product directory path
        post_type: 'redbook' or 'hupu' (determines file pattern)
    
    Returns:
        List of JPG image file paths
    """
    if post_type == 'hupu':
        # Look for discussion_post_*.jpg files (Hupu format)
        jpg_files = list(product_dir.glob("discussion_post_*.jpg"))
    else:
        # Look for it_product_*.jpg files (Redbook format)
        jpg_files = list(product_dir.glob("it_product_*.jpg"))
    
    # Also check for any .jpg files if no pattern-matched files found
    if not jpg_files:
        jpg_files = list(product_dir.glob("*.jpg"))
    
    return sorted(jpg_files, key=lambda x: x.name)


def evaluate_baseline_product(product_dir: Path,
                              use_vision: bool = True,
                              max_workers: int = 5,
                              skip_if_summary_exists: bool = True,
                              post_type: Optional[str] = None) -> Dict:
    """
    Evaluate baseline images in a single product directory
    
    Args:
        product_dir: Product directory path
        use_vision: Whether to use vision model
        max_workers: Maximum concurrent workers
        skip_if_summary_exists: Whether to skip if summary file already exists
        post_type: 'redbook' or 'hupu'. If None, auto-detect from directory path
    
    Returns:
        Dictionary with evaluation results
    """
    # Auto-detect post type if not provided
    if post_type is None:
        post_type = detect_post_type_from_path(str(product_dir))
    
    # Check if summary already exists (resume mechanism)
    summary_file = product_dir / "baseline_evaluation_summary.json"
    if skip_if_summary_exists and summary_file.exists():
        try:
            with open(summary_file, 'r', encoding='utf-8') as f:
                existing_summary = json.load(f)
            print(f"\n{'='*60}")
            print(f"‰∫ßÂìÅ: {product_dir.name}")
            print(f"‚è≠Ô∏è  Â∑≤Â≠òÂú®ËØÑ‰ª∑Ê±áÊÄªÔºåË∑≥ËøáËØÑ‰ª∑")
            print(f"   Âπ≥ÂùáÂæóÂàÜ: {existing_summary.get('average_score', 0):.2f}/10.0")
            print(f"   Â∑≤ËØÑ‰ª∑ÂõæÁâá: {existing_summary.get('successful', 0)}/{existing_summary.get('total_images', 0)}")
            print(f"{'='*60}")
            return existing_summary
        except Exception as e:
            print(f"‚ö†Ô∏è  ËØªÂèñÂ∑≤ÊúâÊ±áÊÄªÊñá‰ª∂Â§±Ë¥•: {e}ÔºåÂ∞ÜÈáçÊñ∞ËØÑ‰ª∑")
    
    jpg_files = find_baseline_images(product_dir, post_type=post_type)
    
    if not jpg_files:
        return {
            'product_dir': str(product_dir),
            'product_id': product_dir.name,
            'images': [],
            'error': 'No JPG files found'
        }
    
    print(f"\n{'='*60}")
    print(f"‰∫ßÂìÅ: {product_dir.name}")
    print(f"Á±ªÂûã: {post_type.upper()}")
    print(f"ÊâæÂà∞ {len(jpg_files)} ‰∏™ÂõæÁâáÊñá‰ª∂")
    print(f"{'='*60}")
    
    results = {}
    errors = {}
    
    # Process files with limited concurrency to avoid API rate limits
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_file = {
            executor.submit(
                evaluate_single_baseline,
                str(jpg_file),
                use_vision=use_vision,
                post_type=post_type
            ): jpg_file
            for jpg_file in jpg_files
        }
        
        # Process results as they complete
        for future in as_completed(future_to_file):
            jpg_file = future_to_file[future]
            image_name = jpg_file.stem  # e.g., "it_product_0"
            
            try:
                result = future.result()
                results[image_name] = result
                print(f"  ‚úÖ {jpg_file.name}: {result.get('overall_score', 0):.2f}/10.0")
            except Exception as e:
                error_msg = str(e)
                errors[image_name] = error_msg
                print(f"  ‚ùå {jpg_file.name}: {error_msg}")
    
    # Compile summary
    summary = {
        'product_dir': str(product_dir),
        'product_id': product_dir.name,
        'post_type': post_type,
        'evaluation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'total_images': len(jpg_files),
        'successful': len(results),
        'failed': len(errors),
        'images': {}
    }
    
    # Add image results
    for image_name, result in results.items():
        summary['images'][image_name] = {
            'image_file': result.get('image_filename', ''),
            'overall_score': result.get('overall_score', 0.0),
            'dimension_scores': {
                ds['dimension']: {
                    'name': ds['dimension_name'],
                    'score': ds['score']
                }
                for ds in result.get('dimension_scores', [])
            }
        }
    
    # Add errors
    if errors:
        summary['errors'] = errors
    
    # Calculate statistics
    if results:
        scores = [r.get('overall_score', 0) for r in results.values()]
        summary['average_score'] = sum(scores) / len(scores)
        summary['max_score'] = max(scores)
        summary['min_score'] = min(scores)
        
        # Find best image
        best_image = max(results.items(), key=lambda x: x[1].get('overall_score', 0))
        summary['best_image'] = {
            'name': best_image[0],
            'score': best_image[1].get('overall_score', 0)
        }
    
    # Save summary to product directory
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\n  üíæ ËØÑ‰ª∑Ê±áÊÄªÂ∑≤‰øùÂ≠òÂà∞: {summary_file}")
    if results:
        print(f"  üìä Âπ≥ÂùáÂæóÂàÜ: {summary['average_score']:.2f}/10.0")
        best_name = summary['best_image']['name']
        best_score = summary['best_image']['score']
        print(f"  üèÜ ÊúÄ‰Ω≥ÂõæÁâá: {best_name} ({best_score:.2f}/10.0)")
    
    return summary


def batch_evaluate_all_baselines(base_dir: str,
                                 use_vision: bool = True,
                                 max_workers_per_product: int = 5,
                                 max_concurrent_products: int = 8,
                                 skip_if_summary_exists: bool = True,
                                 post_type: Optional[str] = None) -> List[Dict]:
    """
    Batch evaluate all baseline products in a directory
    
    Args:
        base_dir: Base directory containing product directories (e.g., generated_redbook_baseline or generated_hupu_baseline)
        use_vision: Whether to use vision model
        max_workers_per_product: Max concurrent images per product
        max_concurrent_products: Max concurrent products (to avoid overwhelming API)
        skip_if_summary_exists: Whether to skip products that already have summary files
        post_type: 'redbook' or 'hupu'. If None, auto-detect from base_dir name
    
    Returns:
        List of product evaluation summaries
    """
    base_path = Path(base_dir)
    
    if not base_path.exists():
        print(f"‚ùå ÁõÆÂΩï‰∏çÂ≠òÂú®: {base_dir}")
        return []
    
    # Auto-detect post type if not provided
    if post_type is None:
        post_type = detect_post_type_from_path(base_dir)
    
    # Find all product directories (format: {index}_{user_id})
    product_dirs = [d for d in base_path.iterdir() if d.is_dir() and not d.name.startswith('.')]
    product_dirs.sort()
    
    if not product_dirs:
        print(f"‚ùå Âú® {base_dir} ‰∏≠Êú™ÊâæÂà∞‰∫ßÂìÅÁõÆÂΩï")
        return []
    
    # Check how many products already have evaluation summaries
    existing_summaries = []
    for product_dir in product_dirs:
        summary_file = product_dir / "baseline_evaluation_summary.json"
        if summary_file.exists():
            existing_summaries.append(product_dir)
    
    # Ask user if they want to re-evaluate all (if summaries exist and not forced to skip)
    should_re_evaluate = False
    if existing_summaries and skip_if_summary_exists:
        print(f"\n{'='*70}")
        print(f"üìã Ê£ÄÊµãÂà∞Â∑≤ÊúâËØÑ‰ª∑Ê±áÊÄª")
        print(f"{'='*70}")
        print(f"ÊÄª‰∫ßÂìÅÊï∞: {len(product_dirs)}")
        print(f"Â∑≤ÊúâÊ±áÊÄª: {len(existing_summaries)} ‰∏™‰∫ßÂìÅ")
        print(f"ÈúÄË¶ÅËØÑ‰ª∑: {len(product_dirs) - len(existing_summaries)} ‰∏™‰∫ßÂìÅ")
        print(f"{'='*70}")
        
        # Ask user for confirmation (non-interactive mode: default to skip)
        try:
            user_input = input("\n‚ùì ÊòØÂê¶ÈáçÊñ∞ËØÑ‰º∞ÂÖ®ÈÉ®‰∫ßÂìÅÔºü(y/nÔºåÈªòËÆ§n): ").strip().lower()
            if user_input in ['y', 'yes', 'ÊòØ', 'Y']:
                should_re_evaluate = True
                print("‚úÖ Â∞ÜÈáçÊñ∞ËØÑ‰º∞ÂÖ®ÈÉ®‰∫ßÂìÅÔºàË¶ÜÁõñÂ∑≤ÊúâÊ±áÊÄªÔºâ")
            else:
                print("‚è≠Ô∏è  Â∞ÜË∑≥ËøáÂ∑≤ÊúâÊ±áÊÄªÁöÑ‰∫ßÂìÅÔºåÂè™ËØÑ‰º∞Êñ∞‰∫ßÂìÅ")
        except (EOFError, KeyboardInterrupt):
            # Non-interactive mode (e.g., script running in background)
            print("‚è≠Ô∏è  Èùû‰∫§‰∫íÊ®°ÂºèÔºöÂ∞ÜË∑≥ËøáÂ∑≤ÊúâÊ±áÊÄªÁöÑ‰∫ßÂìÅ")
            should_re_evaluate = False
    
    # Update skip_if_summary_exists based on user choice
    if should_re_evaluate:
        skip_if_summary_exists = False
    
    print(f"\n{'='*70}")
    print(f"üöÄ ÂºÄÂßãÊâπÈáèËØÑ‰ª∑ Baseline {post_type.upper()} Â∏ñÂ≠ê")
    print(f"üìÅ Âü∫Á°ÄÁõÆÂΩï: {base_dir}")
    print(f"üì¶ ‰∫ßÂìÅÊï∞Èáè: {len(product_dirs)}")
    print(f"‚öôÔ∏è  ÊØè‰∏™‰∫ßÂìÅÊúÄÂ§ßÂπ∂Âèë: {max_workers_per_product}")
    print(f"‚öôÔ∏è  ÊúÄÂ§ßÂπ∂Âèë‰∫ßÂìÅÊï∞: {max_concurrent_products}")
    print(f"üîÑ Ë¶ÜÁõñÊ®°Âºè: {'ÊòØÔºàÈáçÊñ∞ËØÑ‰º∞ÂÖ®ÈÉ®Ôºâ' if not skip_if_summary_exists else 'Âê¶ÔºàË∑≥ËøáÂ∑≤ÊúâÊ±áÊÄªÔºâ'}")
    print(f"{'='*70}\n")
    
    all_summaries = []
    
    # Process products with limited concurrency
    with ThreadPoolExecutor(max_workers=max_concurrent_products) as executor:
        # Submit all product tasks
        future_to_product = {
            executor.submit(
                evaluate_baseline_product,
                product_dir,
                use_vision=use_vision,
                max_workers=max_workers_per_product,
                skip_if_summary_exists=skip_if_summary_exists,
                post_type=post_type
            ): product_dir
            for product_dir in product_dirs
        }
        
        # Process results as they complete
        completed = 0
        for future in as_completed(future_to_product):
            product_dir = future_to_product[future]
            completed += 1
            
            try:
                summary = future.result()
                all_summaries.append(summary)
                status = "‚úÖ" if summary.get('successful', 0) > 0 else "‚ö†Ô∏è"
                print(f"\n{status} [{completed}/{len(product_dirs)}] {product_dir.name}: "
                      f"{summary.get('successful', 0)}/{summary.get('total_images', 0)} ÊàêÂäü")
            except Exception as e:
                print(f"\n‚ùå [{completed}/{len(product_dirs)}] {product_dir.name}: ÈîôËØØ - {e}")
                all_summaries.append({
                    'product_dir': str(product_dir),
                    'product_id': product_dir.name,
                    'error': str(e)
                })
    
    # Print final summary
    print(f"\n{'='*70}")
    print(f"üéâ ÊâπÈáèËØÑ‰ª∑ÂÆåÊàêÔºÅ")
    print(f"{'='*70}")
    print(f"üìä ÊÄªËÆ°: {len(all_summaries)} ‰∏™‰∫ßÂìÅ")
    
    successful_products = [s for s in all_summaries if s.get('successful', 0) > 0]
    if successful_products:
        total_images = sum(s.get('total_images', 0) for s in successful_products)
        total_successful = sum(s.get('successful', 0) for s in successful_products)
        avg_scores = [s.get('average_score', 0) for s in successful_products if 'average_score' in s]
        
        print(f"‚úÖ ÊàêÂäüËØÑ‰ª∑: {len(successful_products)} ‰∏™‰∫ßÂìÅ")
        print(f"üìÑ ÊÄªÂõæÁâáÊï∞: {total_images}")
        print(f"‚úÖ ÊàêÂäüÂõæÁâá: {total_successful}")
        if avg_scores:
            print(f"üìä Âπ≥ÂùáÂæóÂàÜ: {sum(avg_scores) / len(avg_scores):.2f}/10.0")
    
    # Generate global statistics
    global_stats = generate_global_statistics(all_summaries, base_path)
    if global_stats:
        print(f"\n{'='*70}")
        print(f"üìà ÂÖ®Â±ÄÁªüËÆ°Â∑≤‰øùÂ≠ò")
        print(f"{'='*70}")
        print(f"üìä ÊÄªÂàÜÂùáÂÄº: {global_stats.get('overall_score_mean', 0):.2f}/10.0")
        print(f"üìä ÂêÑÁª¥Â∫¶ÂùáÂÄº:")
        for dim_id, dim_stat in global_stats.get('dimension_means', {}).items():
            dim_name = dim_stat.get('name', dim_id)
            dim_mean = dim_stat.get('mean', 0)
            print(f"   - {dim_name}: {dim_mean:.2f}/10.0")
        print(f"{'='*70}\n")
    
    return all_summaries


def generate_global_statistics(all_summaries: List[Dict], base_path: Path) -> Optional[Dict]:
    """
    Generate global statistics from all evaluation summaries
    
    Args:
        all_summaries: List of product evaluation summaries
        base_path: Base directory path for saving statistics
    
    Returns:
        Dictionary with global statistics, or None if no valid data
    """
    # Collect all dimension scores and overall scores
    dimension_scores_collection = {
        'logic': [],
        'visual_presentation': [],
        'human_likeness': []
    }
    overall_scores = []
    
    # Iterate through all products
    for summary in all_summaries:
        if 'images' not in summary:
            continue
        
        # Iterate through all images in this product
        for image_name, image_data in summary.get('images', {}).items():
            overall_score = image_data.get('overall_score', 0)
            if overall_score > 0:  # Only count valid scores
                overall_scores.append(overall_score)
            
            # Collect dimension scores
            dim_scores = image_data.get('dimension_scores', {})
            for dim_id in dimension_scores_collection.keys():
                if dim_id in dim_scores:
                    dim_score = dim_scores[dim_id].get('score', 0)
                    if dim_score > 0:  # Only count valid scores
                        dimension_scores_collection[dim_id].append(dim_score)
    
    # Calculate means
    if not overall_scores:
        print("‚ö†Ô∏è  Ê≤°ÊúâÊúâÊïàÁöÑËØÑ‰ª∑Êï∞ÊçÆÔºåÊó†Ê≥ïÁîüÊàêÂÖ®Â±ÄÁªüËÆ°")
        return None
    
    # Overall score mean
    overall_mean = sum(overall_scores) / len(overall_scores)
    
    # Dimension means
    dimension_means = {}
    # Use redbook evaluator for dimension names (they're the same)
    evaluator = BaselineQualityEvaluator(post_type='redbook')
    for dim_id, scores in dimension_scores_collection.items():
        if scores:
            dim_mean = sum(scores) / len(scores)
            dim_name = evaluator.DIMENSIONS.get(dim_id, {}).get('name', dim_id)
            dimension_means[dim_id] = {
                'name': dim_name,
                'mean': dim_mean,
                'count': len(scores)
            }
    
    # Build statistics dictionary
    global_stats = {
        'evaluation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'total_products': len(all_summaries),
        'successful_products': len([s for s in all_summaries if s.get('successful', 0) > 0]),
        'total_images_evaluated': len(overall_scores),
        'overall_score_mean': overall_mean,
        'overall_score_max': max(overall_scores),
        'overall_score_min': min(overall_scores),
        'dimension_means': dimension_means,
        'dimension_statistics': {}
    }
    
    # Add detailed dimension statistics
    for dim_id, scores in dimension_scores_collection.items():
        if scores:
            dim_name = evaluator.DIMENSIONS.get(dim_id, {}).get('name', dim_id)
            global_stats['dimension_statistics'][dim_id] = {
                'name': dim_name,
                'mean': sum(scores) / len(scores),
                'max': max(scores),
                'min': min(scores),
                'count': len(scores)
            }
    
    # Save to JSON file
    stats_file = base_path / "baseline_global_statistics.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(global_stats, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ ÂÖ®Â±ÄÁªüËÆ°Â∑≤‰øùÂ≠òÂà∞: {stats_file}")
    
    return global_stats


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate baseline post quality (JPG images)")
    parser.add_argument("path", help="Path to JPG file, product directory, or base directory (generated_redbook_baseline or generated_hupu_baseline)")
    parser.add_argument("--no-vision", action="store_true", help="Don't use vision model")
    parser.add_argument("--batch-all", action="store_true", help="Batch evaluate all products in base directory")
    parser.add_argument("--max-workers", type=int, default=5, help="Max concurrent workers per product")
    parser.add_argument("--max-products", type=int, default=8, help="Max concurrent products")
    parser.add_argument("--no-resume", action="store_true", help="Don't skip products with existing summary files (re-evaluate all)")
    parser.add_argument("--post-type", choices=['redbook', 'hupu'], help="Post type (auto-detected from path if not specified)")
    
    args = parser.parse_args()
    
    path = Path(args.path)
    
    # Auto-detect post type if not specified
    post_type = args.post_type
    if post_type is None:
        post_type = detect_post_type_from_path(str(path))
    
    if args.batch_all:
        # Batch evaluate all products in base directory
        batch_evaluate_all_baselines(
            str(path),
            use_vision=not args.no_vision,
            max_workers_per_product=args.max_workers,
            max_concurrent_products=args.max_products,
            skip_if_summary_exists=not args.no_resume,
            post_type=post_type
        )
    elif path.is_file():
        # Single file
        result = evaluate_single_baseline(
            str(path),
            use_vision=not args.no_vision,
            post_type=post_type
        )
        
        print(f"\n‚úÖ ËØÑ‰ª∑ÂÆåÊàêÔºÅ")
        print(f"ÊÄª‰ΩìÂæóÂàÜ: {result['overall_score']:.2f}/10.0")
        
    elif path.is_dir():
        # Check if it's a product directory (has JPG files) or base directory
        jpg_files = list(path.glob("*.jpg"))
        subdirs = [d for d in path.iterdir() if d.is_dir()]
        
        if jpg_files and not subdirs:
            # Product directory - evaluate all images
            evaluate_baseline_product(
                path,
                use_vision=not args.no_vision,
                max_workers=args.max_workers,
                skip_if_summary_exists=not args.no_resume,
                post_type=post_type
            )
        else:
            # Base directory - batch evaluate all products
            batch_evaluate_all_baselines(
                str(path),
                use_vision=not args.no_vision,
                max_workers_per_product=args.max_workers,
                max_concurrent_products=args.max_products,
                skip_if_summary_exists=not args.no_resume,
                post_type=post_type
            )
    else:
        print(f"‚ùå Ë∑ØÂæÑ‰∏çÂ≠òÂú®: {path}")

