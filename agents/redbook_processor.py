import os
import sys
import json
import glob
from datetime import datetime
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# Add agents directory to path if running from project root
script_dir = os.path.dirname(os.path.abspath(__file__))
if script_dir not in sys.path:
    sys.path.insert(0, script_dir)

from text_analyst import TextAnalyst
from image_analyst import ImageAnalyst
from video_analyst import VideoAnalyst
from user_profile_generator import UserProfileGenerator
from profile2idea_it import Profile2Idea
from itproduct_generator import ITProductGenerator, AIRefusalError
from redbook_english import RedBookEnglishConverter


class RedBookProcessor:
    def __init__(self, data_root="download/redbook", output_base="generated_redbook_it"):
        self.data_root = data_root
        self.output_base = output_base
        os.makedirs(output_base, exist_ok=True)

    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        os.environ.update({
            "CHAT_API_KEY": "sk-dVaSXmTEMBh0Gygx49ResSvaONvErml5QV8McBAGkbPmX2mG",
            "CHAT_BASE_URL": "https://yunwu.ai/v1",
            "CHAT_MODEL": "gpt-4o",

            "IMAGE_API_KEY": "sk-dVaSXmTEMBh0Gygx49ResSvaONvErml5QV8McBAGkbPmX2mG",
            "IMAGE_BASE_URL": "https://yunwu.ai/v1",
            "IMAGE_MODEL": "gpt-4-vision-preview",

            "VIDEO_API_KEY": "sk-dVaSXmTEMBh0Gygx49ResSvaONvErml5QV8McBAGkbPmX2mG",
            "VIDEO_BASE_URL": "https://yunwu.ai/v1",
            "VIDEO_MODEL": "gpt-4o",

            "GENERATE_API_KEY": "sk-W7qxtvbQUxwIo9PLlSGh89cUKKuTTo1oUXmqpGoYIhqQULjI",
            "GENERATE_BASE_URL": "https://yunwu.ai/v1beta",
            "GENERATE_MODEL": "gemini-2.5-flash-image-preview",

            "IDEA_API_KEY": "sk-dVaSXmTEMBh0Gygx49ResSvaONvErml5QV8McBAGkbPmX2mG",
            "IDEA_BASE_URL": "https://yunwu.ai/v1",
            "IDEA_MODEL": "gpt-4o",
        })

    def get_available_users(self) -> List[str]:
        users = []
        if os.path.exists(self.data_root):
            for name in os.listdir(self.data_root):
                user_path = os.path.join(self.data_root, name)
                if os.path.isdir(os.path.join(user_path, "historical")):
                    users.append(name)
        return sorted(users)


    def load_user_data(self, user_id: str) -> Optional[Dict]:
        user_path = os.path.join(self.data_root, user_id)
        historical_path = os.path.join(user_path, "historical")

        if not os.path.isdir(historical_path):
            print(f"historical ç›®å½•ä¸å­˜åœ¨: {historical_path}")
            return None

        items = []
        for item_dir in os.listdir(historical_path):
            item_path = os.path.join(historical_path, item_dir)
            if os.path.isdir(item_path):
                item_data = self._parse_item(historical_path, item_dir)
                if item_data:
                    items.append(item_data)

        return {
            "user_id": user_id,
            "total_items": len(items),
            "items": items
        }


    def _parse_item(self, user_path: str, item_dir: str) -> Optional[Dict]:
        """è§£æå•ä¸ªæ”¶è—é¡¹"""
        item_path = os.path.join(user_path, item_dir)
        base_name = '_'.join(item_dir.split('_')[1:])  # å»æ‰itemX_å‰ç¼€

        # æŸ¥æ‰¾æ–‡æœ¬æ–‡ä»¶
        text_files = glob.glob(os.path.join(item_path, "*.txt"))
        text_content = ""
        if text_files:
            try:
                with open(text_files[0], 'r', encoding='utf-8') as f:
                    text_content = f.read().strip()
            except:
                print(f"æ— æ³•è¯»å–æ–‡æœ¬æ–‡ä»¶: {text_files[0]}")

        # æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶ - ä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…
        image_files = []

        # ä¼˜å…ˆåŒ¹é…å¸¦_imgç¼–å·çš„å›¾ç‰‡
        numbered_images = glob.glob(os.path.join(item_path, "*_img[0-9]*.jpg")) + \
                        glob.glob(os.path.join(item_path, "*_img[0-9]*.png"))

        # åŒ¹é…å…¶ä»–å›¾ç‰‡æ–‡ä»¶ï¼Œä½†æ’é™¤ç¼©ç•¥å›¾å’Œå°å›¾
        other_images = glob.glob(os.path.join(item_path, "*.jpg")) + \
                    glob.glob(os.path.join(item_path, "*.png"))

        # è¿‡æ»¤æ‰å¯èƒ½çš„é‡å¤å’Œç¼©ç•¥å›¾
        all_images = set()  # ä½¿ç”¨setå»é‡

        for img_path in numbered_images + other_images:
            # æ’é™¤å¸¸è§çš„ç¼©ç•¥å›¾å‘½å
            if any(thumb in os.path.basename(img_path).lower() for thumb in ['thumb', 'small', 'mini', '_s.']):
                continue

            # æ’é™¤_img0ï¼ˆé€šå¸¸æ˜¯å°é¢ç¼©ç•¥å›¾ï¼‰
            if '_img0.' in img_path.lower():
                continue

            # ä½¿ç”¨ç»å¯¹è·¯å¾„é¿å…é‡å¤
            abs_path = os.path.abspath(img_path)
            all_images.add(abs_path)

        image_files = list(all_images)

        # æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
        video_files = glob.glob(os.path.join(item_path, "*.mp4")) + \
                    glob.glob(os.path.join(item_path, "*.mov"))

        if not text_content and not image_files and not video_files:
            return None

        return {
            "item_id": item_dir,
            "item_name": base_name,
            "text_content": text_content,
            "image_files": image_files,
            "video_files": video_files,
            "item_path": item_path
        }

    def process_user(self, user_id: str, max_workers: int = 4, user_index: Optional[int] = None, generate_english: bool = True) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªç”¨æˆ·æ•°æ®å¹¶ç”Ÿæˆä¸ªæ€§åŒ–å¸–å­"""
        print(f"\n{'='*50}")
        print(f"å¼€å§‹å¤„ç†ç”¨æˆ· [{user_index}] {user_id}" if user_index is not None else f"å¼€å§‹å¤„ç†ç”¨æˆ· {user_id}")
        print(f"{'='*50}")

        # åŠ è½½ç”¨æˆ·æ•°æ® â€”â€”â€”â€” å› ä¸ºå·²ç”Ÿæˆprofileï¼Œæš‚ä¸”å…³é—­
        # user_data = self.load_user_data(user_id)
        # if not user_data or user_data["total_items"] == 0:
        #     print(f"ç”¨æˆ· {user_id} æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡å¤„ç†")
        #     return None

        # print(f"æ‰¾åˆ° {user_data['total_items']} ä¸ªæ”¶è—é¡¹")

        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¸¦åºå·ï¼‰
        if user_index is not None:
            output_dir_name = f"{user_index}_{user_id}"
        else:
            output_dir_name = user_id
        user_output_dir = os.path.join(self.output_base, output_dir_name)
        os.makedirs(user_output_dir, exist_ok=True)

        # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥ï¼šå¦‚æœfinal_results.jsonå·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†
        final_results_path = os.path.join(user_output_dir, "final_results.json")
        if os.path.exists(final_results_path):
            try:
                # éªŒè¯æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆï¼ˆåŒ…å«å¿…è¦å­—æ®µï¼‰
                with open(final_results_path, 'r', encoding='utf-8') as f:
                    existing_result = json.load(f)
                    if existing_result.get("personalized_post") or existing_result.get("discussion_post"):
                        print(f"âœ… æ£€æµ‹åˆ°å·²å­˜åœ¨çš„final_results.jsonï¼Œè·³è¿‡å¤„ç†ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰")
                        print(f"   ğŸ“„ æ–‡ä»¶è·¯å¾„: {final_results_path}")
                        return existing_result
            except (json.JSONDecodeError, Exception) as e:
                print(f"âš ï¸  æ£€æµ‹åˆ°final_results.jsonä½†è§£æå¤±è´¥: {e}ï¼Œå°†é‡æ–°ç”Ÿæˆ")
                # ç»§ç»­å¤„ç†ï¼Œè¦†ç›–æ—§æ–‡ä»¶

        temp_files = []  # è®°å½•æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶è·¯å¾„

        
        try:
            # ===== analysis =====
            # # åˆ†ææ–‡æœ¬å†…å®¹
            # text_files = []
            # for item in user_data["items"]:
            #     if item["text_content"]:
            #         # å°†æ–‡æœ¬å†…å®¹ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶ä¾›åˆ†æ
            #         text_file = os.path.join(user_output_dir, f"temp_{item['item_id']}.txt")
            #         with open(text_file, "w", encoding="utf-8") as f:
            #             f.write(item["text_content"])
            #         text_files.append(text_file)
            #         temp_files.append(text_file)  # è®°å½•ä¸´æ—¶æ–‡ä»¶

            # if text_files:
            #     print("åˆ†ææ–‡æœ¬å†…å®¹...")
            #     text_analyst = TextAnalyst(max_workers=max_workers)
            #     text_analyst(text_files, user_output_dir)
            # else:
            #     print("æ— æ–‡æœ¬å†…å®¹å¯åˆ†æ")

            # # åˆ†æå›¾ç‰‡å†…å®¹
            # image_files = []
            # for item in user_data["items"]:
            #     image_files.extend(item["image_files"])

            # if image_files:
            #     print(f"åˆ†æ {len(image_files)} å¼ å›¾ç‰‡...")
            #     image_analyst = ImageAnalyst(max_workers=max_workers)
            #     image_analyst(image_files, user_output_dir)
            # else:
            #     print("æ— å›¾ç‰‡å¯åˆ†æ")

            # åˆ†æè§†é¢‘å†…å®¹ --------------------- æš‚ä¸”å…³é—­
            # video_files = []
            # for item in user_data["items"]:
            #     video_files.extend(item["video_files"])

            # if video_files:
            #     print(f"åˆ†æ {len(video_files)} ä¸ªè§†é¢‘...")
            #     video_analyst = VideoAnalyst(max_workers=max_workers)
            #     video_analyst(video_files, user_output_dir)
            # else:
            #     print("æ— è§†é¢‘å¯åˆ†æ")


            # # ===== analysis -> profile =====
            # print("ç”Ÿæˆç”¨æˆ·ç”»åƒ...")
            # analysis_data = self._combine_analysis_results(user_output_dir)

            # if not analysis_data.strip():
            #     print("æ— åˆ†ææ•°æ®ï¼Œæ— æ³•ç”Ÿæˆç”¨æˆ·ç”»åƒ")
            #     return None

            # profile_generator = UserProfileGenerator()
            # user_profile = profile_generator(analysis_data)
    

            # ===== ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ user_profile.txt =====
            print("è¯»å–å·²æœ‰çš„ç”¨æˆ·ç”»åƒ...")
            
            # ä» download/redbook/{user_id}/user_profile.txt è¯»å–
            source_profile_path = os.path.join(self.data_root, user_id, "user_profile.txt")
            
            if not os.path.exists(source_profile_path):
                print(f"âŒ æœªæ‰¾åˆ°ç”¨æˆ·ç”»åƒæ–‡ä»¶: {source_profile_path}")
                print("   å°è¯•ç”Ÿæˆæ–°çš„ç”¨æˆ·ç”»åƒ...")
                
                # å¦‚æœä¸å­˜åœ¨ï¼Œå›é€€åˆ°åŸæ¥çš„ç”Ÿæˆé€»è¾‘
                analysis_data = self._combine_analysis_results(user_output_dir)
                if not analysis_data.strip():
                    print("æ— åˆ†ææ•°æ®ï¼Œæ— æ³•ç”Ÿæˆç”¨æˆ·ç”»åƒ")
                    return None
                    
                profile_generator = UserProfileGenerator()
                user_profile_text = profile_generator(analysis_data)
                
                # Extract first preference only
                first_preference = self._extract_first_preference(user_profile_text)
                
                # è½¬æ¢ä¸º JSON æ ¼å¼
                user_profile = {
                    "user_id": user_id,
                    "profile_text": first_preference,
                    "source": "generated",
                    "timestamp": datetime.now().isoformat()
                }
            else:
                # è¯»å–å·²æœ‰çš„ user_profile.txt
                with open(source_profile_path, "r", encoding="utf-8") as f:
                    user_profile_text = f.read().strip()
                
                print(f"âœ… æˆåŠŸè¯»å–ç”¨æˆ·ç”»åƒ (é•¿åº¦: {len(user_profile_text)} å­—ç¬¦)")
                
                # Extract first preference only
                first_preference = self._extract_first_preference(user_profile_text)
                
                # è½¬æ¢ä¸º JSON æ ¼å¼
                user_profile = {
                    "user_id": user_id,
                    "profile_text": first_preference,
                    "source": source_profile_path,
                    "timestamp": datetime.now().isoformat()
                }

            # ä¿å­˜ç”¨æˆ·ç”»åƒåˆ° generated_it/{user_id}/profile.json
            profile_path = os.path.join(user_output_dir, "profile.json")
            with open(profile_path, "w", encoding="utf-8") as f:
                json.dump(user_profile, f, ensure_ascii=False, indent=2)
            
            print(f"   ç”»åƒå·²ä¿å­˜åˆ°: {profile_path}")

            # ===== profile -> ideaï¼ˆåªä½¿ç”¨ Text-Image Contentï¼‰=====
            print("æ ¹æ®ç”¨æˆ·ç”»åƒç”Ÿæˆå†…å®¹åˆ›æ„ï¼ˆText-Image Contentï¼‰...")

            profile2idea = Profile2Idea()

            # å‡†å¤‡ä¼ ç»™ profile2idea çš„å†…å®¹
            if isinstance(user_profile, dict) and "profile_text" in user_profile:
                # å¦‚æœæ˜¯ä» user_profile.txt è¯»å–çš„ï¼Œä½¿ç”¨ profile_text
                profile_content = user_profile["profile_text"]
            elif isinstance(user_profile, dict):
                # å¦‚æœæ˜¯å­—å…¸ä½†æ²¡æœ‰ profile_textï¼Œè½¬ä¸º JSON å­—ç¬¦ä¸²
                profile_content = json.dumps(user_profile, ensure_ascii=False, indent=2)
            else:
                # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                profile_content = user_profile

            raw_ideas = profile2idea(
                user_profile=profile_content,
                user_dir=user_output_dir
            )

            ideas = json.loads(raw_ideas)

            # åªä¿ç•™ Text-Image Content
            ideas = [
                idea for idea in ideas
                if idea.get("main_type") == "Text-Image Content"
            ]

            if not ideas:
                raise ValueError("âŒ æœªç”Ÿæˆä»»ä½• Text-Image Content ç±»å‹çš„ ideas")

            # åªä½¿ç”¨ç¬¬ä¸€ä¸ª idea
            if len(ideas) > 1:
                print(f"âœ… ç”Ÿæˆ {len(ideas)} ä¸ª Text-Image Content ideasï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ª")
                ideas = [ideas[0]]
            else:
                print(f"âœ… ç”Ÿæˆ {len(ideas)} ä¸ª Text-Image Content idea")

            # ===== idea -> post =====
            # ç”Ÿæˆä¸ªæ€§åŒ–å¸–å­
            print("ç”Ÿæˆä¸ªæ€§åŒ–å¸–å­...")
            content_generator = ITProductGenerator(ideas)
            
            try:
                # æ­£ç¡®çš„å‚æ•°é¡ºåºï¼šuser_profile, output_dir, user_profile_path
                personalized_post = content_generator(user_profile, user_output_dir, profile_path)
            except AIRefusalError as e:
                print(f"\n{'='*50}")
                print(f"âš ï¸  AIè¿ç»­æ‹’ç»ç”Ÿæˆå†…å®¹ï¼Œè·³è¿‡ç”¨æˆ· {user_id}")
                print(f"{'='*50}\n")
                return None

            # Save intermediate results (without english_post) for English converter to read
            intermediate_result = {
                "user_id": user_id,
                "user_profile": user_profile,
                "personalized_post": personalized_post
            }
            
            with open(os.path.join(user_output_dir, "final_results.json"), "w", encoding="utf-8") as f:
                json.dump(intermediate_result, f, ensure_ascii=False, indent=2)

            # ===== Auto-generate English version =====
            english_post = None
            if generate_english:
                try:
                    print("\nğŸŒ ç”Ÿæˆè‹±æ–‡ç‰ˆå¸–å­ï¼ˆå›½é™…ç¤¾äº¤åª’ä½“é£æ ¼ï¼‰...")
                    
                    # åˆ›å»ºè‹±æ–‡è½¬æ¢å™¨
                    english_converter = RedBookEnglishConverter(generated_dir=self.output_base)
                    
                    # è½¬æ¢ä¸ºè‹±æ–‡
                    user_dir_name = output_dir_name  # ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„ç›®å½•å
                    english_post = english_converter.convert_post(user_dir_name)
                    
                    if english_post:
                        print(f"âœ… è‹±æ–‡ç‰ˆç”ŸæˆæˆåŠŸï¼")
                        print(f"ğŸ“„ è‹±æ–‡HTML: {english_post['html_path']}")
                    else:
                        print(f"âš ï¸ è‹±æ–‡ç‰ˆç”Ÿæˆå¤±è´¥ï¼Œä½†ä¸­æ–‡ç‰ˆå·²æˆåŠŸç”Ÿæˆ")
                
                except Exception as e:
                    print(f"âš ï¸ ç”Ÿæˆè‹±æ–‡ç‰ˆæ—¶å‡ºé”™: {e}")
                    print(f"ä¸­æ–‡ç‰ˆå·²æˆåŠŸç”Ÿæˆï¼Œå¯ç¨åæ‰‹åŠ¨è½¬æ¢")
                    import traceback
                    traceback.print_exc()

            # Save final results (update with english_post)
            final_result = {
                "user_id": user_id,
                "user_profile": user_profile,
                "personalized_post": personalized_post,
                "english_post": english_post
            }

            with open(os.path.join(user_output_dir, "final_results.json"), "w", encoding="utf-8") as f:
                json.dump(final_result, f, ensure_ascii=False, indent=2)

            img_count = len(personalized_post.get('images', []))
            link_count = len(personalized_post.get('links', []))
            
            print(f"\nâœ… ç”¨æˆ· {user_id} å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“ å¸–å­æ–‡æ¡ˆ: {len(personalized_post['text'])} å­—")
            print(f"ğŸ–¼ï¸  ç”Ÿæˆå›¾ç‰‡: {img_count} å¼ ")
            print(f"ğŸ”— ç›¸å…³é“¾æ¥: {link_count} ä¸ª")
            print(f"ğŸ“„ HTMLæ–‡ä»¶: {personalized_post['html_post']}")
            if english_post:
                print(f"ğŸŒ è‹±æ–‡HTML: {english_post['html_path']}")
            
            # æ¸…ç†è¯¥ç”¨æˆ·çš„embeddingç¼“å­˜
            try:
                content_generator.rag_helper.clear_user_cache("redbook", user_id)
            except Exception as e:
                print(f"âš ï¸  æ¸…ç†embeddingç¼“å­˜å¤±è´¥: {e}")

            return final_result

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files(temp_files)

    def _cleanup_temp_files(self, temp_files):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        cleaned_count = 0
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    cleaned_count += 1
            except Exception as e:
                print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file}: {e}")

        if cleaned_count > 0:
            print(f"ğŸ§¹ å·²æ¸…ç† {cleaned_count} ä¸ªä¸´æ—¶æ–‡ä»¶")

    def _extract_first_preference(self, user_profile_text: str) -> str:
        """Extract the first preference from user profile text.
        
        The user profile format may vary, but always contains:
        "Ordering by user preference level, from highest to lowest:"
        
        Returns the first preference section with the marker line included.
        """
        # Find the marker line (may have ** markdown formatting)
        lines = user_profile_text.split('\n')
        marker_idx = -1
        marker_line = None
        
        for i, line in enumerate(lines):
            # Check if line contains the ordering marker (with or without markdown)
            if "Ordering by user preference level, from highest to lowest" in line:
                marker_idx = i
                marker_line = line
                break
        
        if marker_idx == -1:
            # If marker not found, return original text
            print("âš ï¸  æœªæ‰¾åˆ° preference æ ‡è®°ï¼Œä½¿ç”¨å®Œæ•´ç”¨æˆ·ç”»åƒ")
            return user_profile_text
        
        # Extract first preference
        # Look for the start of the first preference (usually "## 1." or "1. Preference 1:" or similar)
        first_pref_start = -1
        for i in range(marker_idx + 1, len(lines)):
            line = lines[i].strip()
            # Skip empty lines
            if not line:
                continue
            # Check if this is the start of first preference
            # Pattern: "## 1.", "1. Preference 1:", "1. ", etc.
            if (line.startswith("## 1.") or 
                line.startswith("**## 1.") or
                (line.startswith("1.") and ("Preference 1" in line or len(line) > 3)) or
                line.startswith("1. Preference 1")):
                first_pref_start = i
                break
        
        if first_pref_start == -1:
            # If can't find first preference start, return from marker onwards
            first_pref_start = marker_idx + 1
        
        # Find the end of first preference (start of second preference or end of text)
        first_pref_end = len(lines)
        for i in range(first_pref_start + 1, len(lines)):
            line = lines[i].strip()
            # Check if this is the start of second preference
            if (line.startswith("## 2.") or 
                line.startswith("**## 2.") or
                (line.startswith("2.") and ("Preference 2" in line or len(line) > 3)) or
                line.startswith("2. Preference 2")):
                first_pref_end = i
                break
        
        # Extract the first preference section, including the marker line
        first_preference_lines = [marker_line] + lines[first_pref_start:first_pref_end]
        first_preference = '\n'.join(first_preference_lines).strip()
        
        if not first_preference:
            print("âš ï¸  æœªèƒ½æå–åˆ°ç¬¬ä¸€ä¸ª preferenceï¼Œä½¿ç”¨å®Œæ•´ç”¨æˆ·ç”»åƒ")
            return user_profile_text
        
        print(f"âœ… å·²æå– Preference 1 (é•¿åº¦: {len(first_preference)} å­—ç¬¦)")
        return first_preference

    def _combine_analysis_results(self, user_output_dir: str) -> str:
        """åˆå¹¶åˆ†æç»“æœ"""
        analysis_path = os.path.join(user_output_dir, "analysis.json")
        if not os.path.exists(analysis_path):
            return ""

        with open(analysis_path, "r", encoding="utf-8") as f:
            analysis_data = json.load(f)

        combined_analysis = ""
        for media_type, contents in analysis_data.items():
            combined_analysis += f"{media_type.upper()}åˆ†æç»“æœ:\n"
            for idx, content in contents.items():
                combined_analysis += f"é¡¹ç›®{idx}: {content}\n\n"

        return combined_analysis

    def process_users(self, user_ids: Optional[List[str]] = None, max_workers: int = 3, start_index: int = 0, generate_english: bool = True, parallel_users: int = 40) -> Dict:
        """å¤„ç†æŒ‡å®šç”¨æˆ·åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰ç”¨æˆ·ï¼‰
        
        Args:
            user_ids: è¦å¤„ç†çš„ç”¨æˆ·IDåˆ—è¡¨
            max_workers: å•ä¸ªç”¨æˆ·å†…éƒ¨çš„å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆç”¨äºå›¾ç‰‡ç”Ÿæˆç­‰ï¼‰
            start_index: èµ·å§‹åºå·ï¼ˆç”¨äºæ˜¾ç¤ºå’Œç›®å½•å‘½åï¼‰
            generate_english: æ˜¯å¦è‡ªåŠ¨ç”Ÿæˆè‹±æ–‡ç‰ˆï¼ˆé»˜è®¤Trueï¼‰
            parallel_users: å¹¶è¡Œå¤„ç†çš„ç”¨æˆ·æ•°é‡ï¼ˆé»˜è®¤40ä¸ªç”¨æˆ·åŒæ—¶å¤„ç†ï¼‰
        """
        if user_ids is None:
            user_ids = self.get_available_users()
            print(f"è‡ªåŠ¨æ£€æµ‹åˆ° {len(user_ids)} ä¸ªç”¨æˆ·: {user_ids}")

        results = {}
        all_available_users = self.get_available_users()
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¹¶è¡Œå¤„ç†æ¨¡å¼: æœ€å¤š {parallel_users} ä¸ªç”¨æˆ·åŒæ—¶å¤„ç†")
        print(f"   å•ä¸ªç”¨æˆ·å†…éƒ¨å¹¶å‘æ•°: {max_workers}")
        print(f"{'='*60}\n")
        
        # ä½¿ç”¨ ThreadPoolExecutor å¹¶è¡Œå¤„ç†å¤šä¸ªç”¨æˆ·
        with ThreadPoolExecutor(max_workers=parallel_users) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_user = {}
            for user_id in user_ids:
                # è·å–ç”¨æˆ·åœ¨æ€»åˆ—è¡¨ä¸­çš„åºå·
                if user_id in all_available_users:
                    user_index = all_available_users.index(user_id)
                else:
                    user_index = None
                
                future = executor.submit(
                    self._process_user_wrapper,
                    user_id,
                    max_workers,
                    user_index,
                    generate_english
                )
                future_to_user[future] = user_id
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            completed_count = 0
            total_count = len(user_ids)
            
            for future in as_completed(future_to_user):
                user_id = future_to_user[future]
                completed_count += 1
                
                try:
                    result = future.result()
                    if result:
                        results[user_id] = result
                        print(f"\nâœ… [{completed_count}/{total_count}] ç”¨æˆ· {user_id} å¤„ç†å®Œæˆ")
                    else:
                        print(f"\nâš ï¸  [{completed_count}/{total_count}] ç”¨æˆ· {user_id} æœªç”Ÿæˆç»“æœ")
                except Exception as e:
                    print(f"\nâŒ [{completed_count}/{total_count}] å¤„ç†ç”¨æˆ· {user_id} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        if results:
            self._generate_summary(results)

        return results
    
    def _process_user_wrapper(self, user_id: str, max_workers: int, user_index: Optional[int], generate_english: bool) -> Optional[Dict]:
        """åŒ…è£…å‡½æ•°ï¼Œç”¨äºåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ process_user
        
        è¿™ä¸ªåŒ…è£…å‡½æ•°ç¡®ä¿æ¯ä¸ªç”¨æˆ·çš„å¤„ç†è¿‡ç¨‹ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¼šç›¸äº’å¹²æ‰°
        """
        try:
            return self.process_user(user_id, max_workers, user_index=user_index, generate_english=generate_english)
        except Exception as e:
            print(f"ç”¨æˆ· {user_id} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _generate_summary(self, results: Dict):
        """ç”Ÿæˆå¤„ç†æ±‡æ€»æŠ¥å‘Šï¼Œè¿½åŠ æ¨¡å¼"""
        summary_path = os.path.join(self.output_base, "processing_summary.json")

        # è¯»å–ç°æœ‰çš„æ±‡æ€»æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        existing_summary = {}
        if os.path.exists(summary_path):
            try:
                with open(summary_path, 'r', encoding='utf-8') as f:
                    existing_summary = json.load(f)
            except:
                existing_summary = {}

        # è·å–å½“å‰æ—¶é—´æˆ³
        from datetime import datetime
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # æ›´æ–°æ±‡æ€»æ•°æ®
        if "processing_sessions" not in existing_summary:
            existing_summary["processing_sessions"] = []

        # åˆ›å»ºå½“å‰ä¼šè¯è®°å½•
        session_id = f"session_{len(existing_summary['processing_sessions']) + 1}"
        current_session = {
            "session_id": session_id,
            "timestamp": current_time,
            "total_users_processed": len(results),
            "user_results": {}
        }

        for user_id, result in results.items():
            post_data = result.get("personalized_post", {})
            text_len = len(post_data.get("text", ""))
            img_count = len(post_data.get("images", []))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è‹±æ–‡ç‰ˆ
            english_data = result.get("english_post")
            has_english = english_data is not None
            
            current_session["user_results"][user_id] = {
                "post_length": text_len,
                "image_count": img_count,
                "html_path": post_data.get("html_post", ""),
                "user_items_count": result.get("raw_items_count", 0),
                "has_english_version": has_english,
                "english_html_path": english_data.get("html_path", "") if has_english else ""
            }

        # æ·»åŠ åˆ°ä¼šè¯åˆ—è¡¨
        existing_summary["processing_sessions"].append(current_session)

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_sessions = len(existing_summary["processing_sessions"])
        total_users = sum(session["total_users_processed"] for session in existing_summary["processing_sessions"])

        existing_summary["overall_statistics"] = {
            "total_processing_sessions": total_sessions,
            "total_users_processed": total_users,
            "last_updated": current_time
        }

        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(existing_summary, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“Š æ±‡æ€»æŠ¥å‘Šå·²æ›´æ–°: {summary_path}")
        print(f"ğŸ“ˆ å½“å‰ä¼šè¯: {session_id} | å¤„ç†ç”¨æˆ·: {len(results)} ä¸ª")
        print(f"ğŸ“Š ç´¯è®¡ç»Ÿè®¡: {total_sessions} æ¬¡å¤„ç† | {total_users} ä¸ªç”¨æˆ·")

def resolve_user_selection(inputs: List[str], available_users: List[str]) -> List[str]:
    """è¾…åŠ©å‡½æ•°ï¼šå°†è¾“å…¥çš„åºå·æˆ–IDè½¬æ¢ä¸ºçœŸå®çš„ç”¨æˆ·IDåˆ—è¡¨
    
    Supports:
    - Single index: "0", "5", "10"
    - Range: "10-20" (inclusive)
    - User ID: direct user ID string
    - Multiple: "0 5 10-15 20"
    """
    selected_users = []
    
    for item in inputs:
        item = item.strip()
        if not item:
            continue
        
        # Check for range format (e.g., "10-20")
        if '-' in item and not item.startswith('-'):
            parts = item.split('-')
            if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():
                start_idx = int(parts[0])
                end_idx = int(parts[1])
                
                # Validate range
                if start_idx > end_idx:
                    print(f"âš ï¸  è­¦å‘Š: æ— æ•ˆèŒƒå›´ [{item}] - èµ·å§‹å€¼å¿…é¡» <= ç»“æŸå€¼")
                    continue
                
                if start_idx < 0 or end_idx >= len(available_users):
                    print(f"âš ï¸  è­¦å‘Š: èŒƒå›´ [{item}] è¶…å‡ºè¾¹ç•Œ (æœ‰æ•ˆèŒƒå›´: 0-{len(available_users)-1})")
                    # Clamp to valid range
                    start_idx = max(0, start_idx)
                    end_idx = min(len(available_users) - 1, end_idx)
                
                # Add all users in range (inclusive)
                for idx in range(start_idx, end_idx + 1):
                    real_id = available_users[idx]
                    if real_id not in selected_users:
                        selected_users.append(real_id)
                
                print(f"âœ… å·²æ·»åŠ èŒƒå›´ [{start_idx}-{end_idx}]: {end_idx - start_idx + 1} ä¸ªç”¨æˆ·")
                continue
            
        # Try as single index
        if item.isdigit():
            idx = int(item)
            if 0 <= idx < len(available_users):
                real_id = available_users[idx]
                if real_id not in selected_users:
                    selected_users.append(real_id)
            else:
                print(f"âš ï¸  è­¦å‘Š: åºå· [{idx}] è¶…å‡ºèŒƒå›´ (0-{len(available_users)-1})")
        
        # Try as real ID match (å…¼å®¹æ—§ä¹ æƒ¯)
        elif item in available_users:
            if item not in selected_users:
                selected_users.append(item)
        else:
            print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°åºå·æˆ–ç”¨æˆ·ID: {item}")
            
    return selected_users

def main():
    """ä¸»å‡½æ•° - æ”¯æŒåºå·é€‰æ‹©ç”¨æˆ·"""
    import argparse

    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦ç”¨æˆ·æ•°æ®åˆ†æä¸å†…å®¹ç”Ÿæˆ')
    parser.add_argument('--users', nargs='+', help='æŒ‡å®šè¦å¤„ç†çš„ç”¨æˆ·åºå·ã€èŒƒå›´æˆ–IDï¼ˆå¦‚ï¼š0 5-10 15ï¼‰')
    parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰å¯ç”¨ç”¨æˆ·')
    parser.add_argument('--workers', type=int, default=4, help='å•ä¸ªç”¨æˆ·å†…éƒ¨çš„å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆç”¨äºå›¾ç‰‡ç”Ÿæˆç­‰ï¼‰')
    parser.add_argument('--parallel', type=int, default=40, help='å¹¶è¡Œå¤„ç†çš„ç”¨æˆ·æ•°é‡ï¼ˆé»˜è®¤ï¼š40ï¼‰')
    parser.add_argument('--no-english', action='store_true', help='ä¸ç”Ÿæˆè‹±æ–‡ç‰ˆï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--max-reflections', type=int, default=None, help='æœ€å¤§åæ€è½®æ•°ï¼ˆé»˜è®¤ï¼š3ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡MAX_REFLECTION_ITERATIONSè®¾ç½®ï¼‰')

    args = parser.parse_args()

    # åˆ›å»ºå¤„ç†å™¨
    processor = RedBookProcessor()
    
    # è·å–å¯ç”¨ç”¨æˆ·
    available_users = processor.get_available_users()
    if not available_users:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç”¨æˆ·æ•°æ®ï¼Œè¯·æ£€æŸ¥ download/redbook ç›®å½•ç»“æ„")
        return

    processor.setup_environment()
    
    # è®¾ç½®åæ€è½®æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™äº¤äº’å¼è¾“å…¥ï¼‰
    if args.max_reflections is not None:
        max_reflections = args.max_reflections
        os.environ["MAX_REFLECTION_ITERATIONS"] = str(max_reflections)
        print(f"âœ… åæ€è½®æ•°å·²è®¾ç½®ä¸º: {max_reflections}")
    else:
        # äº¤äº’å¼è¾“å…¥åæ€è½®æ•°
        try:
            default_reflections = int(os.getenv("MAX_REFLECTION_ITERATIONS", "3"))
            print(f"\nğŸ’¡ åæ€è½®æ•°è®¾ç½®ï¼ˆé»˜è®¤: {default_reflections}ï¼‰")
            reflection_input = input(f"è¯·è¾“å…¥æœ€å¤§åæ€è½®æ•°ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼ {default_reflections}ï¼‰: ").strip()
            if reflection_input:
                max_reflections = int(reflection_input)
                os.environ["MAX_REFLECTION_ITERATIONS"] = str(max_reflections)
                print(f"âœ… åæ€è½®æ•°å·²è®¾ç½®ä¸º: {max_reflections}")
            else:
                max_reflections = default_reflections
                print(f"âœ… ä½¿ç”¨é»˜è®¤åæ€è½®æ•°: {max_reflections}")
        except ValueError:
            print(f"âš ï¸  è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼: {default_reflections}")
            max_reflections = default_reflections
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·å–æ¶ˆè¾“å…¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            max_reflections = default_reflections

    # æ‰“å°ç”¨æˆ·æ˜ å°„è¡¨
    print(f"\n{'='*20} å¯ç”¨ç”¨æˆ·åˆ—è¡¨ {'='*20}")
    print(f"{'åºå·':<6} | {'ç”¨æˆ·ID'}")
    print("-" * 40)
    for idx, user_id in enumerate(available_users):
        print(f"[{idx:<4}] : {user_id}")
    print("-" * 40)

    target_user_ids = []

    # ç¡®å®šè¦å¤„ç†çš„ç”¨æˆ·
    if args.all:
        print("ğŸš€ å·²é€‰æ‹©å¤„ç†æ‰€æœ‰ç”¨æˆ·")
        target_user_ids = available_users
        
    elif args.users:
        # å‘½ä»¤è¡Œå‚æ•°ä¼ å…¥ (å¯èƒ½æ˜¯åºå·ï¼Œä¹Ÿå¯èƒ½æ˜¯ID)
        target_user_ids = resolve_user_selection(args.users, available_users)
        
    else:
        # äº¤äº’å¼é€‰æ‹©
        print(f"\nå…±æ‰¾åˆ° {len(available_users)} ä¸ªç”¨æˆ·ã€‚")
        print("ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨èŒƒå›´ï¼ˆå¦‚ '0-5' æˆ– '10-20'ï¼‰æˆ–å•ä¸ªåºå·ï¼ˆå¦‚ '0 5 10'ï¼‰")
        user_input = input("è¯·è¾“å…¥è¦å¤„ç†çš„ã€åºå·/èŒƒå›´ã€‘(å¤šä¸ªç”¨ç©ºæ ¼åˆ†éš”ï¼Œè¾“å…¥ 'all' å¤„ç†æ‰€æœ‰): ")

        if user_input.strip().lower() == 'all':
            target_user_ids = available_users
        else:
            target_user_ids = resolve_user_selection(user_input.split(), available_users)

    # æœ€ç»ˆç¡®è®¤
    if not target_user_ids:
        print("âŒ æœªé€‰æ‹©æœ‰æ•ˆçš„ç”¨æˆ·ï¼Œç¨‹åºé€€å‡º")
        return

    print(f"\nâœ… å³å°†å¤„ç†ä»¥ä¸‹ {len(target_user_ids)} ä¸ªç”¨æˆ·:")
    for uid in target_user_ids:
        # åå‘æŸ¥æ‰¾åºå·ä»¥ä¾¿æ˜¾ç¤º
        idx = available_users.index(uid)
        print(f"  [{idx}] {uid}")
    print(f"{'='*50}\n")

    # å¼€å§‹å¤„ç†
    results = processor.process_users(
        target_user_ids, 
        max_workers=args.workers, 
        generate_english=not args.no_english,
        parallel_users=args.parallel
    )

    if results:
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} ä¸ªç”¨æˆ·çš„ä¸ªæ€§åŒ–å†…å®¹")
        for user_id in results.keys():
            idx = available_users.index(user_id) if user_id in available_users else None
            dir_name = f"{idx}_{user_id}" if idx is not None else user_id
            print(f"  ç”¨æˆ· {user_id}: æŸ¥çœ‹ generated_it/{dir_name}/social_media_post.html")
    else:
        print("âŒ æœªå¤„ç†ä»»ä½•ç”¨æˆ·æ•°æ®")

if __name__ == "__main__":
    main()
