import base64
import os
import json
import requests
import urllib.parse
import re
import random
import glob
from datetime import datetime
from pathlib import Path
import http.client

# Handle imports that work both from main.py (absolute) and from agents/ directory (relative)
try:
    # Try absolute import first (when running from main.py)
    from agents.rag_embedding_helper import RAGEmbeddingHelper
except ImportError:
    # Fallback to relative import (when running from agents/ directory)
    from rag_embedding_helper import RAGEmbeddingHelper

# Reflection mechanism imports
try:
    try:
        from agents.evaluate_groupscore import TextEmbeddingEvaluator, evaluate_file_links
    except ImportError:
        from evaluate_groupscore import TextEmbeddingEvaluator, evaluate_file_links
    REFLECTION_AVAILABLE = True
except ImportError:
    REFLECTION_AVAILABLE = False
    print("âš ï¸  Reflection mechanism not available (evaluate_groupscore not found)")


class CommentProductGenerator:
    def __init__(self):
        # Text generation model
        self.chat_api_key = os.getenv("CHAT_API_KEY")
        self.chat_base_url = os.getenv("CHAT_BASE_URL")
        self.chat_model = os.getenv("CHAT_MODEL")

        # Image generation model
        self.generate_api_key = os.getenv("GENERATE_API_KEY")
        self.generate_base_url = os.getenv("GENERATE_BASE_URL")
        self.generate_model = os.getenv("GENERATE_MODEL")
        
        # Search model for hot topic discovery
        self.search_api_key = os.getenv("SEARCH_API_KEY")
        self.search_base_url = os.getenv("SEARCH_BASE_URL")
        self.search_model = os.getenv("SEARCH_MODEL")
        
        # Examples directory
        self.examples_dir = os.path.join(os.path.dirname(__file__), "hupu")
        
        # Cache for example posts (load once, reuse across multiple generations)
        self._examples_cache = None
        
        # RAG settings
        self.rag_enabled = True  # Enable RAG-based example retrieval
        self.dataset_name = "hupu"  # Dataset name for cache identification
        self.data_root = os.path.join(os.path.dirname(__file__), "..", "download", "hupu")
        self._rag_cache = {}  # Cache for RAG retrieval results (key: user_id)
        
        # Initialize RAG embedding helper
        self.rag_helper = RAGEmbeddingHelper(
            api_key=self.search_api_key,
            api_base=self.search_base_url,
            cache_dir=os.path.join(os.path.dirname(__file__), "..", "embeddings_cache")
        )
        
        # Reflection mechanism settings
        self.reflection_enabled = REFLECTION_AVAILABLE and True  # Enable if available
        self.reflection_threshold = float(os.getenv("REFLECTION_THRESHOLD_HUPU", "0.75"))
        # Maximum reflection iterations (can be configured via environment variable)
        # First 3 iterations use specific strategies, iterations >= 3 all use iteration 2's strategy
        self.max_reflection_iterations = int(os.getenv("MAX_REFLECTION_ITERATIONS_HUPU", "10"))
        
        # Initialize reflection components (Link-Text relevance evaluator)
        if self.reflection_enabled:
            try:
                self.text_evaluator = TextEmbeddingEvaluator(
                    api_key=self.search_api_key,
                    api_base=self.search_base_url
                )
                print(f"âœ… Reflectionæœºåˆ¶å·²å¯ç”¨ (è™æ‰‘é“¾æ¥-æ–‡æœ¬è¯„ä¼°, é˜ˆå€¼: {self.reflection_threshold}, æœ€å¤š{self.max_reflection_iterations}æ¬¡è¿­ä»£)")
                if self.max_reflection_iterations > 3:
                    print(f"   â„¹ï¸  å‰3æ¬¡ä½¿ç”¨ç‰¹å®šç­–ç•¥ï¼Œç¬¬4-{self.max_reflection_iterations}æ¬¡é‡å¤ä½¿ç”¨ç¬¬3æ¬¡ç­–ç•¥")
            except Exception as e:
                print(f"âš ï¸  Reflectionæœºåˆ¶åˆå§‹åŒ–å¤±è´¥: {e}, å°†è·³è¿‡reflection")
                self.reflection_enabled = False
        
    def extract_top1_preference(self, profile_data):
        """
        Extract top1 preference from user profile
        
        Args:
            profile_data: User profile dict or string
            
        Returns:
            String containing the top1 preference description
        """
        if isinstance(profile_data, dict):
            profile_text = profile_data.get("profile_text", json.dumps(profile_data, ensure_ascii=False))
        else:
            profile_text = str(profile_data)
        
        # Try to extract "1. Preference 1:" or "Preference 1:" pattern
        patterns = [
            r'1\.\s*Preference\s*1:\s*([^\n]+(?:\n\s+Reason:[^\n]+)?)',
            r'Preference\s*1:\s*([^\n]+(?:\n\s+Reason:[^\n]+)?)',
            r'1\.\s*([^\n]+(?:\n\s+Reason:[^\n]+)?)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, profile_text, re.IGNORECASE)
            if match:
                preference = match.group(1).strip()
                print(f"ğŸ“‹ Extracted Top1 Preference: {preference[:100]}...")
                return preference
        
        # Fallback: return first 500 chars of profile
        print(f"âš ï¸ Could not extract structured preference, using profile preview")
        return profile_text[:500]
    
    def extract_user_id_from_path(self, file_path):
        """
        Extract user ID from file path
        
        Args:
            file_path: Path to user profile or output directory
            
        Returns:
            User ID string, or None if not found
        """
        # Try to extract from path pattern like: generated_it/0_5435e123d6e4a965e190095a/
        # or download/hupu/132349263326575/
        
        # Pattern 1: {number}_{alphanumeric_id}
        match = re.search(r'[\\/](\d+_[a-f0-9]+)[\\/]', file_path)
        if match:
            full_id = match.group(1)
            # Extract the alphanumeric part after underscore
            user_id = full_id.split('_', 1)[1] if '_' in full_id else full_id
            print(f"ğŸ“Œ Extracted user_id from path: {user_id}")
            return user_id
        
        # Pattern 2: pure numeric ID
        match = re.search(r'[\\/](\d{10,})[\\/]', file_path)
        if match:
            user_id = match.group(1)
            print(f"ğŸ“Œ Extracted user_id from path: {user_id}")
            return user_id
        
        print(f"âš ï¸ Could not extract user_id from path: {file_path}")
        return None
    
    def load_examples_with_rag(self, user_id, top1_preference, top_k=3):
        """
        Load examples using RAG (Retrieval-Augmented Generation)
        Retrieves most relevant examples from user's historical and recommended posts
        
        Args:
            user_id: User ID
            top1_preference: Top1 preference text for similarity matching
            top_k: Number of examples to retrieve
            
        Returns:
            List of example dicts with title and content parsed
        """
        # Check cache
        cache_key = f"{user_id}_{top_k}"
        if cache_key in self._rag_cache:
            print(f"âœ… Using cached RAG examples ({len(self._rag_cache[cache_key])} posts)")
            return self._rag_cache[cache_key]
        
        print(f"ğŸ” RAG Mode: Retrieving relevant examples for user {user_id}...")
        
        try:
            # Step 1: Build embeddings for user files
            print(f"ğŸ“Š Building embeddings for user files...")
            embeddings_data = self.rag_helper.build_embeddings_for_user(
                dataset_name=self.dataset_name,
                dataset_root=self.data_root,
                user_id=user_id,
                max_workers=10,
                use_cache=True
            )
            
            if not embeddings_data["embeddings"]:
                print(f"âš ï¸ No embeddings found, falling back to default examples")
                return self.load_examples_fallback()
            
            # Step 2: Retrieve top-k similar files
            print(f"ğŸ¯ Retrieving top-{top_k} similar examples based on preference...")
            similar_files = self.rag_helper.retrieve_top_k_similar(
                query_text=top1_preference,
                embeddings_data=embeddings_data,
                top_k=top_k
            )
            
            if not similar_files:
                print(f"âš ï¸ No similar files found, falling back to default examples")
                return self.load_examples_fallback()
            
            # Step 3: Load file contents and parse
            examples = []
            for i, file_info in enumerate(similar_files):
                try:
                    content = self.rag_helper.get_file_content(file_info["path"])
                    
                    if not content:
                        continue
                    
                    # Parse title and content
                    title = ""
                    parsed_content = content
                    
                    # Check if file has Title: and Content: format
                    if "Title:" in content and "Content:" in content:
                        lines = content.split('\n')
                        for j, line in enumerate(lines):
                            if line.startswith("Title:"):
                                title = line.replace("Title:", "").strip()
                            elif line.startswith("Content:"):
                                parsed_content = '\n'.join(lines[j:]).replace("Content:", "").strip()
                                break
                    
                    examples.append({
                        "filename": file_info["filename"],
                        "title": title,
                        "content": parsed_content,
                        "full_text": content,
                        "similarity": file_info["similarity"],
                        "folder": file_info["folder"],
                        "post_id": file_info["post_id"]
                    })
                    
                    print(f"   âœ… Retrieved: {file_info['filename']} (similarity: {file_info['similarity']:.3f})")
                    if title:
                        print(f"      æ ‡é¢˜: {title[:50]}...")
                        
                except Exception as e:
                    print(f"   âš ï¸ Failed to load {file_info['path']}: {e}")
            
            # Cache the results
            self._rag_cache[cache_key] = examples
            print(f"ğŸ’¾ Cached {len(examples)} RAG example(s)")
            
            return examples
            
        except Exception as e:
            print(f"âš ï¸ RAG retrieval failed: {e}")
            import traceback
            traceback.print_exc()
            return self.load_examples_fallback()
    
    def load_examples_fallback(self):
        """
        Fallback method: Load examples from fixed directory (agents/hupu/)
        This is the original load_examples logic
        """
        examples = []
        
        if not os.path.exists(self.examples_dir):
            print(f"âš ï¸ Examples directory not found: {self.examples_dir}")
            return examples
        
        # Find all .txt files in the examples directory
        txt_files = glob.glob(os.path.join(self.examples_dir, "*.txt"))
        
        if not txt_files:
            print(f"âš ï¸ No example files found in {self.examples_dir}")
            return examples
        
        print(f"ğŸ“š Loading {len(txt_files)} fallback example(s) from {self.examples_dir}...")
        
        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    raw_content = f.read().strip()
                    
                    if not raw_content:
                        continue
                    
                    # Parse title and content
                    title = ""
                    content = raw_content
                    
                    # Check if file has Title: and Content: format
                    if "Title:" in raw_content and "Content:" in raw_content:
                        lines = raw_content.split('\n')
                        for i, line in enumerate(lines):
                            if line.startswith("Title:"):
                                title = line.replace("Title:", "").strip()
                            elif line.startswith("Content:"):
                                content = '\n'.join(lines[i:]).replace("Content:", "").strip()
                                break
                    
                    examples.append({
                        "filename": os.path.basename(txt_file),
                        "title": title,
                        "content": content,
                        "full_text": raw_content
                    })
                    print(f"   âœ… Loaded: {os.path.basename(txt_file)} ({len(content)} chars)")
                    if title:
                        print(f"      æ ‡é¢˜: {title[:50]}...")
                        
            except Exception as e:
                print(f"   âš ï¸ Failed to load {txt_file}: {e}")
        
        return examples
    
    def load_examples(self, user_profile_path=None, profile_data=None):
        """
        Load example posts - supports both RAG mode and fallback mode
        
        Args:
            user_profile_path: Path to user profile (for extracting user_id)
            profile_data: User profile data (for extracting top1 preference)
            
        Returns:
            List of example dicts with title and content parsed
        """
        # If RAG is enabled and we have necessary info, use RAG
        if self.rag_enabled and user_profile_path and profile_data:
            try:
                # Extract user_id from path
                user_id = self.extract_user_id_from_path(user_profile_path)
                
                if user_id:
                    # Extract top1 preference
                    top1_preference = self.extract_top1_preference(profile_data)
                    
                    # Use RAG to retrieve examples
                    examples = self.load_examples_with_rag(user_id, top1_preference, top_k=3)
                    
                    if examples:
                        return examples
                    else:
                        print("âš ï¸ RAG returned no examples, using fallback")
                else:
                    print("âš ï¸ Could not extract user_id, using fallback")
            except Exception as e:
                print(f"âš ï¸ RAG mode failed: {e}, using fallback")
                import traceback
                traceback.print_exc()
        
        # Fallback: Return cached examples if already loaded
        if self._examples_cache is not None:
            print(f"âœ… Using cached fallback examples ({len(self._examples_cache)} posts)")
            return self._examples_cache
        
        # Use fallback method
        examples = self.load_examples_fallback()
        
        # Cache the loaded examples
        self._examples_cache = examples
        print(f"ğŸ’¾ Cached {len(examples)} fallback example(s) for future use")
        
        return examples
    
    def search_hot_topics(self, user_profile, retry_count=0, max_retries=2):
        """
        Real web-search-based hot topic discovery using gpt-5-all.
        Reference: https://yunwu.apifox.cn/api-306423418
        Key: Must include web_search_options: {} parameter!
        
        Args:
            user_profile: ç”¨æˆ·ç”»åƒ
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """

        current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M')

        # æ”¹è¿›ï¼šè¦æ±‚è¿”å›ä¸ç”¨æˆ·å…´è¶£ç›¸å…³çš„å…·ä½“çƒ­ç‚¹äº‹ä»¶
        search_prompt = f"""åŸºäºä»¥ä¸‹ç”¨æˆ·ç”»åƒï¼Œæœç´¢å½“å‰ï¼ˆ{current_time_str}ï¼‰ä¸ç”¨æˆ·å…´è¶£ç›¸å…³çš„2-3ä¸ª**å…·ä½“çƒ­ç‚¹äº‹ä»¶**ã€‚

ç”¨æˆ·ç”»åƒï¼š
{user_profile[:500]}

è¿”å›JSONæ ¼å¼ï¼š
[
  {{
    "topic": "å…·ä½“çƒ­ç‚¹äº‹ä»¶çš„ç®€çŸ­æè¿°ï¼ˆå¦‚ï¼šZyWOoå¤ºå¾—CSGO Major FMVPï¼‰",
    "platform": "å¹³å°åç§°ï¼ˆä¼˜å…ˆä¸­æ–‡å¹³å°å¦‚å¾®åš/çŸ¥ä¹/è™æ‰‘/Bç«™ï¼Œå®åœ¨æ²¡æœ‰å¯ç”¨è‹±æ–‡å¹³å°ï¼‰",
    "search_keyword": "ç²¾ç¡®æœç´¢å…³é”®è¯ï¼ˆç”¨äºç”Ÿæˆæœç´¢é“¾æ¥ï¼Œå¿…å¡«ï¼‰",
    "url": "å¦‚æœæ‰¾åˆ°å…·ä½“è®¨è®º/æ–°é—»é“¾æ¥åˆ™å¡«å†™ï¼ˆé€‰å¡«ï¼‰",
    "title": "å¦‚æœæœ‰URLåˆ™å¡«å†™æ ‡é¢˜ï¼ˆé€‰å¡«ï¼‰"
  }}
]

**æ ¸å¿ƒè¦æ±‚**ï¼š
1. **å¿…é¡»æ˜¯å…·ä½“çš„çƒ­ç‚¹äº‹ä»¶**ï¼Œä¸èƒ½æ˜¯æ³›æ³›çš„"çƒ­æ¦œ"æˆ–"çƒ­æœæ¦œ"
2. çƒ­ç‚¹åº”è¯¥ä¸ç”¨æˆ·çš„å…´è¶£é¢†åŸŸç›¸å…³ï¼ˆå¦‚ç”¨æˆ·å…³æ³¨ä½“è‚²ï¼Œå°±æ‰¾ä½“è‚²çƒ­ç‚¹ï¼‰
3. **search_keyword æ˜¯å¿…å¡«é¡¹**ï¼Œè¦ç²¾ç¡®ï¼ˆå¦‚"ZyWOo FMVP"ã€"CBA é‚±å½ª ç¦èµ›"ï¼‰
4. **é“¾æ¥ä¼˜å…ˆçº§ï¼ˆæŒ‰é¡ºåºå°è¯•ï¼‰**ï¼š
   - ç¬¬ä¸€ä¼˜å…ˆï¼šä¸­æ–‡å¹³å°çš„è®¨è®ºå¸–ï¼ˆå¾®åšã€çŸ¥ä¹ã€è™æ‰‘ã€Bç«™ã€æŠ–éŸ³ã€æ–°æµªä½“è‚²ã€è…¾è®¯ä½“è‚²ç­‰ï¼‰
   - ç¬¬äºŒä¼˜å…ˆï¼šä¸­æ–‡æ–°é—»ç½‘ç«™ï¼ˆæ–°æµªã€ç½‘æ˜“ã€è…¾è®¯ã€æœç‹ç­‰ï¼‰
   - é™çº§æ–¹æ¡ˆï¼šå¦‚æœç¡®å®æ‰¾ä¸åˆ°åˆé€‚çš„ä¸­æ–‡é“¾æ¥ï¼Œè‹±æ–‡æƒå¨åª’ä½“é“¾æ¥ä¹Ÿå¯ä»¥ï¼ˆESPNã€BBCã€åç››é¡¿é‚®æŠ¥ç­‰ï¼‰
5. **æ ‡é¢˜è¯­è¨€åº”ä¸é“¾æ¥è¯­è¨€ä¸€è‡´**ï¼ˆä¸­æ–‡é“¾æ¥ç”¨ä¸­æ–‡æ ‡é¢˜ï¼Œè‹±æ–‡é“¾æ¥ç”¨è‹±æ–‡æ ‡é¢˜ï¼‰
6. **ç¡®ä¿æ¯ä¸ªçƒ­ç‚¹éƒ½æœ‰æœ‰æ•ˆçš„ search_keyword**
7. å¦‚æœçœŸçš„æ‰¾ä¸åˆ°ä»»ä½•ç›¸å…³çƒ­ç‚¹ï¼Œæ‰è¿”å›ï¼šNO_VERIFIED_TRENDS_FOUND"""

        try:
            print("ğŸ” Calling gpt-5-all with web_search_options...")
            print(f"ğŸ“ API Endpoint: https://yunwu.ai/v1/chat/completions")
            print(f"ğŸ“ Model: gpt-5-all")
            print(f"ğŸ“ Search Query Time: {current_time_str}")
            
            # ä½¿ç”¨ gpt-5-all + web_search_options
            response = requests.post(
                "https://yunwu.ai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.search_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-5-all",
                    "web_search_options": {},  # å¯ç”¨è”ç½‘æœç´¢
                    "messages": [
                        {
                            "role": "user",
                            "content": search_prompt
                        }
                    ],
                    "temperature": 0.7  # æ·»åŠ æ¸©åº¦å‚æ•°ä½¿è¾“å‡ºæ›´å¯æ§
                },
                timeout=90  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°90ç§’
            )
            
            print(f"ğŸ“¡ API Response Status: {response.status_code}")
            
            if response.status_code != 200:
                print(f"âš ï¸ API Error {response.status_code}")
                print(f"âš ï¸ Response Text: {response.text[:500]}")
                return "NO_VERIFIED_TRENDS_FOUND"
            
            resp_json = response.json()
            
            # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            debug_path = os.path.join(os.path.dirname(__file__), "..", "debug_search_response.json")
            try:
                with open(debug_path, 'w', encoding='utf-8') as f:
                    json.dump(resp_json, f, ensure_ascii=False, indent=2)
                print(f"ğŸ› Debug: Full API response saved to {debug_path}")
            except:
                pass
            
            if "choices" not in resp_json:
                print(f"âš ï¸ Unexpected response format (no 'choices' field)")
                print(f"âš ï¸ Response keys: {list(resp_json.keys())}")
                return "NO_VERIFIED_TRENDS_FOUND"
            
            content = resp_json["choices"][0]["message"]["content"]
            
            if not content:
                print("âš ï¸ Empty content from search")
                return "NO_VERIFIED_TRENDS_FOUND"
            
            # ğŸ” è°ƒè¯•ï¼šæ‰“å°åŸå§‹ content
            print(f"ğŸ“„ Web Search Raw Response (first 300 chars):")
            print(f"{content[:300]}")
            print(f"...")
            
            # æ£€æŸ¥æ˜ç¡®çš„å¤±è´¥æ¶ˆæ¯ï¼ˆä½†å…è®¸æœ‰JSONæ•°ç»„çš„æƒ…å†µï¼‰
            if "NO_VERIFIED_TRENDS_FOUND" in content and "[" not in content:
                print("âš ï¸ Web search explicitly failed (no JSON array found)")
                return "NO_VERIFIED_TRENDS_FOUND"
            
            # æ¸…ç† content - æ›´æ¿€è¿›çš„æ¸…ç†ç­–ç•¥
            # 1. ç§»é™¤å¼•ç”¨å—ï¼ˆ> å¼€å¤´çš„è¡Œï¼ŒåŒ…æ‹¬æœç´¢å‘½ä»¤å’Œå¼•ç”¨é“¾æ¥ï¼‰
            content = re.sub(r'^>.*?$', '', content, flags=re.MULTILINE)
            
            # 2. ç§»é™¤æ‰€æœ‰ markdown é“¾æ¥å¼•ç”¨ï¼ˆ**[æ–‡æœ¬](url)** Â· *domain* æ ¼å¼ï¼‰
            content = re.sub(r'\*\*\[.*?\]\(.*?\)\*\*\s*Â·\s*\*.*?\*', '', content)
            
            # 3. ç§»é™¤æ™®é€š markdown é“¾æ¥
            content = re.sub(r'\[.*?\]\(.*?\)', '', content)
            
            # 4. ç§»é™¤ä»£ç å—æ ‡è®°
            content = re.sub(r'^```json\s*', '', content, flags=re.MULTILINE)
            content = re.sub(r'^```\s*', '', content, flags=re.MULTILINE)
            
            # 5. ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
            content = re.sub(r'\n{2,}', '\n', content).strip()
            
            print(f"ğŸ“„ After cleaning (first 300 chars):")
            print(f"{content[:300]}")
            print(f"...")
            
            # æå– JSON æ•°ç»„ - æ›´æ™ºèƒ½çš„æå–
            # å…ˆå°è¯•æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„ JSON æ•°ç»„
            json_content = None
            
            # æ–¹æ³•1: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ç®€å•çš„ JSON æ•°ç»„æ¨¡å¼
            # åŒ¹é… [ ... ] ä½†å…è®¸å†…éƒ¨æœ‰é€—å·åˆ†éš”çš„å¯¹è±¡
            simple_pattern = r'\[\s*\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}(?:\s*,\s*\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})*\s*\]'
            match = re.search(simple_pattern, content, re.DOTALL)
            
            if match:
                try:
                    test_json = match.group(0)
                    # æ¸…ç†å¯èƒ½çš„å¹²æ‰°å­—ç¬¦
                    test_json = re.sub(r'\n\s*\n', '\n', test_json)
                    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON
                    test_obj = json.loads(test_json)
                    if isinstance(test_obj, list) and len(test_obj) > 0:
                        json_content = test_json
                        print(f"âœ… Found valid JSON array (pattern matching): {len(json_content)} chars")
                except Exception as e:
                    print(f"âš ï¸ Pattern match found JSON-like structure but parsing failed: {e}")
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œä½¿ç”¨æ‹¬å·åŒ¹é…
            if not json_content:
                first_bracket = content.find('[')
                if first_bracket == -1:
                    print(f"âš ï¸ No JSON array found after cleaning")
                    print(f"ğŸ“„ Cleaned content (first 500 chars): {content[:500]}")
                    return "NO_VERIFIED_TRENDS_FOUND"
                
                print(f"ğŸ“ Found '[' at position {first_bracket}, trying bracket matching...")
                
                # ä»ç¬¬ä¸€ä¸ª '[' å¼€å§‹ï¼Œæ‰¾åˆ°åŒ¹é…çš„ ']'
                bracket_count = 0
                last_bracket = -1
                in_string = False
                escape_next = False
                
                for i in range(first_bracket, len(content)):
                    char = content[i]
                    
                    # å¤„ç†å­—ç¬¦ä¸²å†…çš„å¼•å·å’Œè½¬ä¹‰
                    if escape_next:
                        escape_next = False
                        continue
                    if char == '\\':
                        escape_next = True
                        continue
                    if char == '"':
                        in_string = not in_string
                        continue
                    
                    # åªåœ¨å­—ç¬¦ä¸²å¤–è®¡æ•°æ‹¬å·
                    if not in_string:
                        if char == '[':
                            bracket_count += 1
                        elif char == ']':
                            bracket_count -= 1
                            if bracket_count == 0:
                                last_bracket = i
                                break
                
                if last_bracket != -1:
                    json_content = content[first_bracket:last_bracket + 1].strip()
                    print(f"âœ… Found JSON array (bracket matching): {len(json_content)} chars")
                else:
                    print(f"âš ï¸ Could not find matching ']' for JSON array")
            
            if not json_content:
                print(f"âš ï¸ Could not extract valid JSON array")
                print(f"ğŸ“„ Content sample: {content[:500]}")
                return "NO_VERIFIED_TRENDS_FOUND"
            
            # ğŸ” è°ƒè¯•ï¼šæ‰“å°æå–åçš„ JSON å­—ç¬¦ä¸²
            print(f"ğŸ” Extracted JSON array length: {len(json_content)} chars")
            print(f"ğŸ” Extracted JSON (first 400 chars):")
            print(f"{json_content[:400]}")
            if len(json_content) > 400:
                print(f"...")
                print(f"ğŸ” Last 200 chars:")
                print(f"...{json_content[-200:]}")

            # -------------------------
            # 3. è§£æ JSON
            # -------------------------
            try:
                topics = json.loads(json_content)
                print(f"âœ… JSON parsed successfully: {len(topics)} topics found")
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON Parse Error: {e}")
                print(f"âš ï¸ Error position: line {e.lineno}, column {e.colno}")
                # å°è¯•ä¿®å¤å¸¸è§çš„ JSON é—®é¢˜
                try:
                    content_fixed = json_content
                    
                    # 1. ç§»é™¤ trailing commas
                    content_fixed = re.sub(r',\s*]', ']', content_fixed)
                    content_fixed = re.sub(r',\s*}', '}', content_fixed)
                    
                    # 2. ç§»é™¤å¯èƒ½çš„ BOM å’Œç‰¹æ®Šå­—ç¬¦
                    content_fixed = content_fixed.encode('utf-8').decode('utf-8-sig').strip()
                    
                    # 3. ç§»é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œã€å›è½¦ã€åˆ¶è¡¨ç¬¦ï¼‰
                    content_fixed = ''.join(
                        char for char in content_fixed 
                        if ord(char) >= 32 or char in '\n\r\t'
                    )
                    
                    # 4. ç§»é™¤åµŒå¥—çš„ JSON æ•°ç»„ï¼ˆå¯èƒ½æ˜¯é‡å¤å†…å®¹ï¼‰
                    # å¦‚æœå‘ç°å­—ç¬¦ä¸²ä¸­åŒ…å« "[" è¯´æ˜å¯èƒ½æœ‰åµŒå¥—
                    lines = content_fixed.split('\n')
                    cleaned_lines = []
                    in_string = False
                    for line in lines:
                        # ç®€å•æ£€æµ‹ï¼šå¦‚æœè¡Œå¼€å¤´å°±æ˜¯ '['ï¼Œå¯èƒ½æ˜¯é‡å¤çš„æ•°ç»„å¼€å§‹
                        if line.strip().startswith('[') and cleaned_lines:
                            print(f"ğŸ”§ Detected nested array start, truncating...")
                            break
                        cleaned_lines.append(line)
                    content_fixed = '\n'.join(cleaned_lines)
                    
                    print(f"ğŸ”§ Attempting to fix JSON...")
                    print(f"ğŸ” Fixed content (first 300 chars): {content_fixed[:300]}")
                    
                    topics = json.loads(content_fixed)
                    print(f"âœ… JSON fixed and parsed successfully!")
                except Exception as e2:
                    print(f"âš ï¸ Retry Parse Error: {e2}")
                    print(f"âŒ Failed content (first 500 chars): {repr(json_content[:500])}")
                    if len(json_content) > 500:
                        print(f"âŒ Failed content (last 200 chars): {repr(json_content[-200:])}")
                    return "NO_VERIFIED_TRENDS_FOUND"

            # -------------------------
            # 4. æ ¼å¼æ ¡éªŒ + ç¤ºä¾‹æ•°æ®æ£€æµ‹ + URLæå–
            # -------------------------
            verified = []
            if isinstance(topics, list):
                for t in topics:
                    # æ£€æŸ¥å¿…éœ€å­—æ®µï¼ˆæ–°æ ¼å¼ï¼štopic, platform, title, urlï¼‰
                    if (
                        isinstance(t, dict)
                        and t.get("topic")
                        and t.get("platform")
                    ):
                        # æ£€æµ‹æ˜¯å¦è¿”å›äº†ç¤ºä¾‹æ•°æ®ï¼ˆè€ŒéçœŸå®æœç´¢ç»“æœï¼‰
                        topic_text = t.get("topic", "").lower()
                        
                        # æ‹’ç»åŒ…å«ç¤ºä¾‹/æ¨¡æ¿å…³é”®è¯çš„æ•°æ®
                        if any(x in topic_text for x in ["ç®€è¦", "çƒ­ç‚¹æè¿°", "example", "ç¤ºä¾‹"]):
                            print(f"âš ï¸ Detected template/example data, rejecting: {t.get('topic')}")
                            continue
                        
                        # æ–°å¢ï¼šæ‹’ç»æ³›æ³›çš„"çƒ­æ¦œ"ã€"çƒ­æœæ¦œ"ç­‰èšåˆé¡µé¢ï¼ˆä½†è¦æ›´ç²¾ç¡®ï¼‰
                        if any(x in topic_text for x in ["ä»Šæ—¥çƒ­æ¦œ", "å®æ—¶çƒ­æœæ¦œ", "çƒ­é—¨æ’è¡Œæ¦œ", "ç»¼åˆçƒ­æ¦œ"]):
                            print(f"âš ï¸ Detected generic hot list, rejecting: {t.get('topic')}")
                            continue
                        
                        # æ”¹è¿›çš„éªŒè¯é€»è¾‘ï¼šä¼˜å…ˆçº§ search_keyword > URL
                        has_search_keyword = bool(t.get("search_keyword"))
                        has_url_and_title = bool(t.get("url") and t.get("title"))
                        
                        # å¦‚æœæœ‰ search_keywordï¼ŒåŸºæœ¬ä¸Šå°±æ¥å—ï¼ˆå¯ä»¥ç”Ÿæˆæœç´¢é“¾æ¥ï¼‰
                        if has_search_keyword:
                            keyword_text = t.get("search_keyword", "").lower()
                            # åªæ‹’ç»æ˜æ˜¾çš„æ¨¡æ¿å…³é”®è¯
                            if any(x in keyword_text for x in ["å…³é”®è¯ç¤ºä¾‹", "keyword example", "æœç´¢ç¤ºä¾‹"]):
                                print(f"âš ï¸ Detected template keyword, rejecting: {t.get('search_keyword')}")
                                continue
                            print(f"âœ… Valid topic with search_keyword: {t.get('topic')[:50]}...")
                            verified.append(t)
                            continue
                        
                        # å¦‚æœæ²¡æœ‰ search_keywordï¼Œæ£€æŸ¥ URL
                        if has_url_and_title:
                            url = t.get("url", "")
                            # éªŒè¯ URL æ ¼å¼
                            if not (url.startswith("http://") or url.startswith("https://")):
                                print(f"âš ï¸ Invalid URL format, skipping: {url}")
                                continue
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯çƒ­æ¦œèšåˆç½‘ç«™ï¼ˆè¿™äº›è¦æ‹’ç»ï¼Œå› ä¸ºä¸å¤Ÿå…·ä½“ï¼‰
                            hotlist_domains = [
                                "tophub.today",
                                "remenla.com",
                                "shenmehuole.com",
                                "imshuai.com",
                                "v2hot.com"
                            ]
                            
                            is_hotlist_site = any(domain in url for domain in hotlist_domains)
                            
                            if is_hotlist_site:
                                print(f"âš ï¸ Hotlist aggregator URL rejected: {url[:60]}...")
                                continue
                            
                            # URL æ£€æŸ¥ï¼ˆå·²ç»ä¼˜åŒ–è¿‡ï¼Œæ›´å®½æ¾ï¼‰
                            if self._is_homepage_url(url):
                                print(f"âš ï¸ Homepage URL without search_keyword, skipping: {url[:60]}...")
                                continue
                            
                            print(f"âœ… Valid topic with URL: {t.get('title')[:40]}... -> {url[:60]}...")
                            verified.append(t)
                            continue
                        
                        # ä¸¤è€…éƒ½æ²¡æœ‰ï¼Œè·³è¿‡
                        print(f"âš ï¸ Missing both URL and search_keyword, skipping topic")
                        continue

            if not verified:
                print("âš ï¸ No valid real topics found (only examples/templates)")
                
                # é‡è¯•æœºåˆ¶
                if retry_count < max_retries:
                    print(f"ğŸ”„ Retrying... (attempt {retry_count + 1}/{max_retries})")
                    import time
                    time.sleep(2)  # ç­‰å¾…2ç§’
                    return self.search_hot_topics(user_profile, retry_count + 1, max_retries)
                
                # é™çº§ç­–ç•¥ï¼šè¿”å›é€šç”¨çƒ­ç‚¹æ¨¡æ¿
                print("ğŸ“‹ Using fallback generic hot topics")
                return self._get_fallback_topics()

            print(f"ğŸ” VERIFIED HOT TOPICS ({current_time_str}):")
            for t in verified:
                if t.get('url') and t.get('title'):
                    # æ–°æ ¼å¼ï¼šå¸¦çœŸå®é“¾æ¥
                    print(f"- [{t['platform']}] {t['topic']}")
                    print(f"  ğŸ“° {t['title']}")
                    print(f"  ğŸ”— {t['url'][:80]}...")
                elif t.get('search_keyword'):
                    # æ—§æ ¼å¼ï¼šæœç´¢å…³é”®è¯
                    print(f"- [{t['platform']}] {t['topic']} | æœç´¢è¯: {t['search_keyword']}")
                else:
                    print(f"- [{t['platform']}] {t['topic']}")

            return verified

        except Exception as e:
            print(f"âš ï¸ Search exception: {e}")
            import traceback
            traceback.print_exc()
            
            # é‡è¯•æœºåˆ¶
            if retry_count < max_retries:
                print(f"ğŸ”„ Retrying after exception... (attempt {retry_count + 1}/{max_retries})")
                import time
                time.sleep(2)
                return self.search_hot_topics(user_profile, retry_count + 1, max_retries)
            
            # é™çº§ç­–ç•¥ï¼šè¿”å›é€šç”¨çƒ­ç‚¹æ¨¡æ¿
            print("ğŸ“‹ Using fallback generic hot topics after exception")
            return self._get_fallback_topics()
    
    def _get_fallback_topics(self):
        """
        Fallback generic hot topics when web search fails
        è¿”å›é€šç”¨çš„çƒ­ç‚¹è¯é¢˜æ¨¡æ¿ï¼ˆä¸ä¾èµ–å®æ—¶æœç´¢ï¼‰
        """
        fallback_topics = [
            {
                "topic": "æœ€è¿‘ç¤¾äº¤åª’ä½“ä¸Šçš„çƒ­é—¨è¯é¢˜è®¨è®º",
                "platform": "ç»¼åˆå¹³å°",
                "search_keyword": "çƒ­ç‚¹è¯é¢˜ è®¨è®º",
                "title": "å½“å‰çƒ­é—¨è¯é¢˜ç»¼åˆ",
                "url": "https://tophub.today"
            },
            {
                "topic": "å½“ä¸‹æµè¡Œçš„ç½‘ç»œçƒ­æ¢—å’Œæ–‡åŒ–ç°è±¡",
                "platform": "è™æ‰‘/å¾®åš",
                "search_keyword": "ç½‘ç»œçƒ­æ¢— æ–‡åŒ–",
                "title": "ç½‘ç»œæ–‡åŒ–çƒ­ç‚¹",
                "url": "https://www.zhihu.com/hot"
            }
        ]
        
        print("âš ï¸ Using 2 fallback topics (generic templates)")
        for i, t in enumerate(fallback_topics, 1):
            print(f"  {i}. [{t['platform']}] {t['topic']}")
        
        return fallback_topics
    


    def generate_idea_from_topics(self, user_profile, hot_topics):
        """Generate discussion post idea based on hot topics and user profile
        æ”¹è¿›ï¼šç”Ÿæˆçš„åˆ›æ„åº”è¯¥æ˜¯"å¦‚ä½•è¯„è®ºè¿™ä¸ªçƒ­ç‚¹"ï¼Œè€Œä¸æ˜¯ç‹¬ç«‹çš„è¯é¢˜
        """
        # æ„å»ºçƒ­ç‚¹åˆ—è¡¨
        topics_text = ""
        if isinstance(hot_topics, list):
            for i, t in enumerate(hot_topics):
                topics_text += f"{i+1}. [{t.get('platform', '')}] {t.get('topic', '')}\n"
        
        prompt = f"""
        You are a creative content strategist. Based on the user profile and current hot topics,
        generate 2-3 discussion post ideas that comment on or discuss these hot topics.
        
        User Profile:
        {user_profile}
        
        Hot Topics:
        {topics_text}
        
        Requirements:
        1. **Each idea should be about commenting on/discussing one of the hot topics above**
        2. One idea should be emotional/casual style
        3. One idea should be controversial or direct (a bit aggressive)
        4. One idea should be analytical/rational style
        5. **Must specify which hot topic to discuss**
        
        Return as JSON array:
        [
          {{
            "idea": "How to comment on this hot topic (e.g., analyze from fan perspective)",
            "hot_topic_index": 1,
            "angle": "Unique perspective or approach",
            "tone": "Casual/Aggressive/Analytical"
          }}
        ]
        """

        try:
            resp = requests.post(
                f"{self.search_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.search_api_key}"},
                json={
                    "model": self.search_model,
                    "web_search_options": {},
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.8,
                }
            )
            
            # Check response status
            if resp.status_code != 200:
                print(f"âš ï¸ API Error {resp.status_code}: {resp.text[:200]}")
                raise Exception(f"API returned status code {resp.status_code}")
            
            resp_data = resp.json()
            
            # Debug: Print response structure if choices not found
            if "choices" not in resp_data:
                print(f"âš ï¸ Unexpected response structure:")
                print(json.dumps(resp_data, ensure_ascii=False, indent=2)[:500])
                raise Exception("Response missing 'choices' field")
            
            ideas_text = resp_data["choices"][0]["message"]["content"].strip()
            
            # Try to extract JSON from response
            if "```json" in ideas_text:
                ideas_text = ideas_text.split("```json")[1].split("```")[0].strip()
            elif "```" in ideas_text:
                ideas_text = ideas_text.split("```")[1].split("```")[0].strip()
                
            ideas = json.loads(ideas_text)
            print(f"ğŸ’¡ Generated {len(ideas)} post ideas")
            return ideas
            
        except Exception as e:
            print(f"âš ï¸ Idea generation error: {e}")
            import traceback
            traceback.print_exc()
            return [{
                "idea": "å¯¹ç¬¬ä¸€ä¸ªçƒ­ç‚¹å‘è¡¨ä¸ªäººçœ‹æ³•",
                "hot_topic_index": 1,
                "angle": "ä¸ªäººè§‚ç‚¹åˆ†äº«",
                "tone": "Casual"
            }]

    def generate_text(self, user_profile, hot_topics, ideas, user_profile_path=None, profile_data=None):
        """Generate discussion post text based on REAL EXAMPLES (æ ·ä¾‹ä¼˜å…ˆ)
        æ ¸å¿ƒé€»è¾‘ï¼šåŸºäºæ ·ä¾‹é£æ ¼ï¼Œå¯¹æ‰¾åˆ°çš„çƒ­ç‚¹äº‹ä»¶è¿›è¡Œè¯„è®º/è®¨è®º
        
        Args:
            user_profile: User profile text
            hot_topics: Hot topics list
            ideas: Ideas list
            user_profile_path: Path to user profile (for RAG)
            profile_data: Profile data dict (for RAG)
        """
        
        # åŠ è½½çœŸå®æ ·ä¾‹ (æ”¯æŒRAGæ¨¡å¼)
        examples = self.load_examples(user_profile_path=user_profile_path, profile_data=profile_data)
        
        if not examples:
            print("âš ï¸ No examples found, using fallback style templates")
            return self._generate_text_fallback(user_profile, hot_topics, ideas)
        
        print(f"âœ¨ Generating text based on {len(examples)} real example(s)")
        
        # æ„å»ºæ ·ä¾‹æ–‡æœ¬ï¼Œçªå‡ºæ˜¾ç¤ºæ ‡é¢˜ï¼ˆç‰¹åˆ«æ˜¯"ç†æ€§è®¨è®º"ç­‰å¼€å¤´ï¼‰
        examples_text_parts = []
        for i, ex in enumerate(examples):
            if ex.get('title'):
                examples_text_parts.append(
                    f"ã€æ ·ä¾‹ {i+1}ã€‘\næ ‡é¢˜ï¼š{ex['title']}\n\n{ex['content']}"
                )
            else:
                examples_text_parts.append(
                    f"ã€æ ·ä¾‹ {i+1}ã€‘\n{ex['content']}"
                )
        
        examples_text = "\n\n---\n\n".join(examples_text_parts)
        
        # æ„å»ºçƒ­ç‚¹ä¸Šä¸‹æ–‡ï¼šæå–å…·ä½“çš„çƒ­ç‚¹äº‹ä»¶æè¿°
        hot_topics_context = ""
        if isinstance(hot_topics, list) and len(hot_topics) > 0:
            hot_topics_context = "**å½“å‰å¯é€‰çš„çƒ­ç‚¹äº‹ä»¶ï¼š**\n"
            for i, topic in enumerate(hot_topics):
                topic_desc = topic.get("topic", "")
                platform = topic.get("platform", "")
                hot_topics_context += f"{i+1}. [{platform}] {topic_desc}\n"
        
        # æ–°çš„ promptï¼šæ ·ä¾‹é£æ ¼ + çƒ­ç‚¹è¯„è®º
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè™æ‰‘è€ç”¨æˆ·ï¼Œéœ€è¦åˆ›ä½œä¸€ç¯‡è®¨è®ºå¸–ã€‚

**æ ¸å¿ƒä»»åŠ¡ï¼šé€‰æ‹©ä¸€ä¸ªçƒ­ç‚¹äº‹ä»¶ï¼Œç”¨æ ·ä¾‹é£æ ¼å¯¹å…¶è¿›è¡Œè¯„è®º/è®¨è®º**

{examples_text}

---

{hot_topics_context}

**ç”¨æˆ·èƒŒæ™¯ï¼š**
{user_profile[:300]}

**åˆ›ä½œè¦æ±‚ï¼š**
1. **ä»ä¸Šé¢çš„çƒ­ç‚¹ä¸­é€‰æ‹©ä¸€ä¸ªä¸ç”¨æˆ·å…´è¶£ç›¸å…³çš„äº‹ä»¶**
2. **å®Œå…¨æ¨¡ä»¿æ ·ä¾‹çš„é£æ ¼ã€è¯­æ°”ã€ç”¨è¯ã€ç»“æ„æ¥è¯„è®ºè¿™ä¸ªçƒ­ç‚¹**
3. å­—æ•°æ§åˆ¶åœ¨200-400å­—ä»¥å†…
4. ç›´æ¥ã€ç®€æ´ï¼Œæ¯å¥è¯éƒ½æœ‰ä¿¡æ¯é‡
5. å¦‚æœæ ·ä¾‹æ˜¯"ç†æ€§è®¨è®º"é£æ ¼ï¼Œä½ ä¹Ÿç”¨ç†æ€§è®¨è®ºå¼€å¤´
6. **å¿…é¡»åœ¨å¸–å­ä¸­æ˜ç¡®æåˆ°ä½ é€‰æ‹©çš„çƒ­ç‚¹äº‹ä»¶ï¼ˆå¦‚äººç‰©åã€äº‹ä»¶åï¼‰**
7. **å¦‚æœæœ‰æ ‡é¢˜ï¼Œç›´æ¥è¾“å‡ºæ ‡é¢˜å†…å®¹ï¼Œä¸è¦åŠ "æ ‡é¢˜ï¼š"ç­‰å‰ç¼€**

**å…³é”®åŸåˆ™ï¼š**
- æ ·ä¾‹æ€ä¹ˆå†™ï¼Œä½ å°±æ€ä¹ˆå†™
- ä¸è¦æ³›æ³›è€Œè°ˆï¼Œè¦é’ˆå¯¹å…·ä½“çƒ­ç‚¹å‘è¡¨çœ‹æ³•
- ä¿æŒæ ·ä¾‹çš„ç®€æ´å’Œç›´æ¥
- å¦‚æœç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜ï¼Œç›´æ¥å†™æ ‡é¢˜å†…å®¹å³å¯ï¼Œä¸éœ€è¦"æ ‡é¢˜ï¼š"å‰ç¼€

è¯·å¼€å§‹åˆ›ä½œï¼ˆåªè¾“å‡ºå¸–å­æ­£æ–‡ï¼‰ï¼š
"""

        try:
            resp = requests.post(
                f"{self.search_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.search_api_key}"},
                json={
                    "model": self.search_model,
                    "web_search_options": {},
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,  # é™ä½æ¸©åº¦ï¼Œæ›´è´´è¿‘æ ·ä¾‹é£æ ¼
                    "top_p": 0.9,
                }
            )
            
            if resp.status_code != 200:
                print(f"âš ï¸ Text Generation API Error {resp.status_code}: {resp.text[:200]}")
                raise Exception(f"API returned status code {resp.status_code}")
            
            resp_data = resp.json()
            
            if "choices" not in resp_data:
                print(f"âš ï¸ Unexpected response structure:")
                print(json.dumps(resp_data, ensure_ascii=False, indent=2)[:500])
                raise Exception("Response missing 'choices' field")
            
            full_content = resp_data["choices"][0]["message"]["content"].strip()
            
            # æ¸…ç†æ€è€ƒæ ‡è®°å’Œæœç´¢å…ƒæ•°æ®
            full_content = re.sub(r'```\s*\n?\s*\{[^}]*?"search_query".*?\}\s*```', '', full_content, flags=re.DOTALL)
            full_content = re.sub(r'```json\s*\n?\s*\{[^}]*?"search_query".*?\}\s*```', '', full_content, flags=re.DOTALL)
            full_content = re.sub(r'^>.*?$', '', full_content, flags=re.MULTILINE)
            full_content = re.sub(r'\*Thought for \d+s\*', '', full_content)
            full_content = re.sub(r'> \*\*.*?\*\*\n?', '', full_content, flags=re.MULTILINE)
            full_content = re.sub(r'\n{3,}', '\n\n', full_content)
            full_content = full_content.strip()
            
            # å­—æ•°æ£€æŸ¥å’Œæˆªæ–­
            full_content = self._ensure_concise_text(full_content)
            
            print(f"âœ… Generated {len(full_content)} chars based on examples")
            
            # è¿”å›æ–‡æœ¬å’Œä¸€ä¸ªç®€å•çš„é£æ ¼æ ‡è®°
            return full_content, [], {"name": "æ ·ä¾‹å­¦ä¹ é£æ ¼", "desc": "åŸºäºçœŸå®æ ·ä¾‹å­¦ä¹ çš„é£æ ¼"}
            
        except Exception as e:
            print(f"âš ï¸ Text generation error: {e}")
            import traceback
            traceback.print_exc()
            # é™çº§åˆ°å¤‡ç”¨æ–¹æ³•
            return self._generate_text_fallback(user_profile, hot_topics, ideas)
    
    def _ensure_concise_text(self, text, max_chars=450):
        """
        Ensure text is concise and within character limit
        ç¡®ä¿æ–‡æœ¬ç®€çŸ­ï¼Œä¸è¶…è¿‡æœ€å¤§å­—ç¬¦æ•°
        """
        if len(text) <= max_chars:
            return text
        
        print(f"âš ï¸ Text too long ({len(text)} chars), truncating to {max_chars} chars")
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        # ä¿ç•™å‰å‡ æ®µï¼Œç›´åˆ°æ¥è¿‘é™åˆ¶
        result_paragraphs = []
        current_length = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # å¦‚æœåŠ ä¸Šè¿™æ®µä¼šè¶…è¿‡é™åˆ¶
            if current_length + len(para) + 4 > max_chars:  # +4 for \n\n
                # å¦‚æœå·²ç»æœ‰è‡³å°‘1æ®µäº†ï¼Œå°±åœæ­¢
                if result_paragraphs:
                    break
                # å¦‚æœè¿˜æ²¡æœ‰ä»»ä½•æ®µè½ï¼Œæˆªæ–­è¿™ä¸€æ®µ
                remaining = max_chars - current_length - 20  # ç•™ç©ºé—´åŠ ç»“å°¾
                if remaining > 50:
                    para = para[:remaining] + "..."
                    result_paragraphs.append(para)
                break
            
            result_paragraphs.append(para)
            current_length += len(para) + 2  # +2 for \n\n
        
        result = '\n\n'.join(result_paragraphs)
        
        # å¦‚æœç»“å°¾ä¸æ˜¯äº’åŠ¨å‹ç»“å°¾ï¼Œæ·»åŠ ä¸€ä¸ª
        if result and not any(end in result[-20:] for end in ['ï¼Ÿ', '?', 'ğŸ¤”', 'ğŸ¤', 'JRs']):
            result += "\n\nJRsæ€ä¹ˆçœ‹ï¼ŸğŸ¤”"
        
        print(f"âœ… Truncated to {len(result)} chars")
        return result
    
    def _generate_text_fallback(self, user_profile, hot_topics, ideas):
        """Fallback method using style templates (åŸæ¥çš„æ–¹æ³•)"""
        
        # å®šä¹‰ä¸‰ç§æˆªç„¶ä¸åŒçš„é£æ ¼æ¨¡æ¿
        style_templates = {
            "casual": {
                "name": "è™æ‰‘ä¹å­äºº (åƒç“œ/ç©æ¢—)",
                "desc": "è™æ‰‘å…¸å‹åƒç“œç½‘å‹ï¼Œçˆ±å” å—‘ã€ç©ä¸“å±æ¢—ï¼Œè‡ªå¸¦åæ§½buffï¼Œè¯´è¯æ¥åœ°æ°”åˆæœ‰æ¢—",
                "weight": 3,
                "instructions": """
                1. **è¯­æ°”**ï¼šè½»æ¾è°ƒä¾ƒã€è‡ªå¸¦æˆè°‘æ„Ÿï¼Œåƒå’ŒJRsè¹²åœ¨æ­¥è¡Œè¡—ä¸»å¹²é“å” å—‘ï¼Œä¸ç«¯ç€
                2. **ç”¨è¯**ï¼šä¼˜å…ˆç”¨è™æ‰‘çƒ­æ¢—ï¼ˆèšŒåŸ ä½äº†ã€ç»·ä¸ä½äº†ã€è°æ‡‚å•Šï¼‰ï¼Œå°‘ç”¨ã€Œç»ç»å­ã€è¿™ç±»è½¯èŒè¯
                3. **ç§°å‘¼**ï¼šå¤šç”¨è™æ‰‘ä¸“å±ç§°å‘¼ã€ŒJRsã€ã€Œå®¶äººä»¬ã€
                4. **Emoji**ï¼šç”¨è™æ‰‘é«˜é¢‘è¡¨æƒ…ï¼ˆğŸ¤£ğŸ¤¡ğŸ™‰ğŸ”¥ğŸ˜œï¼‰ï¼Œä¸å †ç Œ
                5. **ç‰¹ç‚¹**ï¼šé€»è¾‘éšç¼˜ï¼Œä¸»æ‰“æƒ…ç»ªå…±é¸£+ç©æ¢—
                6. **å­—æ•°**ï¼š200-400å­—ï¼Œç®€çŸ­æœ‰åŠ›
                """
            },
            "aggressive": {
                "name": "è™æ‰‘æš´èºè€å“¥ (ç›´çƒ/æ€¼äºº)",
                "desc": "è™æ‰‘ç¡¬æ ¸è€å›å‹ï¼Œçœ‹ä¸æƒ¯å°±æ€¼ï¼Œè¯´è¯ä¸ç»•å¼¯ï¼Œåæ§½ç›´å‡»ç—›ç‚¹",
                "weight": 3,
                "instructions": """
                1. **è¯­æ°”**ï¼šå†²ã€ç›´çƒã€å¸¦åˆºï¼Œä¸ç£¨å½
                2. **ç”¨è¯**ï¼šè™æ‰‘å¼åæ§½è¯ï¼ˆæè¿™æ‰¯çŠŠå­å‘¢ã€çº¯çº¯ntã€åˆ«æ´—äº†ï¼‰
                3. **Emoji**ï¼šæå°‘ç”¨ï¼Œé¡¶å¤šç»“å°¾åŠ ğŸ˜…/ğŸ™„
                4. **ç‰¹ç‚¹**ï¼šç›´å‡»é—®é¢˜æ ¸å¿ƒ
                5. **å­—æ•°**ï¼š200-350å­—ï¼ŒçŸ­å¥ä¸ºä¸»
                """
            },
            "analytical": {
                "name": "è™æ‰‘æ‡‚å“¥ (ç†æ™º/æ‘†äº‹å®)",
                "desc": "è™æ‰‘èµ„æ·±å›å‹ï¼Œä¸»æ‰“ã€Œæ‘†æ•°æ®ã€è®²äº‹å®ã€",
                "weight": 4,
                "instructions": """
                1. **å¼€å¤´**ï¼šå¯ä»¥ç”¨"ç†æ€§è®¨è®º"å¼€å¤´
                2. **è¯­æ°”**ï¼šå†·é™å®¢è§‚ã€ä¸å‘ä¸äº¢
                3. **ç”¨è¯**ï¼šç›´æ¥ã€ç®€æ´ã€æœ‰é€»è¾‘ï¼ˆå…ˆè¯´ç¼ºç‚¹å†è¯´ä¼˜ç‚¹ï¼‰
                4. **Emoji**ï¼šåŸºæœ¬ä¸ç”¨æˆ–æå°‘ç”¨
                5. **ç‰¹ç‚¹**ï¼šå…ˆç»™ç»“è®ºå†æ‹†è®ºæ®ï¼Œæœ‰å…·ä½“æ•°æ®æ”¯æ’‘
                6. **å­—æ•°**ï¼š200-400å­—
                """
            }
        }

        styles = list(style_templates.keys())
        weights = [style_templates[s]["weight"] for s in styles]
        selected_style_key = random.choices(styles, weights=weights, k=1)[0]
        selected_style = style_templates[selected_style_key]
        
        print(f"ğŸ­ Using fallback style: {selected_style['name']}")

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè™æ‰‘ç”¨æˆ·ï¼Œéœ€è¦åˆ›ä½œä¸€ç¯‡è®¨è®ºå¸–ã€‚

é£æ ¼ï¼š{selected_style['name']}

å‚è€ƒä¿¡æ¯ï¼š
- ç”¨æˆ·ç”»åƒï¼š{user_profile}
- çƒ­ç‚¹è¯é¢˜ï¼š{hot_topics}
- åˆ›æ„æ–¹å‘ï¼š{json.dumps(ideas, ensure_ascii=False)}

é£æ ¼è¦æ±‚ï¼š
{selected_style['instructions']}

æ ¸å¿ƒåŸåˆ™ï¼š
- å­—æ•°200-400å­—
- ç›´æ¥ã€ç®€æ´ã€ä¸åºŸè¯
- æ¯å¥è¯éƒ½æœ‰ä¿¡æ¯é‡
- å¦‚æœæœ‰æ ‡é¢˜ï¼Œç›´æ¥å†™æ ‡é¢˜å†…å®¹ï¼Œä¸è¦åŠ "æ ‡é¢˜ï¼š"ç­‰å‰ç¼€

è¯·å¼€å§‹åˆ›ä½œï¼ˆåªè¾“å‡ºå¸–å­æ­£æ–‡ï¼‰ï¼š
"""

        try:
            resp = requests.post(
                f"{self.search_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.search_api_key}"},
                json={
                    "model": self.search_model,
                    "web_search_options": {},
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                    "top_p": 0.9,
                }
            )
            
            if resp.status_code != 200:
                raise Exception(f"API error {resp.status_code}")
            
            resp_data = resp.json()
            full_content = resp_data["choices"][0]["message"]["content"].strip()
            
            # æ¸…ç†
            full_content = re.sub(r'^>.*?$', '', full_content, flags=re.MULTILINE)
            full_content = re.sub(r'\n{3,}', '\n\n', full_content)
            full_content = full_content.strip()
            
            # å­—æ•°æ£€æŸ¥å’Œæˆªæ–­
            full_content = self._ensure_concise_text(full_content)
            
            print(f"âœ… Generated {len(full_content)} chars (fallback)")
            
        except Exception as e:
            print(f"âš ï¸ Fallback generation error: {e}")
            full_content = f"è¿™æ˜¯ä¸€ç¯‡å…³äºçƒ­ç‚¹è¯é¢˜çš„è®¨è®ºå¸–ã€‚{selected_style['name']}é£æ ¼çš„å†…å®¹ç”Ÿæˆå¤±è´¥ã€‚"
        
        return full_content, [], selected_style
    
    def _is_homepage_url(self, url):
        """æ£€æµ‹æ˜¯å¦ä¸ºé¦–é¡µé“¾æ¥ï¼ˆéœ€è¦è¿‡æ»¤æ‰ï¼‰
        ä¼˜åŒ–ï¼šæ›´å®½æ¾çš„æ£€æŸ¥ï¼Œå‡å°‘è¯¯åˆ¤
        """
        if not url or url == "#":
            return True
        
        # æå–è·¯å¾„éƒ¨åˆ†
        from urllib.parse import urlparse
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        query = parsed.query
        
        # å®½æ¾ç­–ç•¥ï¼šå¦‚æœæœ‰è·¯å¾„ï¼ˆå³ä½¿å¾ˆçŸ­ï¼‰ï¼Œå°±è®¤ä¸ºä¸æ˜¯é¦–é¡µ
        # ä¾‹å¦‚ï¼š/cba/ æˆ– /news/ ç­‰éƒ½åº”è¯¥ä¿ç•™
        if path:
            return False
        
        # å¦‚æœæœ‰æŸ¥è¯¢å‚æ•°ï¼ˆå³ä½¿åªæ˜¯ä¸€ä¸ªé—®å·ï¼‰ï¼Œä¹Ÿä¿ç•™
        # å› ä¸ºæŸäº›åŠ¨æ€ç½‘ç«™çš„URLå°±æ˜¯è¿™æ ·çš„
        # ä¾‹å¦‚ï¼šhttps://sports.sina.cn/cba/? å¯èƒ½æ˜¯æœ‰æ•ˆçš„åˆ†ç±»é¡µé¢
        
        # åªæœ‰åœ¨å®Œå…¨æ²¡æœ‰è·¯å¾„å’ŒæŸ¥è¯¢å‚æ•°æ—¶æ‰åˆ¤æ–­ä¸ºé¦–é¡µ
        if not path and not query:
            return True
        
        return False

    def _generate_search_url(self, platform, keyword):
        """Generate real search URL based on platform and keyword
        ä¿®æ”¹ï¼šä½¿ç”¨ç²¾ç¡®çš„å…³é”®è¯ç”Ÿæˆæœç´¢é“¾æ¥
        """
        # æ¸…ç†å…³é”®è¯ä¸­å¯èƒ½çš„å¹³å°åç§°
        kw_cleaned = keyword
        platform_names = ["å¾®åš", "çŸ¥ä¹", "è™æ‰‘", "bç«™", "bilibili", "æŠ–éŸ³", "å°çº¢ä¹¦"]
        for pname in platform_names:
            kw_cleaned = kw_cleaned.replace(pname, "").strip()
        
        kw_encoded = urllib.parse.quote(kw_cleaned)
        p = platform.lower()
        
        if "bç«™" in p or "bilibili" in p:
            return f"https://search.bilibili.com/all?keyword={kw_encoded}"
        elif "å°çº¢ä¹¦" in p:
            return f"https://www.xiaohongshu.com/search_result?keyword={kw_encoded}"
        elif "çŸ¥ä¹" in p:
            return f"https://www.zhihu.com/search?type=content&q={kw_encoded}"
        elif "æŠ–éŸ³" in p:
            return f"https://www.douyin.com/search/{kw_encoded}"
        elif "å¾®åš" in p:
            return f"https://s.weibo.com/weibo?q={kw_encoded}"
        elif "è™æ‰‘" in p or "hupu" in p:
            return f"https://s.hupu.com/all?q={kw_encoded}"
        else:
            return f"https://www.baidu.com/s?wd={kw_encoded}"

    def generate_images(self, user_profile, text_content, ideas, output_dir):
        """Generate images for the post"""
        word_count = len(text_content.strip())
        num_images = 1 if word_count <= 400 else (2 if word_count <= 900 else 3)
        print(f"Text length: {word_count} chars, generating {num_images} images")

        idea_types = [idea.get("angle", "") for idea in ideas]
        idea_prompt = f"Content angles: {', '.join(idea_types)}"

        image_paths = []
        max_retries = 3

        for i in range(num_images):
            image_path = os.path.join(output_dir, f"discussion_post_{i+1}.png")
            success = False
            for attempt in range(1, max_retries + 1):
                prompt = f"""
                Generate an image for a casual discussion post (image {i+1}/{num_images}):
                User Profile: {user_profile}
                Post Content: {text_content[:500]}...
                {idea_prompt}
                Requirements: Natural, relatable image suitable for social media discussion posts. 
                Can be casual, humorous, or reflective. No text in image.
                """
                try:
                    resp = requests.post(
                        f"{self.generate_base_url}/models/gemini-2.5-flash-image-preview:generateContent",
                        headers={"Content-Type": "application/json", "Authorization": f"Bearer {self.generate_api_key}"},
                        json={"contents": [{"parts": [{"text": prompt}]}], "generationConfig": {"temperature": 0.7}},
                        timeout=60
                    )
                    if resp.status_code == 200:
                        parts = resp.json().get("candidates", [{}])[0].get("content", {}).get("parts", [])
                        for part in parts:
                            if "inlineData" in part:
                                with open(image_path, "wb") as f:
                                    f.write(base64.b64decode(part["inlineData"]["data"]))
                                image_paths.append(image_path)
                                success = True
                                print(f"âœ… Image {i+1} saved")
                                break
                    if success: break
                except Exception as e:
                    print(f"âš ï¸ Image generation error: {e}")
            if not success: print(f"âŒ Image {i+1} skipped")
        return image_paths

    def _render_markdown(self, text):
        """
        Simple Markdown to HTML converter
        Converts **text** to bold styling
        NOTE: Links are handled separately as cards, not inline
        """
        # Remove any markdown links [text](url) that might have slipped through cleaning
        # We don't want inline links, only card-style links
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
        
        # Handle bold **text** -> <strong>text</strong>
        text = re.sub(r'\*\*(.*?)\*\*', r'<strong style="color: #333; font-weight: 700;">\1</strong>', text)
        
        # Handle headers (in case AI outputs them)
        text = re.sub(r'^###\s+(.*?)$', r'<h3>\1</h3>', text, flags=re.MULTILINE)
        text = re.sub(r'^##\s+(.*?)$', r'<h3>\1</h3>', text, flags=re.MULTILINE)
        
        # Handle list items
        text = re.sub(r'^\-\s+(.*?)$', r'â€¢ \1', text, flags=re.MULTILINE)
        text = re.sub(r'^\â€¢\s+(.*?)$', r'â€¢ \1', text, flags=re.MULTILINE)

        return text

    # --- FIX: å¢åŠ äº† selected_style å‚æ•° ---
    def generate_html_post(self, text_content, image_paths, links, hot_topics_summary, selected_style, output_path="discussion_post.html"):
        """Generate HTML discussion post with Markdown rendering"""
        
        # Remove "æ ‡é¢˜ï¼š" prefix if exists at the beginning
        text_content = re.sub(r'^æ ‡é¢˜ï¼š\s*', '', text_content.strip())
        text_content = re.sub(r'^Title:\s*', '', text_content.strip(), flags=re.IGNORECASE)
        
        # Split paragraphs by double newlines
        raw_paragraphs = [p for p in text_content.split('\n\n') if p.strip()]
        
        # Render Markdown for each paragraph
        processed_paragraphs = []
        for i, p in enumerate(raw_paragraphs):
            # First paragraph is likely the title - make it bold and larger
            if i == 0 and len(p) < 200:  # Titles are usually shorter
                rendered_p = f'<h2 style="font-size: 1.5em; font-weight: 700; color: #1a1a1a; margin-bottom: 0.8em; line-height: 1.4;">{self._render_markdown(p)}</h2>'
            else:
                rendered_p = self._render_markdown(p)
            processed_paragraphs.append(rendered_p)
            
        paragraphs = processed_paragraphs

        insertions = []
        
        # Insert images
        for i, img_path in enumerate(image_paths):
            insertions.append({"type": "image", "content": img_path, "index": i})
        # Insert links
        for link in links:
            insertions.append({"type": "link", "content": link})
            
        html_parts = []
        num_paras = len(paragraphs)
        
        if num_paras == 0:
            html_parts.append(text_content)
        else:
            num_inserts = len(insertions)
            if num_inserts > 0:
                step = max(1, num_paras // (num_inserts + 1))
                current_insert_idx = 0
                for i, para in enumerate(paragraphs):
                    if para.startswith('<h2') or para.startswith('<h3'):
                         html_parts.append(para)
                    else:
                         html_parts.append(f"<p>{para}</p>")
                         
                    if current_insert_idx < num_inserts:
                        if (i + 1) % step == 0 or i == num_paras - 1:
                            item = insertions[current_insert_idx]
                            if item["type"] == "image":
                                html_parts.append(self._create_image_tag(item["content"], item["index"]))
                            elif item["type"] == "link":
                                html_parts.append(self._create_link_tag(item["content"]))
                            current_insert_idx += 1
                            if i == num_paras - 1:
                                while current_insert_idx < num_inserts:
                                    item = insertions[current_insert_idx]
                                    if item["type"] == "image":
                                        html_parts.append(self._create_image_tag(item["content"], item["index"]))
                                    elif item["type"] == "link":
                                        html_parts.append(self._create_link_tag(item["content"]))
                                    current_insert_idx += 1
            else:
                for para in paragraphs:
                    if para.startswith('<h2') or para.startswith('<h3'):
                        html_parts.append(para)
                    else:
                        html_parts.append(f"<p>{para}</p>")

        html_content = "\n".join(html_parts)
                
        html_template = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>çƒ­ç‚¹è®¨è®ºå¸–å­</title>
            <style>
                body {{ font-family: 'Helvetica Neue', Helvetica, 'Microsoft YaHei', sans-serif; line-height: 1.8; max-width: 850px; margin: 0 auto; padding: 15px; background: #f8f9fa; color: #333; }}
                .post-container {{ background: white; border-radius: 10px; padding: 25px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); margin: 10px 0; }}
                
                /* Header with hot topic badge */
                .post-header {{ display: flex; align-items: center; margin-bottom: 20px; padding-bottom: 15px; border-bottom: 1px solid #f0f0f0; }} 
                .avatar {{ width: 48px; height: 48px; border-radius: 50%; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); margin-right: 15px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }} 
                .user-info {{ flex: 1; }}
                .user-info h3 {{ margin: 0; font-size: 18px; font-weight: 600; }} 
                .post-time {{ color: #999; font-size: 13px; margin-top: 4px; }} 
                .hot-badge {{ background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 4px 10px; border-radius: 12px; font-size: 12px; font-weight: bold; margin-left: 10px; display: inline-block; }}
                
                /* Content styling - casual and readable */
                .post-content {{ font-size: 16px; color: #2c3e50; letter-spacing: 0.02em; line-height: 1.9; }}
                .post-content p {{ margin: 1em 0; text-align: left; }}
                .post-content strong {{ color: #000; font-weight: 700; background: linear-gradient(to bottom, transparent 60%, #fff3cd 60%); }}
                .post-content h3 {{ font-size: 1.2em; margin-top: 1.5em; margin-bottom: 0.5em; color: #1a1a1a; }}
                
                .post-image {{ margin: 20px -25px; width: calc(100% + 50px); text-align: center; }}
                .post-image img {{ width: 100%; display: block; border-radius: 8px; }}
                .image-caption {{ color: #999; font-size: 13px; margin-top: 8px; font-style: italic; padding: 0 25px; }}

                /* Link card styling */
                .link-card {{
                    display: flex;
                    align-items: center;
                    background: #fcfcfc;
                    border: 1px solid #eee;
                    padding: 12px 15px;
                    margin: 25px 0;
                    text-decoration: none;
                    border-radius: 8px;
                    transition: all 0.2s;
                }}
                .link-card:hover {{ transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0,0,0,0.08); }}
                .link-card.platform-bç«™ {{ border-left: 5px solid #23ade5; }}
                .link-card.platform-å°çº¢ä¹¦ {{ border-left: 5px solid #ff2442; }}
                .link-card.platform-çŸ¥ä¹ {{ border-left: 5px solid #0084ff; }}
                .link-card.platform-æŠ–éŸ³ {{ border-left: 5px solid #1c1e21; }}
                .link-card.platform-å¾®åš {{ border-left: 5px solid #ea5d5c; }}
                .link-card.platform-è™æ‰‘ {{ border-left: 5px solid #ff6600; }}
                .link-card.platform-è…¾è®¯æ–°é—» {{ border-left: 5px solid #00a4ff; }}
                .link-card.platform-ç½‘æ˜“æ–°é—» {{ border-left: 5px solid #c00000; }}
                .link-card.platform-æ¾æ¹ƒæ–°é—» {{ border-left: 5px solid #2b7fd1; }}
                
                .link-info {{ flex: 1; }}
                .link-platform-tag {{ 
                    font-size: 12px; font-weight: bold; margin-bottom: 4px; display: inline-block; padding: 2px 6px; border-radius: 4px; color: white;
                }}
                .tag-bç«™ {{ background: #23ade5; }}
                .tag-å°çº¢ä¹¦ {{ background: #ff2442; }}
                .tag-çŸ¥ä¹ {{ background: #0084ff; }}
                .tag-æŠ–éŸ³ {{ background: #000; }}
                .tag-å¾®åš {{ background: #ea5d5c; }}
                .tag-è™æ‰‘ {{ background: #ff6600; }}
                .tag-è…¾è®¯æ–°é—» {{ background: #00a4ff; }}
                .tag-ç½‘æ˜“æ–°é—» {{ background: #c00000; }}
                .tag-æ¾æ¹ƒæ–°é—» {{ background: #2b7fd1; }}
                
                .link-title {{ font-weight: bold; color: #333; font-size: 15px; margin-top: 2px; }}
                .link-action {{ color: #999; font-size: 12px; margin-top: 4px; }}
                .link-icon {{ font-size: 24px; margin-right: 15px; }}

                .footer {{ margin-top:30px; border-top:1px solid #eee; padding-top:15px; color:#ccc; font-size:12px; text-align:center; }}
                .hot-topic-tag {{ display: inline-block; background: #fff3cd; color: #856404; padding: 3px 8px; border-radius: 4px; font-size: 12px; margin: 5px 5px 5px 0; }}

                @media (max-width: 600px) {{ 
                    .post-container {{ padding: 15px; }} 
                    .post-image {{ margin: 15px -15px; width: calc(100% + 30px); }}
                }}
            </style>
        </head>
        <body>
            <div class="post-container">
                <div class="post-header">
                    <div class="avatar"></div>
                    <div class="user-info">
                        <h3>è®ºå›æ´»è·ƒJR <span class="hot-badge">ğŸ”¥ {selected_style['name'].split()[0]}</span></h3>
                        <div class="post-time">{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}</div>
                    </div>
                </div>
                <div class="post-content">{html_content}</div>
                <div class="footer">
                    <div style="margin-bottom: 10px;">
                        {' '.join([f'<span class="hot-topic-tag">#{topic}</span>' for topic in ['çƒ­ç‚¹è¯é¢˜', 'è®¨è®º', 'AIç”Ÿæˆ']])}
                    </div>
                    Generated by AI â€¢ {len(image_paths)} Images â€¢ {len(links)} Links â€¢ Based on Hot Topics
                </div>
            </div>
        </body>
        </html>
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_template)
        return output_path

    def _create_image_tag(self, image_path, index):
        return f'<div class="post-image"><img src="{os.path.basename(image_path)}"><div class="image-caption">å›¾ {index + 1}</div></div>'

    def _create_link_tag(self, link_data):
        title = link_data.get('title', 'ç›¸å…³å†…å®¹')
        platform = link_data.get('platform', 'ç½‘é¡µ').strip()
        url = link_data.get('url', '#')
        
        css_class = "platform-other"
        tag_class = "tag-other"
        icon = "ğŸ”—"
        
        p = platform.lower()
        if "bç«™" in p or "bilibili" in p:
            css_class = "platform-bç«™"
            tag_class = "tag-bç«™"
            icon = "ğŸ“º"
        elif "å°çº¢ä¹¦" in p:
            css_class = "platform-å°çº¢ä¹¦"
            tag_class = "tag-å°çº¢ä¹¦"
            icon = "ğŸ“•"
        elif "çŸ¥ä¹" in p:
            css_class = "platform-çŸ¥ä¹"
            tag_class = "tag-çŸ¥ä¹"
            icon = "â“"
        elif "æŠ–éŸ³" in p:
            css_class = "platform-æŠ–éŸ³"
            tag_class = "tag-æŠ–éŸ³"
            icon = "ğŸµ"
        elif "å¾®åš" in p or "weibo" in p:
            css_class = "platform-å¾®åš"
            tag_class = "tag-å¾®åš"
            icon = "ğŸ‘ï¸"
        elif "è™æ‰‘" in p or "hupu" in p:
            css_class = "platform-è™æ‰‘"
            tag_class = "tag-è™æ‰‘"
            icon = "ğŸ€"
        elif "è…¾è®¯" in p or "qq" in p or "tencent" in p:
            css_class = "platform-è…¾è®¯æ–°é—»"
            tag_class = "tag-è…¾è®¯æ–°é—»"
            icon = "ğŸ“°"
        elif "ç½‘æ˜“" in p or "netease" in p or "163" in p:
            css_class = "platform-ç½‘æ˜“æ–°é—»"
            tag_class = "tag-ç½‘æ˜“æ–°é—»"
            icon = "ğŸ“°"
        elif "æ¾æ¹ƒ" in p or "thepaper" in p:
            css_class = "platform-æ¾æ¹ƒæ–°é—»"
            tag_class = "tag-æ¾æ¹ƒæ–°é—»"
            icon = "ğŸ“°"
            
        return f'''
        <a href="{url}" class="link-card {css_class}" target="_blank">
            <div class="link-icon">{icon}</div>
            <div class="link-info">
                <span class="link-platform-tag {tag_class}">{platform}</span>
                <div class="link-title">{title}</div>
                <div class="link-action">ç‚¹å‡»å» {platform} æŸ¥çœ‹è¯¦æƒ… &gt;</div>
            </div>
        </a>
        '''

    def _filter_relevant_links(self, text_content, hot_topics):
        """
        Filter hot topics to keep only those relevant to the post content
        ä½¿ç”¨ LLM åˆ¤æ–­å“ªäº›çƒ­ç‚¹ä¸å¸–å­å†…å®¹çœŸæ­£ç›¸å…³ï¼ˆå®½æ¾æ¨¡å¼ï¼‰
        """
        if not isinstance(hot_topics, list) or len(hot_topics) == 0:
            return []
        
        # å¦‚æœåªæœ‰1ä¸ªçƒ­ç‚¹ï¼Œç›´æ¥è¿”å›
        if len(hot_topics) == 1:
            print(f"   ğŸ“Œ Only 1 hot topic, keeping it")
            return hot_topics
        
        # å¦‚æœåªæœ‰2ä¸ªçƒ­ç‚¹ï¼Œä¹Ÿæ¯”è¾ƒå®½æ¾ï¼ˆä¿ç•™è‡³å°‘1ä¸ªï¼‰
        if len(hot_topics) == 2:
            print(f"   ğŸ“Œ Only 2 hot topics, using relaxed filtering")
        
        print(f"   ğŸ” Filtering {len(hot_topics)} hot topics for relevance...")
        
        # æ„å»ºç­›é€‰ prompt
        topics_summary = []
        for i, t in enumerate(hot_topics):
            topics_summary.append(f"{i+1}. {t.get('topic', '')} (å¹³å°: {t.get('platform', '')})")
        
        filter_prompt = f"""ä½ éœ€è¦åˆ¤æ–­å“ªäº›çƒ­ç‚¹è¯é¢˜ä¸è¿™ç¯‡å¸–å­å†…å®¹ç›¸å…³ã€‚

å¸–å­å†…å®¹ï¼ˆå‰800å­—ï¼‰ï¼š
{text_content[:800]}

å¯é€‰çš„çƒ­ç‚¹è¯é¢˜ï¼š
{chr(10).join(topics_summary)}

è¦æ±‚ï¼š
1. é€‰æ‹©ä¸å¸–å­å†…å®¹**ç›¸å…³æˆ–å¯èƒ½ç›¸å…³**çš„çƒ­ç‚¹ï¼ˆæ ‡å‡†å®½æ¾ï¼Œåªè¦æœ‰å…³è”å³å¯ï¼‰
2. å¸–å­å¯èƒ½æ˜¯åŸºäºè¿™äº›çƒ­ç‚¹å†™çš„ï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®æåˆ°çƒ­ç‚¹åç§°ï¼Œä¹Ÿå¯èƒ½åœ¨è®¨è®ºç›¸å…³è¯é¢˜
3. ä¿ç•™ 1-2 ä¸ªæœ€ç›¸å…³çš„çƒ­ç‚¹
4. **å¦‚æœä¸ç¡®å®šï¼Œå€¾å‘äºä¿ç•™è€Œéåˆ é™¤**ï¼ˆå®å¯å¤šä¸å¯å°‘ï¼‰

è¿”å›JSONæ ¼å¼ï¼ˆåªè¿”å›åºå·æ•°ç»„ï¼‰ï¼š
{{"selected": [1]}}  # è‡³å°‘ä¿ç•™1ä¸ª
"""

        try:
            resp = requests.post(
                f"{self.search_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.search_api_key}"},
                json={
                    "model": self.search_model,
                    "messages": [{"role": "user", "content": filter_prompt}],
                    "temperature": 0.5,  # æé«˜æ¸©åº¦ï¼Œæ›´å®½æ¾çš„åˆ¤æ–­
                },
                timeout=30
            )
            
            if resp.status_code != 200:
                print(f"   âš ï¸ Filter API error, keeping first topic as fallback")
                return [hot_topics[0]] if hot_topics else []
            
            resp_data = resp.json()
            content = resp_data["choices"][0]["message"]["content"].strip()
            
            # æå– JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            # æŸ¥æ‰¾ JSON å¯¹è±¡
            json_match = re.search(r'\{[^}]*"selected"[^}]*\}', content)
            if json_match:
                content = json_match.group(0)
            
            result = json.loads(content)
            selected_indices = result.get("selected", [])
            
            if not selected_indices:
                # å®½æ¾æ¨¡å¼ï¼šå¦‚æœæ²¡é€‰ä¸­ä»»ä½•çƒ­ç‚¹ï¼Œé»˜è®¤ä¿ç•™ç¬¬ä¸€ä¸ª
                print(f"   âš ï¸ No topics selected by filter, keeping first topic as fallback")
                return [hot_topics[0]] if hot_topics else []
            
            # æ ¹æ®é€‰ä¸­çš„åºå·ç­›é€‰çƒ­ç‚¹
            filtered = []
            for idx in selected_indices:
                if 1 <= idx <= len(hot_topics):
                    filtered.append(hot_topics[idx - 1])
                    print(f"   âœ… Kept topic {idx}: {hot_topics[idx - 1].get('topic', '')[:50]}...")
            
            # å¦‚æœç­›é€‰åæ²¡æœ‰ç»“æœï¼Œä¿ç•™ç¬¬ä¸€ä¸ªä½œä¸ºé™çº§
            if not filtered:
                print(f"   âš ï¸ Filtering resulted in empty list, keeping first topic")
                return [hot_topics[0]] if hot_topics else []
            
            return filtered
            
        except Exception as e:
            print(f"   âš ï¸ Filter exception ({e}), keeping first topic as fallback")
            # é™çº§ç­–ç•¥ï¼šä¿ç•™ç¬¬ä¸€ä¸ªçƒ­ç‚¹è€Œä¸æ˜¯å…¨éƒ¨
            return [hot_topics[0]] if hot_topics else []
    
    def _extract_links_from_topics(self, topics):
        """
        Extract links from filtered topics
        ä»ç­›é€‰åçš„çƒ­ç‚¹ä¸­æå–é“¾æ¥
        """
        links = []
        if not isinstance(topics, list):
            return links
            
        for topic in topics:
            if not isinstance(topic, dict):
                continue
                
            # æ–°æ ¼å¼ï¼šæœ‰ URL å’Œ title
            if topic.get("url") and topic.get("title"):
                url = topic.get("url", "")
                # è¿‡æ»¤é¦–é¡µé“¾æ¥
                if not self._is_homepage_url(url):
                    links.append({
                        "title": topic.get("title", "ç›¸å…³æ–°é—»"),
                        "platform": topic.get("platform", "ç½‘é¡µ"),
                        "url": url
                    })
                    print(f"   âœ… [æ–°æ ¼å¼] {topic.get('title', '')[:50]}...")
            
            # æ—§æ ¼å¼ï¼šåªæœ‰ search_keywordï¼Œç”Ÿæˆæœç´¢é“¾æ¥
            elif topic.get("search_keyword") and topic.get("platform"):
                keyword = topic.get("search_keyword", "")
                platform = topic.get("platform", "")
                search_url = self._generate_search_url(platform, keyword)
                
                # ä½¿ç”¨æ›´æ¸…æ™°çš„æ ‡é¢˜æ ¼å¼
                topic_desc = topic.get("topic", "")
                title = f"{topic_desc}" if topic_desc else f"{platform}æœç´¢: {keyword}"
                
                links.append({
                    "title": title[:80],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                    "platform": platform,
                    "url": search_url
                })
                print(f"   âš ï¸ [æ—§æ ¼å¼/æœç´¢] {title[:50]}... | å…³é”®è¯: {keyword}")
        
        return links
    
    def _generate_ideas_from_profile(self, user_profile):
        """
        Generate post ideas based on user profile (fallback when no hot topics)
        åŸºäºç”¨æˆ·ç”»åƒç”Ÿæˆåˆ›æ„ï¼ˆæ— çƒ­ç‚¹æ—¶çš„é™çº§æ–¹æ¡ˆï¼‰
        """
        prompt = f"""Based on the user profile, generate 2-3 discussion post ideas for a Hupu-style forum.

User Profile:
{user_profile}

Requirements:
1. Generate ideas that match the user's interests and posting style
2. Ideas should be relatable topics that don't require real-time hot topics
3. Can be about general interests, opinions, or experiences
4. One casual/fun idea, one opinion-based idea, one analytical idea

Return as JSON array:
[
  {{
    "idea": "Discussion post idea",
    "angle": "Unique perspective or approach",
    "tone": "Casual/Opinionated/Analytical"
  }}
]
"""

        try:
            resp = requests.post(
                f"{self.search_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.search_api_key}"},
                json={
                    "model": self.search_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.8,
                },
                timeout=30
            )
            
            if resp.status_code != 200:
                raise Exception(f"API error {resp.status_code}")
            
            resp_data = resp.json()
            ideas_text = resp_data["choices"][0]["message"]["content"].strip()
            
            # Extract JSON
            if "```json" in ideas_text:
                ideas_text = ideas_text.split("```json")[1].split("```")[0].strip()
            elif "```" in ideas_text:
                ideas_text = ideas_text.split("```")[1].split("```")[0].strip()
                
            ideas = json.loads(ideas_text)
            print(f"ğŸ’¡ Generated {len(ideas)} profile-based ideas")
            return ideas
            
        except Exception as e:
            print(f"âš ï¸ Idea generation error: {e}")
            # Fallback ideas
            return [{
                "idea": "åˆ†äº«æœ€è¿‘çš„è§‚ç‚¹å’Œæƒ³æ³•",
                "angle": "ä¸ªäººç»éªŒå’Œè§è§£",
                "tone": "Casual"
            }]
    
    def _generate_text_from_profile(self, user_profile, ideas, user_profile_path=None, profile_data=None):
        """
        Generate text based on user profile and examples (no hot topics)
        åŸºäºç”¨æˆ·ç”»åƒå’Œæ ·ä¾‹ç”Ÿæˆå†…å®¹ï¼ˆæ— çƒ­ç‚¹ï¼‰
        
        Args:
            user_profile: User profile text
            ideas: Ideas list
            user_profile_path: Path to user profile (for RAG)
            profile_data: Profile data dict (for RAG)
        """
        # åŠ è½½æ ·ä¾‹ (æ”¯æŒRAGæ¨¡å¼)
        examples = self.load_examples(user_profile_path=user_profile_path, profile_data=profile_data)
        
        if not examples:
            print("âš ï¸ No examples, using basic fallback")
            return self._generate_text_basic_fallback(user_profile, ideas)
        
        print(f"âœ¨ Generating profile-based text using {len(examples)} example(s)")
        
        # æ„å»ºæ ·ä¾‹æ–‡æœ¬
        examples_text_parts = []
        for i, ex in enumerate(examples):
            if ex.get('title'):
                examples_text_parts.append(
                    f"ã€æ ·ä¾‹ {i+1}ã€‘\næ ‡é¢˜ï¼š{ex['title']}\n\n{ex['content']}"
                )
            else:
                examples_text_parts.append(
                    f"ã€æ ·ä¾‹ {i+1}ã€‘\n{ex['content']}"
                )
        
        examples_text = "\n\n---\n\n".join(examples_text_parts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè™æ‰‘è€ç”¨æˆ·ï¼Œéœ€è¦åˆ›ä½œä¸€ç¯‡è®¨è®ºå¸–ã€‚

**æ ¸å¿ƒä»»åŠ¡ï¼šå®Œå…¨æ¨¡ä»¿ä»¥ä¸‹çœŸå®æ ·ä¾‹çš„é£æ ¼ï¼ˆæƒé‡90%ï¼‰**

{examples_text}

---

**å‚è€ƒä¿¡æ¯ï¼š**
- ä½ çš„èƒŒæ™¯ï¼š{user_profile}
- è¯é¢˜æ–¹å‘ï¼š{json.dumps(ideas, ensure_ascii=False, indent=2)}

**åˆ›ä½œè¦æ±‚ï¼š**
1. **å®Œå…¨æ¨¡ä»¿æ ·ä¾‹çš„é£æ ¼ã€è¯­æ°”ã€ç”¨è¯**
2. å­—æ•°200-400å­—ä»¥å†…
3. ç›´æ¥ã€ç®€æ´ï¼Œä¸åºŸè¯
4. å¦‚æœæ˜¯ç†æ€§åˆ†æç±»ï¼Œå¯ä»¥ç”¨"ç†æ€§è®¨è®º"å¼€å¤´
5. **å¦‚æœæœ‰æ ‡é¢˜ï¼Œç›´æ¥è¾“å‡ºæ ‡é¢˜å†…å®¹ï¼Œä¸è¦åŠ "æ ‡é¢˜ï¼š"ç­‰å‰ç¼€**

**å…³é”®åŸåˆ™ï¼šæ ·ä¾‹æ€ä¹ˆå†™ï¼Œä½ å°±æ€ä¹ˆå†™**

è¯·å¼€å§‹åˆ›ä½œï¼ˆåªè¾“å‡ºå¸–å­æ­£æ–‡ï¼‰ï¼š
"""

        try:
            resp = requests.post(
                f"{self.search_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.search_api_key}"},
                json={
                    "model": self.search_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                },
                timeout=60
            )
            
            if resp.status_code != 200:
                raise Exception(f"API error {resp.status_code}")
            
            resp_data = resp.json()
            full_content = resp_data["choices"][0]["message"]["content"].strip()
            
            # æ¸…ç†
            full_content = re.sub(r'^>.*?$', '', full_content, flags=re.MULTILINE)
            full_content = re.sub(r'\n{3,}', '\n\n', full_content)
            full_content = full_content.strip()
            
            # å­—æ•°æ£€æŸ¥å’Œæˆªæ–­
            full_content = self._ensure_concise_text(full_content)
            
            print(f"âœ… Generated {len(full_content)} chars (profile-based)")
            
            return full_content, [], {"name": "ç”¨æˆ·ç”»åƒé£æ ¼", "desc": "åŸºäºç”¨æˆ·ç”»åƒå’Œæ ·ä¾‹"}
            
        except Exception as e:
            print(f"âš ï¸ Profile-based generation error: {e}")
            return self._generate_text_basic_fallback(user_profile, ideas)
    
    def _generate_text_basic_fallback(self, user_profile, ideas):
        """Basic fallback when everything else fails"""
        text = f"""JRså¥½ï¼Œæƒ³èŠä¸ªäº‹ã€‚

{ideas[0].get('idea', 'æœ€è¿‘çš„ä¸€äº›æƒ³æ³•')}

å¤§å®¶æ€ä¹ˆçœ‹ï¼Ÿæ¬¢è¿è®¨è®ºğŸ¤"""
        
        print(f"âš ï¸ Using basic fallback text ({len(text)} chars)")
        return text, [], {"name": "åŸºç¡€é™çº§", "desc": "æœ€ç®€å•çš„é™çº§æ–¹æ¡ˆ"}

    def __call__(self, user_profile_path, output_dir):
        """Main execution flow - ç¡®ä¿å†…å®¹ä¸€å®šä¼šç”Ÿæˆ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Load user profile
        with open(user_profile_path, 'r', encoding='utf-8') as f:
            profile_data = json.load(f)
        
        user_profile = profile_data.get("profile_text", json.dumps(profile_data, ensure_ascii=False))
        
        print("\nğŸ” Step 1: Searching for hot topics...")
        hot_topics = self.search_hot_topics(user_profile)
        
        # æ–°é€»è¾‘ï¼šå³ä½¿æœç´¢å¤±è´¥ä¹Ÿç»§ç»­ï¼Œè€Œä¸æ˜¯ç›´æ¥é€€å‡º
        use_fallback_mode = False
        if hot_topics == "NO_VERIFIED_TRENDS_FOUND":
            print("âš ï¸ Web search failed. Using profile-based content generation mode.")
            use_fallback_mode = True
            # ä¸å†è¿”å›é”™è¯¯ï¼Œè€Œæ˜¯ç»§ç»­ç”Ÿæˆ

        # --- ä¿å­˜çƒ­ç‚¹ä¿¡æ¯ ---
        topics_save_path = os.path.join(output_dir, "topics.json")
        parsed_topics = []
        raw_text_content = ""

        try:
            if isinstance(hot_topics, list):
                parsed_topics = [t.get('topic', 'æœªçŸ¥è¯é¢˜') for t in hot_topics]
                raw_text_content = json.dumps(hot_topics, ensure_ascii=False, indent=2)
            else:
                parsed_topics = ["åŸºäºç”¨æˆ·ç”»åƒç”Ÿæˆï¼ˆæ— å®æ—¶çƒ­ç‚¹ï¼‰"]
                raw_text_content = "Fallback mode: No hot topics available"

            with open(topics_save_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "mode": "fallback" if use_fallback_mode else "hot_topics",
                    "structured_topics": parsed_topics,
                    "raw_text": raw_text_content
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Topics info saved to: {topics_save_path}")
        except Exception as e:
            print(f"âš ï¸ Failed to save topics.json: {e}")
        
        # --- ç”Ÿæˆåˆ›æ„å’Œå†…å®¹ ---
        if use_fallback_mode:
            # é™çº§æ¨¡å¼ï¼šåŸºäºç”¨æˆ·ç”»åƒç”Ÿæˆ
            print("\nğŸ’¡ Step 2: Generating post ideas from user profile (fallback mode)...")
            ideas = self._generate_ideas_from_profile(user_profile)
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šåŸºäºçƒ­ç‚¹ç”Ÿæˆ
            print("\nğŸ’¡ Step 2: Generating post ideas from hot topics...")
            ideas = self.generate_idea_from_topics(user_profile, hot_topics)
        
        # Only use the first idea
        if len(ideas) > 1:
            print(f"ğŸ’¡ Generated {len(ideas)} ideas, using only the first one")
            ideas = [ideas[0]]
        else:
            print(f"ğŸ’¡ Generated {len(ideas)} idea")
        
        # Save ideas
        with open(os.path.join(output_dir, "discussion_ideas.json"), 'w', encoding='utf-8') as f:
            json.dump(ideas, f, ensure_ascii=False, indent=2)
        
        print("\nğŸ“ Step 3: Generating discussion post text...")
        if use_fallback_mode:
            # é™çº§æ¨¡å¼ï¼šåŸºäºç”¨æˆ·ç”»åƒå’Œæ ·ä¾‹ç”Ÿæˆ
            text, _, style = self._generate_text_from_profile(user_profile, ideas, 
                                                              user_profile_path=user_profile_path,
                                                              profile_data=profile_data)
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šåŸºäºçƒ­ç‚¹ç”Ÿæˆ
            text, _, style = self.generate_text(user_profile, hot_topics, ideas,
                                                user_profile_path=user_profile_path,
                                                profile_data=profile_data)
        
        # --- å¤„ç†é“¾æ¥ ---
        links = []
        if use_fallback_mode:
            # é™çº§æ¨¡å¼ï¼šä¸æ·»åŠ é“¾æ¥ï¼ˆå› ä¸ºæ²¡æœ‰ç›¸å…³çƒ­ç‚¹ï¼‰
            print("\nğŸ”— Step 4: Skipping link generation (fallback mode, no hot topics)")
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šç­›é€‰å’Œæå–é“¾æ¥
            print("\nğŸ¯ Step 4: Filtering relevant hot topics for the post...")
            relevant_topics = self._filter_relevant_links(text, hot_topics)
            
            print(f"\nğŸ”— Step 5: Extracting links from {len(relevant_topics)} relevant topic(s)...")
            links = self._extract_links_from_topics(relevant_topics)
        
        if links:
            print(f"   ğŸ“Š Total {len(links)} links extracted")
        else:
            print(f"   â„¹ï¸ No links added to this post")
        
        print("\nğŸ–¼ï¸ Step 6: Generating images...")
        # images = self.generate_images(user_profile, text, ideas, output_dir)
        images = []  # ---æš‚ä¸”å…³é—­imageåŠŸèƒ½---

        print("\nğŸŒ Step 7: Generating HTML...")
        html_path = os.path.join(output_dir, "discussion_post_v0.html")
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"   ğŸ“ Text: {len(text)} chars")
        print(f"   ğŸ–¼ï¸  Images: {len(images)}")
        print(f"   ğŸ”— Links: {len(links)}")
        if links:
            for i, link in enumerate(links):
                print(f"      Link {i+1}: {link.get('title', '')[:40]}... [{link.get('platform', '')}]")
        
        self.generate_html_post(text, images, links, parsed_topics, style, html_path)
        print(f"âœ… Complete: {html_path}")
        
        # ========================= Reflection Mechanism =========================
        # è¯„ä¼°é“¾æ¥-æ–‡æœ¬ç›¸å…³æ€§ï¼Œå¦‚æœä½äºé˜ˆå€¼åˆ™è§¦å‘reflection
        current_html_path = html_path
        reflection_history = []
        removed_links_history = []  # è®°å½•æ‰€æœ‰å·²åˆ é™¤çš„é“¾æ¥ï¼ˆé¿å…é‡å¤æ·»åŠ ï¼‰
        
        # è¿ç»­ä¸å˜è®¡æ•°å™¨ï¼ˆç”¨äºæå‰åœæ­¢ï¼‰
        no_improvement_count = 0
        last_best_score = None
        NO_IMPROVEMENT_THRESHOLD = 5  # è¿ç»­5-6æ¬¡ä¸å˜ååœæ­¢ï¼ˆå¯é…ç½®ï¼‰
        
        # åªåœ¨éfallbackæ¨¡å¼ä¸”æœ‰é“¾æ¥æ—¶æ‰è¿›è¡Œreflection
        if self.reflection_enabled and not use_fallback_mode and links:
            print("\n" + "="*80)
            print(f"ğŸ”„ å¯åŠ¨Reflectionæœºåˆ¶ï¼ˆè™æ‰‘é“¾æ¥-æ–‡æœ¬è¯„ä¼°ï¼Œæœ€å¤š{self.max_reflection_iterations}æ¬¡è¿­ä»£ï¼‰")
            print(f"   é˜ˆå€¼: Link-Text GroupScore â‰¥ {self.reflection_threshold}")
            print(f"   ä¼˜åŒ–: ç¼“å­˜+å¹¶è¡ŒåŒ–ï¼Œè¿ç»­{NO_IMPROVEMENT_THRESHOLD}æ¬¡ä¸å˜è‡ªåŠ¨åœæ­¢")
            print("="*80)
            
            for iteration in range(self.max_reflection_iterations):
                print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                print(f"ğŸ“Š ç¬¬{iteration+1}æ¬¡è¯„ä¼° (å½“å‰ç‰ˆæœ¬: v{iteration})")
                print(f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                
                # 1. è®¡ç®—Link-Text GroupScore
                print("\n1ï¸âƒ£  è®¡ç®—Link-Text GroupScore...")
                try:
                    eval_result = evaluate_file_links(
                        html_path=current_html_path,
                        evaluator=self.text_evaluator,
                        verbose=False
                    )
                    groupscore = eval_result.group_score_mean  # ä½¿ç”¨ç®—æœ¯å¹³å‡ï¼ˆæ›´ç¨³å®šï¼‰
                    print(f"   âœ… GroupScore (Mean): {groupscore:.4f}")
                    print(f"      (Harmonic: {eval_result.group_score_harmonic:.4f})")
                    print(f"      - è¯„ä¼°äº† {eval_result.num_pairs} ä¸ªé“¾æ¥")
                except Exception as e:
                    print(f"   âŒ GroupScoreè®¡ç®—å¤±è´¥: {e}")
                    groupscore = 0.0  # å¤±è´¥åˆ™è®¾ä¸º0ï¼Œå¼ºåˆ¶è§¦å‘reflection
                
                reflection_history.append({
                    "version": f"v{iteration}",
                    "groupscore": groupscore,
                    "html_path": current_html_path
                })
                
                # 2. åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
                if groupscore >= self.reflection_threshold:
                    print(f"\n   âœ… GroupScore {groupscore:.4f} è¾¾åˆ°é˜ˆå€¼ {self.reflection_threshold}ï¼Œåœæ­¢Reflectionã€‚")
                    break
                
                # 2.5. æ£€æŸ¥è¿ç»­ä¸å˜ï¼ˆæå‰åœæ­¢ä¼˜åŒ–ï¼‰
                if last_best_score is not None:
                    # è·å–å½“å‰æœ€ä½³åˆ†æ•°
                    current_best = max(reflection_history, key=lambda x: x.get("groupscore", 0))
                    current_best_score = current_best["groupscore"]
                    
                    # å¦‚æœæœ€ä½³åˆ†æ•°æ²¡æœ‰æå‡ï¼ˆå…è®¸å°çš„æµ®åŠ¨ï¼Œ0.001ï¼‰
                    if abs(current_best_score - last_best_score) < 0.001:
                        no_improvement_count += 1
                        print(f"\n   â„¹ï¸  è¿ç»­ {no_improvement_count} æ¬¡è¿­ä»£æœ€ä½³åˆ†æ•°æœªæå‡ ({current_best_score:.4f})")
                        if no_improvement_count >= NO_IMPROVEMENT_THRESHOLD:
                            print(f"   â¹ï¸  è¿ç»­ {NO_IMPROVEMENT_THRESHOLD} æ¬¡ä¸å˜ï¼Œæå‰åœæ­¢Reflection")
                            break
                    else:
                        # æœ‰æå‡ï¼Œé‡ç½®è®¡æ•°å™¨
                        no_improvement_count = 0
                        last_best_score = current_best_score
                else:
                    # ç¬¬ä¸€æ¬¡è¿­ä»£ï¼Œè®°å½•åˆå§‹æœ€ä½³åˆ†æ•°
                    last_best_score = groupscore
                
                # ğŸ¯ ç¬¬2+æ¬¡Reflectionï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å›é€€åˆ°å†å²æœ€ä½³ç‰ˆæœ¬
                if iteration >= 1 and len(reflection_history) >= 2:
                    # æ‰¾å‡ºå†å²æœ€ä½³ç‰ˆæœ¬
                    best_record = max(reflection_history, key=lambda x: x.get("groupscore", 0))
                    best_score = best_record["groupscore"]
                    best_html_path = best_record["html_path"]
                    best_version = best_record["version"]
                    
                    print(f"\nğŸ’¡ å†å²æœ€ä½³ç‰ˆæœ¬æ£€æŸ¥:")
                    print(f"   - å†å²æœ€é«˜åˆ†: {best_version} ({best_score:.4f})")
                    print(f"   - å½“å‰åˆ†æ•°: v{iteration} ({groupscore:.4f})")
                    
                    # å¦‚æœå½“å‰ç‰ˆæœ¬ä¸æ˜¯æœ€ä½³ç‰ˆæœ¬ï¼Œä¸”å·®è·æ˜æ˜¾ï¼Œåˆ‡æ¢åˆ°æœ€ä½³ç‰ˆæœ¬
                    if current_html_path != best_html_path and groupscore < best_score:
                        score_gap = best_score - groupscore
                        print(f"   ğŸ”„ åˆ†æ•°å·®è· {score_gap:.4f}ï¼Œåˆ‡æ¢åˆ°æœ€ä½³ç‰ˆæœ¬è¿›è¡Œä¼˜åŒ–")
                        current_html_path = best_html_path
                        groupscore = best_score
                        
                        # æ›´æ–°reflection_historyï¼Œæ ‡è®°è¿™æ¬¡åˆ‡æ¢
                        reflection_history[-1]['switched_to_best'] = True
                        
                        # å¦‚æœåˆ‡æ¢åå·²ç»è¾¾æ ‡ï¼Œç›´æ¥ç»“æŸ
                        if groupscore >= self.reflection_threshold:
                            print(f"   âœ… æœ€ä½³ç‰ˆæœ¬å·²è¾¾æ ‡ï¼Œæ— éœ€ç»§ç»­ä¼˜åŒ–")
                            break
                    else:
                        print(f"   âœ… å½“å‰ç‰ˆæœ¬å·²æ˜¯æœ€ä½³æˆ–æ¥è¿‘æœ€ä½³")
                
                # 3. åˆ†æå¹¶æ”¹è¿›
                print(f"\n3ï¸âƒ£  åˆ†æé“¾æ¥ç›¸å…³æ€§é—®é¢˜...")
                print(f"   âš ï¸  GroupScore {groupscore:.4f} ä½äºé˜ˆå€¼ {self.reflection_threshold}")
                
                # æå–åˆ†æ•°ä½çš„é“¾æ¥
                low_score_links = []
                if hasattr(eval_result, 'pair_scores') and eval_result.pair_scores:
                    for pair_score in eval_result.pair_scores:
                        if pair_score.get('combined_score', 0) < self.reflection_threshold:
                            low_score_links.append({
                                "title": pair_score.get('link_title', ''),
                                "platform": pair_score.get('link_platform', ''),
                                "score": pair_score.get('combined_score', 0)
                            })
                
                if low_score_links:
                    print(f"   ğŸ“‰ å‘ç° {len(low_score_links)} ä¸ªä½ç›¸å…³æ€§é“¾æ¥:")
                    for link in low_score_links[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        print(f"      - {link['title'][:50]}... [åˆ†æ•°: {link['score']:.4f}]")
                
                # 4. åº”ç”¨æ”¹è¿›ï¼ˆç§»é™¤ä½ç›¸å…³æ€§é“¾æ¥ + é‡æ–°ç”Ÿæˆæ–°é“¾æ¥ï¼‰
                print(f"\n4ï¸âƒ£  åº”ç”¨æ”¹è¿›...")
                result = self._apply_reflection_suggestions_hupu(
                    current_html_path,
                    eval_result,
                    self.reflection_threshold,
                    iteration,
                    text_content=text,
                    hot_topics=hot_topics,
                    removed_links_history=removed_links_history,
                    user_profile=user_profile  # æ·»åŠ user_profileå‚è€ƒ
                )
                
                # è§£æè¿”å›å€¼ï¼ˆå¯èƒ½æ˜¯å…ƒç»„æˆ–å­—å…¸ï¼‰
                if isinstance(result, tuple):
                    new_html_path, newly_removed = result
                    new_links_added = 0  # é»˜è®¤å€¼ï¼Œå¦‚æœå‡½æ•°è¿”å›äº†æ›´å¤šä¿¡æ¯éœ€è¦æ›´æ–°
                    text_optimized = False
                else:
                    # å¦‚æœè¿”å›å­—å…¸ï¼Œæå–ä¿¡æ¯
                    new_html_path = result.get('html_path')
                    newly_removed = result.get('removed_links', [])
                    new_links_added = result.get('new_links_added', 0)
                    text_optimized = result.get('text_optimized', False)
                
                # æ›´æ–°åˆ é™¤å†å²
                if newly_removed:
                    removed_links_history.extend(newly_removed)
                
                if new_html_path:
                    # ä¼˜åŒ–éªŒè¯ï¼šåªåœ¨æœ‰æ˜æ˜¾æ”¹è¿›é¢„æœŸæ—¶æ‰éªŒè¯ï¼ˆå‡å°‘è¯„ä¼°æ¬¡æ•°ï¼‰
                    # ç­–ç•¥ï¼šå¦‚æœæ·»åŠ äº†æ–°é“¾æ¥æˆ–ä¼˜åŒ–äº†æ–‡æœ¬ï¼Œæ‰è¿›è¡ŒéªŒè¯
                    # å¦‚æœåªæ˜¯åˆ é™¤äº†é“¾æ¥ï¼Œç›´æ¥é‡‡ç”¨ï¼ˆåˆ é™¤ä½åˆ†é“¾æ¥é€šå¸¸ä¸ä¼šé™ä½åˆ†æ•°ï¼‰
                    should_verify = True
                    verify_reason = "ä¿®æ”¹åéªŒè¯"
                    
                    # æ£€æŸ¥æ˜¯å¦åªæ˜¯åˆ é™¤é“¾æ¥ï¼ˆæ²¡æœ‰æ·»åŠ æ–°é“¾æ¥æˆ–ä¼˜åŒ–æ–‡æœ¬ï¼‰
                    if new_links_added == 0 and not text_optimized:
                        # åªåˆ é™¤é“¾æ¥ï¼Œé€šå¸¸ä¸ä¼šé™ä½åˆ†æ•°ï¼Œå¯ä»¥è·³è¿‡éªŒè¯
                        should_verify = False
                        verify_reason = "ä»…åˆ é™¤é“¾æ¥ï¼Œè·³è¿‡éªŒè¯ï¼ˆé€šå¸¸ä¸ä¼šé™ä½åˆ†æ•°ï¼‰"
                    
                    if should_verify:
                        print(f"\n5ï¸âƒ£  éªŒè¯æ–°ç‰ˆæœ¬æ•ˆæœ...")
                        try:
                            new_eval_result = evaluate_file_links(
                                html_path=new_html_path,
                                evaluator=self.text_evaluator,
                                verbose=False
                            )
                            new_groupscore = new_eval_result.group_score_mean
                            score_delta = new_groupscore - groupscore
                            
                            print(f"   ğŸ“Š ä¿®æ”¹å‰: {groupscore:.4f}")
                            print(f"   ğŸ“Š ä¿®æ”¹å: {new_groupscore:.4f}")
                            print(f"   ğŸ“Š å˜åŒ–: {score_delta:+.4f}")
                            
                            if new_groupscore >= groupscore:
                                # åˆ†æ•°æå‡æˆ–æŒå¹³ï¼Œé‡‡ç”¨æ–°ç‰ˆæœ¬
                                current_html_path = new_html_path
                                print(f"   âœ… åˆ†æ•°{'æå‡' if score_delta > 0 else 'æŒå¹³'}ï¼Œé‡‡ç”¨æ–°ç‰ˆæœ¬")
                            else:
                                # åˆ†æ•°ä¸‹é™ï¼Œä¸é‡‡ç”¨æ–°ç‰ˆæœ¬
                                print(f"   âš ï¸  åˆ†æ•°ä¸‹é™ï¼Œä¿ç•™åŸç‰ˆæœ¬")
                                # åˆ é™¤æ–°ç”Ÿæˆçš„HTMLæ–‡ä»¶
                                if os.path.exists(new_html_path):
                                    os.remove(new_html_path)
                        except Exception as e:
                            print(f"   âš ï¸  éªŒè¯å¤±è´¥: {e}")
                            # éªŒè¯å¤±è´¥ï¼Œä¿å®ˆèµ·è§ä¸é‡‡ç”¨æ–°ç‰ˆæœ¬
                            if os.path.exists(new_html_path):
                                os.remove(new_html_path)
                    else:
                        # è·³è¿‡éªŒè¯ï¼Œç›´æ¥é‡‡ç”¨æ–°ç‰ˆæœ¬
                        print(f"\n5ï¸âƒ£  {verify_reason}ï¼Œç›´æ¥é‡‡ç”¨æ–°ç‰ˆæœ¬")
                        current_html_path = new_html_path
                else:
                    # ç¬¬3æ¬¡åŠä»¥åçš„åæ€ï¼Œå¦‚æœä¿®æ”¹å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£
                    # å‰3æ¬¡å¦‚æœä¿®æ”¹å¤±è´¥ï¼Œå¯èƒ½çœŸçš„æ²¡æœ‰éœ€è¦æ”¹è¿›çš„åœ°æ–¹ï¼Œå¯ä»¥åœæ­¢
                    if iteration >= 2:
                        iteration_num = iteration + 1
                        print(f"   â„¹ï¸  æ— éœ€ä¿®æ”¹æˆ–ä¿®æ”¹å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£ï¼ˆç¬¬{iteration_num}æ¬¡åæ€ï¼‰")
                        # ç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£ï¼Œä¸break
                        continue
                    else:
                        print(f"   â„¹ï¸  æ— éœ€ä¿®æ”¹æˆ–ä¿®æ”¹å¤±è´¥ï¼Œåœæ­¢Reflectionã€‚")
                        break
            
            # æœ€åä¸€æ¬¡è¿­ä»£åï¼Œå†è¯„ä¼°ä¸€æ¬¡æœ€ç»ˆç‰ˆæœ¬çš„åˆ†æ•°
            if current_html_path != html_path:  # å¦‚æœæœ‰ç”Ÿæˆæ–°ç‰ˆæœ¬
                print(f"\nğŸ“Š è¯„ä¼°æœ€ç»ˆç‰ˆæœ¬...")
                try:
                    final_eval_result = evaluate_file_links(
                        html_path=current_html_path,
                        evaluator=self.text_evaluator,
                        verbose=False
                    )
                    final_groupscore = final_eval_result.group_score_mean  # ä½¿ç”¨ç®—æœ¯å¹³å‡
                    print(f"   âœ… æœ€ç»ˆ GroupScore (Mean): {final_groupscore:.4f}")
                    print(f"      (Harmonic: {final_eval_result.group_score_harmonic:.4f})")
                    
                    # æ›´æ–°æœ€åä¸€ä¸ªç‰ˆæœ¬çš„åˆ†æ•°ï¼ˆå¦‚æœæ˜¯breakå‡ºæ¥çš„ï¼Œå·²ç»æœ‰äº†ï¼›å¦‚æœæ˜¯æœ€åä¸€æ¬¡è¿­ä»£ï¼Œéœ€è¦æ›´æ–°ï¼‰
                    if reflection_history and reflection_history[-1]["html_path"] == current_html_path:
                        reflection_history[-1]["groupscore"] = final_groupscore
                    else:
                        # æ·»åŠ æœ€ç»ˆç‰ˆæœ¬è®°å½•
                        reflection_history.append({
                            "version": f"v{len(reflection_history)}",
                            "groupscore": final_groupscore,
                            "html_path": current_html_path
                        })
                except Exception as e:
                    print(f"   âš ï¸  æœ€ç»ˆè¯„ä¼°å¤±è´¥: {e}")
            
            # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ç‰ˆæœ¬
            if reflection_history:
                best_version = max(reflection_history, key=lambda x: x.get("groupscore", 0))
                best_html_path = best_version["html_path"]
                best_score = best_version["groupscore"]
                best_version_name = best_version["version"]
                
                print(f"\nğŸ“Š Reflectionæ€»ç»“:")
                print(f"   - æ€»è¿­ä»£æ¬¡æ•°: {len(reflection_history)}")
                print(f"   - æœ€é«˜åˆ†ç‰ˆæœ¬: {best_version_name} (åˆ†æ•°: {best_score:.4f})")
                for hist in reflection_history:
                    indicator = "ğŸ‘‘" if hist["html_path"] == best_html_path else "  "
                    print(f"   {indicator} {hist['version']}: {hist['groupscore']:.4f}")
                
                current_html_path = best_html_path
                print(f"\nâœ… é€‰æ‹©ç‰ˆæœ¬ {best_version_name} ç”¨äºæœ€ç»ˆè¾“å‡ºå’Œè‹±æ–‡è½¬æ¢")
            
            print("\n" + "="*80)
            print(f"âœ… Reflectionæœºåˆ¶ç»“æŸã€‚æœ€ç»ˆç‰ˆæœ¬: {os.path.basename(current_html_path)}")
            print("="*80)
            
            # æ›´æ–°html_pathä¸ºæœ€ç»ˆç‰ˆæœ¬ï¼ˆæœ€é«˜åˆ†ç‰ˆæœ¬ï¼‰
            html_path = current_html_path
        elif self.reflection_enabled and use_fallback_mode:
            print("\nâš ï¸  Fallbackæ¨¡å¼æ— é“¾æ¥ï¼Œè·³è¿‡Reflectionè¯„ä¼°")
        elif self.reflection_enabled and not links:
            print("\nâš ï¸  æœ¬å¸–æ— é“¾æ¥ï¼Œè·³è¿‡Reflectionè¯„ä¼°")
        # ========================= End of Reflection =========================
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "text": text, 
            "images": images, 
            "links": links, 
            "hot_topics": hot_topics if not use_fallback_mode else [],
            "ideas": ideas,
            "html_post": html_path,
            "style": style,
            "mode": "fallback" if use_fallback_mode else "hot_topics",
            "reflection_history": reflection_history if reflection_history else None
        }
        
        # å¦‚æœæ˜¯é™çº§æ¨¡å¼ï¼Œæ·»åŠ è¯´æ˜
        if use_fallback_mode:
            result["note"] = "Generated from user profile (no hot topics available)"
        
        return result
    
    def _apply_reflection_suggestions_hupu(self, html_path, eval_result, threshold, iteration, 
                                           text_content=None, hot_topics=None, removed_links_history=None,
                                           user_profile=None):
        """
        åº”ç”¨Reflectionæ”¹è¿›å»ºè®®ï¼ˆè™æ‰‘ç‰ˆæœ¬ï¼šç§»é™¤ä½ç›¸å…³æ€§é“¾æ¥ + é‡æ–°ç”Ÿæˆæ–°é“¾æ¥ï¼‰
        
        Args:
            html_path: å½“å‰HTMLæ–‡ä»¶è·¯å¾„
            eval_result: è¯„ä¼°ç»“æœï¼ˆGroupScoreResultå¯¹è±¡ï¼‰
            threshold: ç›¸å…³æ€§é˜ˆå€¼
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            text_content: å¸–å­æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºé‡æ–°ç­›é€‰ç›¸å…³é“¾æ¥ï¼‰
            hot_topics: çƒ­ç‚¹è¯é¢˜åˆ—è¡¨ï¼ˆç”¨äºé‡æ–°ç”Ÿæˆé“¾æ¥ï¼‰
            removed_links_history: å·²åˆ é™¤é“¾æ¥çš„å†å²è®°å½•ï¼ˆé¿å…é‡å¤æ·»åŠ ï¼‰
            user_profile: ç”¨æˆ·ç”»åƒï¼ˆç”¨äºé‡æ–°æœç´¢çƒ­ç‚¹è¯é¢˜æ—¶å‚è€ƒï¼‰
            
        Returns:
            (æ–°HTMLæ–‡ä»¶è·¯å¾„, æœ¬æ¬¡åˆ é™¤çš„é“¾æ¥åˆ—è¡¨)ï¼Œå¦‚æœæ— éœ€ä¿®æ”¹åˆ™è¿”å›(None, [])
        """
        if removed_links_history is None:
            removed_links_history = []
        from bs4 import BeautifulSoup
        
        try:
            # è¯»å–å½“å‰HTML
            with open(html_path, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
            
            # æ‰¾åˆ°æ‰€æœ‰é“¾æ¥
            content_div = soup.find('div', class_='post-content')
            if not content_div:
                print("   âš ï¸  æœªæ‰¾åˆ°post-content div")
                return {
                    'html_path': None,
                    'removed_links': [],
                    'new_links_added': 0,
                    'text_optimized': False
                }
            
            link_cards = content_div.find_all('a', class_='link-card')
            
            if not link_cards:
                print("   âš ï¸  æœªæ‰¾åˆ°ä»»ä½•é“¾æ¥")
                return {
                    'html_path': None,
                    'removed_links': [],
                    'new_links_added': 0,
                    'text_optimized': False
                }
            
            # åˆ†æå“ªäº›é“¾æ¥éœ€è¦ç§»é™¤
            links_to_remove = []
            if hasattr(eval_result, 'pair_scores') and eval_result.pair_scores:
                for i, pair_score in enumerate(eval_result.pair_scores):
                    combined_score = pair_score.get('combined_score', 0)
                    if combined_score < threshold:
                        link_title = pair_score.get('link_title', '')
                        links_to_remove.append({
                            "index": i,
                            "title": link_title,
                            "score": combined_score
                        })
            
            if not links_to_remove:
                print("   â„¹ï¸  æ‰€æœ‰é“¾æ¥ç›¸å…³æ€§å‡è¾¾æ ‡ï¼Œæ— éœ€ç§»é™¤")
                return {
                    'html_path': None,
                    'removed_links': [],
                    'new_links_added': 0,
                    'text_optimized': False
                }
            
            print(f"   ğŸ—‘ï¸  å‡†å¤‡ç§»é™¤ {len(links_to_remove)} ä¸ªä½ç›¸å…³æ€§é“¾æ¥:")
            for link_info in links_to_remove:
                print(f"      - {link_info['title'][:50]}... [åˆ†æ•°: {link_info['score']:.4f}]")
            
            # å…ˆè®°å½•è¦åˆ é™¤çš„é“¾æ¥ä¿¡æ¯ï¼ˆä½†å…ˆä¸åˆ é™¤ï¼Œç­‰ç¡®è®¤æœ‰æ–°é“¾æ¥å¯æ›¿ä»£ï¼‰
            removed_count = 0
            newly_removed_titles = []
            links_to_remove_elements = []
            
            for link_info in links_to_remove:
                idx = link_info['index']
                if idx < len(link_cards):
                    link_card = link_cards[idx]
                    # è®°å½•é“¾æ¥ä¿¡æ¯
                    title_div = link_card.find('div', class_='link-title')
                    if title_div:
                        link_title = title_div.get_text(strip=True)
                        link_url = link_card.get('href', '')
                        newly_removed_titles.append({
                            "title": link_title,
                            "url": link_url,
                            "element": link_card  # ä¿å­˜å…ƒç´ å¼•ç”¨
                        })
                        links_to_remove_elements.append(link_card)
            
            print(f"   â¸ï¸  æš‚å­˜ {len(newly_removed_titles)} ä¸ªå¾…å¤„ç†é“¾æ¥")
            
            # =================== ç­–ç•¥é€‰æ‹©ï¼šæ·»åŠ æ–°é“¾æ¥ OR ä¼˜åŒ–æ–‡æœ¬ ===================
            new_links_added = 0
            text_optimized = False
            
            if text_content and hot_topics:
                print(f"\n   ğŸ”„ å°è¯•é‡æ–°ç”Ÿæˆæ›´ç›¸å…³çš„é“¾æ¥...")
                
                try:
                    # 1. ç­›é€‰ç›¸å…³è¯é¢˜
                    relevant_topics = self._filter_relevant_links(text_content, hot_topics)
                    print(f"      âœ… ç­›é€‰å‡º {len(relevant_topics)} ä¸ªç›¸å…³è¯é¢˜")
                    
                    # 2. ç”Ÿæˆæ–°é“¾æ¥
                    new_links = self._extract_links_from_topics(relevant_topics)
                    print(f"      âœ… ç”Ÿæˆ {len(new_links)} ä¸ªæ–°é“¾æ¥å€™é€‰")
                    
                    # 3. è·å–å·²å­˜åœ¨çš„é“¾æ¥å’Œé»‘åå•
                    remaining_links = content_div.find_all('a', class_='link-card')
                    existing_titles = set()
                    for link in remaining_links:
                        title_div = link.find('div', class_='link-title')
                        if title_div:
                            existing_titles.add(title_div.get_text(strip=True))
                    
                    # é»‘åå•ï¼ˆä½¿ç”¨URLä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
                    removed_urls_set = set()
                    for removed in removed_links_history:
                        if removed.get('url'):
                            removed_urls_set.add(removed['url'])
                    for removed in newly_removed_titles:
                        if removed.get('url'):
                            removed_urls_set.add(removed['url'])
                    
                    print(f"      â„¹ï¸  é»‘åå•: {len(removed_urls_set)} ä¸ªå†å²åˆ é™¤çš„é“¾æ¥URL")
                    
                    # 4. ç­›é€‰å¯ç”¨çš„æ–°é“¾æ¥
                    available_new_links = []
                    for new_link in new_links:
                        link_title = new_link.get('title', '')
                        link_url = new_link.get('url', '')
                        
                        is_blacklisted = link_url in removed_urls_set if link_url else False
                        
                        if is_blacklisted:
                            print(f"      ğŸš« è¿‡æ»¤é»‘åå•: {link_title[:50]}...")
                        elif link_title in existing_titles:
                            print(f"      â­ï¸  è·³è¿‡é‡å¤: {link_title[:50]}...")
                        else:
                            available_new_links.append(new_link)
                    
                    # 5. æ ¹æ®æ˜¯å¦æœ‰å¯ç”¨æ–°é“¾æ¥å’Œè¿­ä»£æ¬¡æ•°é€‰æ‹©ç­–ç•¥
                    if available_new_links:
                        # ç­–ç•¥A: æœ‰å¯ç”¨æ–°é“¾æ¥ â†’ åˆ é™¤æ—§é“¾æ¥ï¼Œæ·»åŠ æ–°é“¾æ¥ï¼ˆæ‰€æœ‰è¿­ä»£éƒ½å¯ç”¨ï¼‰
                        iteration_num = iteration + 1
                        print(f"\n   âœ… å‘ç° {len(available_new_links)} ä¸ªå¯ç”¨æ–°é“¾æ¥")
                        print(f"   ğŸ—‘ï¸  ç¡®è®¤åˆ é™¤æ—§é“¾æ¥...")
                        
                        # çœŸæ­£åˆ é™¤æ—§é“¾æ¥
                        for link_elem in links_to_remove_elements:
                            link_elem.decompose()
                            removed_count += 1
                        
                        print(f"   â• æ·»åŠ  {len(available_new_links)} ä¸ªæ–°é“¾æ¥...")
                        for new_link in available_new_links:
                            link_title = new_link.get('title', '')
                            link_html = self._create_link_tag(new_link)
                            link_soup = BeautifulSoup(link_html, 'html.parser')
                            content_div.append(link_soup)
                            new_links_added += 1
                            print(f"      âœ… {link_title[:50]}...")
                    elif iteration == 0:
                        # ç­–ç•¥B: æ— å¯ç”¨æ–°é“¾æ¥ + ç¬¬ä¸€æ¬¡Reflection â†’ é‡æ–°æœç´¢æ–°çƒ­ç‚¹
                        # åªåœ¨ç¬¬1æ¬¡åæ€æ—¶å…è®¸é‡æ–°æœç´¢
                        iteration_num = iteration + 1
                        print(f"\n   âš ï¸  æ²¡æœ‰å¯ç”¨çš„æ–°é“¾æ¥ï¼ˆéƒ½åœ¨é»‘åå•æˆ–å·²å­˜åœ¨ï¼‰")
                        print(f"   ğŸ”„ [ç¬¬{iteration_num}æ¬¡Reflection] é‡æ–°æœç´¢æ–°çš„çƒ­ç‚¹è¯é¢˜...")
                        
                        try:
                            # é‡æ–°è°ƒç”¨æœç´¢APIè·å–æ–°çš„çƒ­ç‚¹
                            import sys
                            # ä½¿ç”¨çœŸæ­£çš„user_profileï¼ˆå¦‚æœæä¾›ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨text_contentæ¨æ–­
                            if user_profile:
                                search_profile = user_profile
                                print(f"      ğŸ“‹ ä½¿ç”¨ç”¨æˆ·ç”»åƒé‡æ–°æœç´¢çƒ­ç‚¹è¯é¢˜...")
                            else:
                                search_profile = f"ç”¨æˆ·å…´è¶£ï¼š{text_content[:200]}..."
                                print(f"      âš ï¸  æœªæä¾›user_profileï¼Œä½¿ç”¨æ–‡æœ¬å†…å®¹æ¨æ–­...")
                            
                            new_hot_topics = self.search_hot_topics(search_profile)
                            
                            if new_hot_topics and new_hot_topics != "NO_VERIFIED_TRENDS_FOUND":
                                print(f"      âœ… æœç´¢åˆ° {len(new_hot_topics)} ä¸ªæ–°çƒ­ç‚¹")
                                
                                # ä»æ–°çƒ­ç‚¹ä¸­ç­›é€‰ç›¸å…³çš„
                                new_relevant_topics = self._filter_relevant_links(text_content, new_hot_topics)
                                print(f"      âœ… ç­›é€‰å‡º {len(new_relevant_topics)} ä¸ªç›¸å…³æ–°è¯é¢˜")
                                
                                # ä»æ–°çƒ­ç‚¹ç”Ÿæˆé“¾æ¥
                                additional_links = self._extract_links_from_topics(new_relevant_topics)
                                print(f"      âœ… ä»æ–°çƒ­ç‚¹ç”Ÿæˆ {len(additional_links)} ä¸ªæ–°é“¾æ¥")
                                
                                # è¿‡æ»¤é»‘åå•å’Œé‡å¤
                                for add_link in additional_links:
                                    link_title = add_link.get('title', '')
                                    link_url = add_link.get('url', '')
                                    
                                    is_blacklisted = link_url in removed_urls_set if link_url else False
                                    
                                    if is_blacklisted:
                                        print(f"         ğŸš« è¿‡æ»¤é»‘åå•: {link_title[:50]}...")
                                    elif link_title in existing_titles:
                                        print(f"         â­ï¸  è·³è¿‡é‡å¤: {link_title[:50]}...")
                                    else:
                                        # çœŸæ­£åˆ é™¤æ—§é“¾æ¥ï¼ˆå¦‚æœè¿˜æ²¡åˆ é™¤ï¼‰
                                        if removed_count == 0:
                                            print(f"      ğŸ—‘ï¸  ç¡®è®¤åˆ é™¤æ—§é“¾æ¥...")
                                            for link_elem in links_to_remove_elements:
                                                link_elem.decompose()
                                                removed_count += 1
                                        
                                        # æ·»åŠ æ–°é“¾æ¥
                                        link_html = self._create_link_tag(add_link)
                                        link_soup = BeautifulSoup(link_html, 'html.parser')
                                        content_div.append(link_soup)
                                        new_links_added += 1
                                        existing_titles.add(link_title)
                                        print(f"         â• æ·»åŠ : {link_title[:50]}...")
                                
                                if new_links_added > 0:
                                    print(f"      âœ… ä»æ–°çƒ­ç‚¹æ·»åŠ  {new_links_added} ä¸ªé“¾æ¥")
                                else:
                                    print(f"      âš ï¸  æ–°çƒ­ç‚¹çš„é“¾æ¥ä¹Ÿéƒ½ä¸å¯ç”¨ï¼Œè½¬è€Œä¼˜åŒ–æ–‡æœ¬...")
                                    # è½¬åˆ°ç­–ç•¥C
                                    removed_count = 0
                                    newly_removed_titles = []
                            else:
                                print(f"      âš ï¸  æœç´¢æ–°çƒ­ç‚¹å¤±è´¥ï¼Œè½¬è€Œä¼˜åŒ–æ–‡æœ¬...")
                                # è½¬åˆ°ç­–ç•¥Cï¼ˆä¼šåœ¨ä¸‹é¢çš„elseä¸­å¤„ç†ï¼‰
                        
                        except Exception as e:
                            print(f"      âš ï¸  æœç´¢æ–°çƒ­ç‚¹å¼‚å¸¸: {e}")
                            import traceback
                            traceback.print_exc()
                    
                    # ç­–ç•¥C: æœ€åæ‰‹æ®µ â†’ ä¿ç•™åŸé“¾æ¥ï¼Œä¼˜åŒ–æ–‡æœ¬
                    # ç¬¬2æ¬¡åŠä»¥åï¼ˆiteration >= 1ï¼‰ï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨æ–°é“¾æ¥ï¼Œç›´æ¥ä½¿ç”¨ç­–ç•¥C
                    # ç¬¬3æ¬¡åŠä»¥åï¼ˆiteration >= 2ï¼‰ï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨æ–°é“¾æ¥ï¼Œä¹Ÿä½¿ç”¨ç­–ç•¥Cï¼ˆé‡å¤ç¬¬3æ¬¡ç­–ç•¥ï¼‰
                    if new_links_added == 0 and removed_count == 0:
                        iteration_num = iteration + 1
                        if iteration >= 2:
                            print(f"\n   ğŸ’¡ [ç¬¬{iteration_num}æ¬¡Reflection] ä½¿ç”¨ç¬¬3æ¬¡ç­–ç•¥: ä¿ç•™åŸé“¾æ¥ï¼Œä¼˜åŒ–æ–‡æœ¬ä½¿å…¶ä¸é“¾æ¥æ›´ç›¸å…³...")
                        else:
                            print(f"\n   ğŸ’¡ [ç¬¬{iteration_num}æ¬¡Reflection] æœ€åç­–ç•¥: ä¿ç•™åŸé“¾æ¥ï¼Œä¼˜åŒ–æ–‡æœ¬ä½¿å…¶ä¸é“¾æ¥æ›´ç›¸å…³...")
                        
                        # ä¸åˆ é™¤é“¾æ¥ï¼Œä¿æŒç°çŠ¶
                        removed_count = 0
                        newly_removed_titles = []  # æ¸…ç©ºåˆ é™¤åˆ—è¡¨
                        
                        # æå–å½“å‰æ‰€æœ‰é“¾æ¥ä¿¡æ¯ç”¨äºæ–‡æœ¬ä¼˜åŒ–
                        current_links_for_optimization = []
                        for link_card in link_cards:
                            title_div = link_card.find('div', class_='link-title')
                            if title_div:
                                link_title = title_div.get_text(strip=True)
                                current_links_for_optimization.append({
                                    "title": link_title,
                                    "url": link_card.get('href', '')
                                })
                        
                        if current_links_for_optimization:
                            print(f"      ğŸ“ ä¼˜åŒ–ç›®æ ‡: ä½¿æ–‡æœ¬ä¸ä»¥ä¸‹ {len(current_links_for_optimization)} ä¸ªé“¾æ¥æ›´ç›¸å…³")
                            for link in current_links_for_optimization[:2]:
                                print(f"         - {link['title'][:50]}...")
                            
                            # è°ƒç”¨AIä¼˜åŒ–æ–‡æœ¬
                            optimized_paragraphs = self._optimize_text_for_links_hupu(
                                soup, 
                                content_div, 
                                current_links_for_optimization[:2],
                                hot_topics
                            )
                            
                            if optimized_paragraphs > 0:
                                text_optimized = True
                                print(f"   âœ… å·²ä¼˜åŒ– {optimized_paragraphs} ä¸ªæ®µè½ï¼Œä¿ç•™ {len(current_links_for_optimization)} ä¸ªé“¾æ¥")
                            else:
                                print(f"   âš ï¸  æ–‡æœ¬ä¼˜åŒ–å¤±è´¥")
                    
                except Exception as e:
                    print(f"   âš ï¸  å¤„ç†å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print(f"   âš ï¸  ç¼ºå°‘text_contentæˆ–hot_topicsï¼Œæ— æ³•å¤„ç†")
            
            # æ£€æŸ¥æœ€ç»ˆé“¾æ¥æ•°é‡å’Œä¼˜åŒ–çŠ¶æ€
            final_links = content_div.find_all('a', class_='link-card')
            action_desc = f"ç§»é™¤{removed_count}ä¸ª"
            if new_links_added > 0:
                action_desc += f"ï¼Œæ·»åŠ {new_links_added}ä¸ª"
            if text_optimized:
                action_desc += f"ï¼Œä¼˜åŒ–æ–‡æœ¬"
            print(f"   ğŸ“Š æœ€ç»ˆé“¾æ¥æ•°: {len(final_links)} ({action_desc})")
            
            if not final_links:
                print(f"   âš ï¸  å½“å‰æ— é“¾æ¥ï¼ˆå¯èƒ½åœ¨åç»­è¿­ä»£ä¸­æ·»åŠ ï¼‰")
            
            # ä¿å­˜æ–°ç‰ˆæœ¬
            output_dir = Path(html_path).parent
            version = f"_v{iteration+1}"  # v1, v2, v3
            new_html_path = output_dir / f"discussion_post{version}.html"
            
            with open(new_html_path, 'w', encoding='utf-8') as f:
                f.write(str(soup.prettify()))
            
            print(f"   âœ… æ–°ç‰ˆæœ¬å·²ä¿å­˜: {new_html_path.name}")
            # è¿”å›è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¿®æ”¹ç±»å‹
            return {
                'html_path': str(new_html_path),
                'removed_links': newly_removed_titles,
                'new_links_added': new_links_added,
                'text_optimized': text_optimized
            }
            
        except Exception as e:
            print(f"   âŒ åº”ç”¨æ”¹è¿›å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'html_path': None,
                'removed_links': [],
                'new_links_added': 0,
                'text_optimized': False
            }
    
    def _optimize_text_for_links_hupu(self, soup, content_div, removed_links, hot_topics):
        """
        ä¼˜åŒ–æ–‡æœ¬å†…å®¹ä»¥æé«˜ä¸é“¾æ¥çš„ç›¸å…³æ€§ï¼ˆHupuç‰ˆæœ¬ï¼‰
        
        å½“æ— æ³•ç”Ÿæˆæ–°é“¾æ¥æ—¶ï¼Œé€šè¿‡ä¼˜åŒ–æ–‡æœ¬ä½¿å…¶ä¸å·²æœ‰è¯é¢˜æ›´ç›¸å…³ï¼Œä»è€Œæé«˜GroupScore
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            content_div: å†…å®¹å®¹å™¨div
            removed_links: è¢«åˆ é™¤çš„é“¾æ¥åˆ—è¡¨ï¼ˆåŒ…å«title, urlï¼‰
            hot_topics: çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
            
        Returns:
            ä¼˜åŒ–çš„æ®µè½æ•°é‡
        """
        try:
            # æå–æ‰€æœ‰æ®µè½
            paragraphs = content_div.find_all('p')
            if not paragraphs:
                print(f"      âš ï¸  æœªæ‰¾åˆ°æ®µè½")
                return 0
            
            # æ„å»ºè¯é¢˜ä¸Šä¸‹æ–‡
            topics_context = ""
            if removed_links:
                topics_context = "éœ€è¦æé«˜ç›¸å…³æ€§çš„è¯é¢˜:\n"
                for link in removed_links:
                    topics_context += f"- {link['title']}\n"
            
            # æ‰¾åˆ°ä¸è¯é¢˜ç›¸å…³çš„çƒ­ç‚¹è¯¦æƒ…
            topic_details = ""
            if hot_topics and isinstance(hot_topics, list):
                topic_details = "\nç›¸å…³çƒ­ç‚¹è¯¦æƒ…:\n"
                for topic in hot_topics[:3]:
                    if isinstance(topic, dict):
                        topic_details += f"- {topic.get('topic', '')}\n"
            
            # ä¼˜åŒ–å‰ä¸¤ä¸ªæ®µè½ï¼ˆé€šå¸¸æ˜¯å¼•è¨€å’Œä¸»è¦è§‚ç‚¹ï¼‰
            optimized_count = 0
            for i, para in enumerate(paragraphs[:2]):
                old_text = para.get_text(strip=True)
                if not old_text or len(old_text) < 20:
                    continue
                
                print(f"      ğŸ”„ ä¼˜åŒ–æ®µè½ {i+1}...")
                
                # æ„å»ºä¼˜åŒ–prompt
                optimize_prompt = f"""ä½ æ˜¯è™æ‰‘è®ºå›çš„èµ„æ·±ç”¨æˆ·ã€‚è¯·ä¼˜åŒ–ä»¥ä¸‹è®¨è®ºå¸–çš„æ®µè½ï¼Œä½¿å…¶ä¸ç›¸å…³è¯é¢˜æ›´ç´§å¯†ç»“åˆã€‚

{topics_context}

{topic_details}

**åŸå§‹æ®µè½ï¼š**
{old_text}

**ä¼˜åŒ–è¦æ±‚ï¼š**
1. ä¿æŒè™æ‰‘è®ºå›é£æ ¼ï¼ˆç›´æ¥ã€æœ‰è§‚ç‚¹ã€æ¥åœ°æ°”ï¼‰
2. åœ¨æ®µè½ä¸­è‡ªç„¶èå…¥ä¸ä¸Šè¿°è¯é¢˜ç›¸å…³çš„è®¨è®ºç‚¹
3. å¯ä»¥æåŠå…·ä½“çš„çƒå‘˜ã€çƒé˜Ÿã€æ•°æ®ç­‰ç»†èŠ‚
4. ä¿æŒæ®µè½é•¿åº¦ç›¸è¿‘ï¼ˆä¸è¦è¿‡åº¦æ‰©å†™ï¼‰
5. å¢å¼ºä¸è¯é¢˜çš„å…³è”æ€§ï¼Œä½†è¦è‡ªç„¶ï¼Œä¸è¦ç”Ÿç¡¬

**åªè¾“å‡ºä¼˜åŒ–åçš„æ®µè½æ–‡å­—ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚**"""
                
                try:
                    resp = requests.post(
                        f"{self.search_base_url}/chat/completions",
                        headers={"Authorization": f"Bearer {self.search_api_key}"},
                        json={
                            "model": self.search_model,
                            "messages": [{"role": "user", "content": optimize_prompt}],
                            "temperature": 0.7,
                            "max_tokens": 500
                        },
                        timeout=30
                    )
                    
                    if resp.status_code == 200:
                        result = resp.json()
                        new_text = result['choices'][0]['message']['content'].strip()
                        
                        if new_text and new_text != old_text:
                            # æ›¿æ¢æ®µè½å†…å®¹
                            para.clear()
                            para.string = new_text
                            optimized_count += 1
                            print(f"         âœ… å·²ä¼˜åŒ– (é•¿åº¦: {len(old_text)} â†’ {len(new_text)})")
                        else:
                            print(f"         â­ï¸  æ— å˜åŒ–")
                    else:
                        print(f"         âš ï¸  APIé”™è¯¯: {resp.status_code}")
                        
                except Exception as e:
                    print(f"         âš ï¸  ä¼˜åŒ–å¤±è´¥: {e}")
                    continue
            
            return optimized_count
            
        except Exception as e:
            print(f"      âŒ æ–‡æœ¬ä¼˜åŒ–å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return 0

