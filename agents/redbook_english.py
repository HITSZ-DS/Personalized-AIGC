import os
import json
import glob
import requests
import re
from datetime import datetime

# å°è¯•åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åŠ è½½ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv æœªå®‰è£…ï¼Œè·³è¿‡


class RedBookEnglishConverter:
    """å°†å°çº¢ä¹¦é£æ ¼çš„ä¸­æ–‡å¸–å­è½¬æ¢ä¸ºç¬¦åˆå›½å¤–ç¤¾äº¤åª’ä½“ï¼ˆTikTok/YouTubeï¼‰é£æ ¼çš„è‹±æ–‡å¸–å­"""
    
    def __init__(self, generated_dir="generated_it"):
        self.generated_dir = generated_dir
        
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ API é…ç½®
        self.chat_api_key = os.getenv("CHAT_API_KEY")
        self.chat_base_url = os.getenv("CHAT_BASE_URL")
        self.chat_model = os.getenv("CHAT_MODEL")
        
        # è”ç½‘æœç´¢æ¨¡å‹ï¼ˆç”¨äºè·å–çœŸå®é“¾æ¥ï¼‰
        self.search_api_key = os.getenv("SEARCH_API_KEY")
        self.search_base_url = os.getenv("SEARCH_BASE_URL", "https://yunwu.ai/v1")
        self.search_model = os.getenv("SEARCH_MODEL", "gpt-5-all")
    
    def get_available_posts(self):
        """è·å–æ‰€æœ‰å·²ç”Ÿæˆçš„å¸–å­ç›®å½•"""
        if not os.path.exists(self.generated_dir):
            return []
        
        posts = []
        for item in os.listdir(self.generated_dir):
            item_path = os.path.join(self.generated_dir, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥æ˜¯å¦æœ‰ final_results.json
                result_file = os.path.join(item_path, "final_results.json")
                if os.path.exists(result_file):
                    posts.append(item)
        
        return sorted(posts)
    
    def load_post_data(self, user_dir):
        """åŠ è½½å¸–å­æ•°æ®"""
        user_path = os.path.join(self.generated_dir, user_dir)
        result_file = os.path.join(user_path, "final_results.json")
        
        if not os.path.exists(result_file):
            return None
        
        with open(result_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return data
    
    def _extract_from_html(self, html_path, fallback_post_data):
        """
        ä»HTMLä¸­æå–æ”¹è¿›åçš„æ–‡æœ¬å†…å®¹
        
        Args:
            html_path: HTMLæ–‡ä»¶è·¯å¾„
            fallback_post_data: å¦‚æœHTMLè§£æå¤±è´¥ï¼Œä½¿ç”¨çš„fallbackæ•°æ®
            
        Returns:
            (text, tags, images, links) tuple
        """
        try:
            if not os.path.exists(html_path):
                print(f"   âš ï¸  HTMLæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨fallbackæ•°æ®: {html_path}")
                return (
                    fallback_post_data.get("text", ""),
                    fallback_post_data.get("tags", []),
                    fallback_post_data.get("images", []),
                    fallback_post_data.get("links", [])
                )
            
            from bs4 import BeautifulSoup
            
            with open(html_path, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
            
            # æå–æ ‡ç­¾
            tags = []
            tag_elements = soup.find_all('span', class_='tag')
            for tag_elem in tag_elements:
                tag_text = tag_elem.get_text(strip=True)
                # å»é™¤ # ç¬¦å·
                tag_text = tag_text.replace('#', '').strip()
                if tag_text:
                    tags.append(tag_text)
            
            # æå–æ–‡æœ¬å†…å®¹
            content_div = soup.find('div', class_='post-content')
            if not content_div:
                print(f"   âš ï¸  æœªæ‰¾åˆ°post-contentï¼Œä½¿ç”¨fallbackæ•°æ®")
                return (
                    fallback_post_data.get("text", ""),
                    tags or fallback_post_data.get("tags", []),
                    fallback_post_data.get("images", []),
                    fallback_post_data.get("links", [])
                )
            
            # æå–æ‰€æœ‰æ®µè½æ–‡æœ¬ï¼ˆæ’é™¤é“¾æ¥å¡ç‰‡ï¼‰
            paragraphs = []
            for elem in content_div.find_all(['p', 'h3']):
                text = elem.get_text(strip=True)
                if text:
                    paragraphs.append(text)
            
            chinese_text = '\n\n'.join(paragraphs)
            
            # ä»HTMLä¸­æå–å›¾ç‰‡è·¯å¾„ï¼ˆè¿™æ ·å¯ä»¥è·å–é‡æ–°ç”Ÿæˆçš„å›¾ç‰‡ï¼‰
            images = []
            html_dir = os.path.dirname(html_path)
            for img_div in soup.find_all('div', class_='post-image'):
                img_tag = img_div.find('img')
                if img_tag and img_tag.get('src'):
                    img_src = img_tag['src']
                    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                    if not os.path.isabs(img_src):
                        img_path = os.path.join(html_dir, img_src)
                    else:
                        img_path = img_src
                    # è§„èŒƒåŒ–è·¯å¾„
                    img_path = os.path.normpath(img_path)
                    images.append(img_path)
            
            # å¦‚æœHTMLä¸­æ²¡æœ‰å›¾ç‰‡ï¼Œå°è¯•ä»fallbackè·å–
            if not images:
                print(f"   âš ï¸  HTMLä¸­æœªæ‰¾åˆ°å›¾ç‰‡ï¼Œä½¿ç”¨fallbackæ•°æ®")
                images = fallback_post_data.get("images", [])
            
            # linksä»ä»fallbackæ•°æ®è·å–ï¼ˆHTMLä¸­åªæ˜¯å¼•ç”¨ï¼Œå®é™…æ•°æ®åœ¨JSONä¸­ï¼‰
            links = fallback_post_data.get("links", [])
            
            print(f"   âœ… ä»HTMLæå–: {len(chinese_text)} å­—ç¬¦, {len(tags)} æ ‡ç­¾, {len(images)} å›¾ç‰‡")
            
            return (chinese_text, tags, images, links)
            
        except Exception as e:
            print(f"   âš ï¸  HTMLè§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨fallbackæ•°æ®")
            return (
                fallback_post_data.get("text", ""),
                fallback_post_data.get("tags", []),
                fallback_post_data.get("images", []),
                fallback_post_data.get("links", [])
            )
    
    def convert_to_english(self, chinese_text, tags, links):
        """å°†ä¸­æ–‡æ–‡æ¡ˆã€æ ‡ç­¾å’Œé“¾æ¥æ ‡é¢˜è½¬æ¢ä¸ºç¬¦åˆå›½å¤–ç¤¾äº¤åª’ä½“é£æ ¼çš„è‹±æ–‡"""
        
        # æ„å»ºé“¾æ¥ä¿¡æ¯
        links_info = ""
        if links:
            links_info = "\n**Original Link Titles (need translation):**\n"
            for i, link in enumerate(links):
                links_info += f"{i+1}. {link.get('title', '')}\n"
        
        prompt = f"""
You are a professional social media content creator who specializes in creating engaging content for TikTok, YouTube, and Instagram. Your task is to transform Chinese social media content into natural, platform-appropriate English content.

**Original Chinese Content:**
{chinese_text}

**Original Tags:**
{', '.join(tags) if tags else 'None'}
{links_info}

**Conversion Requirements:**

1. **Language Style - Match TikTok/YouTube/Instagram Culture:**
   - Use casual, conversational English like talking to friends
   - Popular phrases: "Guys!", "Besties!", "Y'all", "No cap", "Literally", "Fr fr" (for real), "Ngl" (not gonna lie)
   - Enthusiastic expressions: "OMG!", "This is insane!", "I'm obsessed!", "You NEED this!", "Game changer!"
   - For recommendations: "Highly recommend", "10/10 would recommend", "You're missing out", "Trust me on this"
   - For warnings: "Skip this", "Save your money", "Major red flag"
   - Use "lowkey/highkey" for emphasis
   - Use emojis naturally but don't overdo it

2. **Content Adaptation (NOT Direct Translation):**
   - Keep the same message and tone but adapt cultural references
   - If mentioning Chinese platforms (å°çº¢ä¹¦/Bç«™/æŠ–éŸ³), convert to equivalent:
     * å°çº¢ä¹¦ â†’ Instagram/Pinterest
     * Bç«™ â†’ YouTube
     * æŠ–éŸ³ â†’ TikTok
     * çŸ¥ä¹ â†’ Reddit/Quora
     * å¾®åš â†’ Twitter/X
   - Adapt Chinese slang to English internet slang
   - Keep specific place names, product names, and prices as-is (or add USD conversion if relevant)

3. **Structure:**
   - Start with an engaging hook (like TikTok/YouTube intros)
   - Keep paragraphs short and punchy
   - Use line breaks for emphasis
   - End with a call-to-action or question to boost engagement

4. **Tags:**
   - Convert tags to English hashtags
   - Make them TikTok/Instagram-friendly
   - Use popular English hashtags format

5. **Link Titles:**
   - Convert link titles to natural, engaging English
   - Make them clickable and appealing
   - Keep the same topic/theme

**Output Format:**
Return ONLY a JSON object (no markdown formatting):
{{
  "english_text": "The converted English content here...",
  "english_tags": ["tag1", "tag2", "tag3", "tag4"],
  "english_link_titles": ["English title 1", "English title 2"]
}}

IMPORTANT: 
- Return PURE JSON only, no ```json or ``` wrappers
- Make it sound natural and native, NOT like a translation
- Keep the enthusiasm and energy of the original
- The number of english_link_titles should match the number of original link titles
"""
        
        try:
            resp = requests.post(
                f"{self.chat_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.chat_api_key}"},
                json={
                    "model": self.chat_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.8,  # ç¨é«˜ä¸€ç‚¹ï¼Œè®©å†…å®¹æ›´æœ‰åˆ›æ„
                    "top_p": 0.9,
                }
            )
            
            result = resp.json()["choices"][0]["message"]["content"].strip()
            
            # æ¸…æ´—å¯èƒ½çš„ markdown ä»£ç å—
            if result.startswith("```json"):
                result = result[7:]
            elif result.startswith("```"):
                result = result[3:]
            
            if result.endswith("```"):
                result = result[:-3]
            
            result = result.strip()
            
            # è§£æ JSON
            converted = json.loads(result)
            return (
                converted.get("english_text", ""), 
                converted.get("english_tags", []),
                converted.get("english_link_titles", [])
            )
        
        except Exception as e:
            print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
            return None, None, None
    
    def _generate_international_search_url(self, platform, keyword):
        """æ ¹æ®å¹³å°å’Œå…³é”®è¯ç”ŸæˆçœŸå®çš„æœç´¢é“¾æ¥ï¼ˆæ”¯æŒå›½å¤–å’Œä¸­æ–‡å¹³å°ï¼‰"""
        import urllib.parse
        kw_encoded = urllib.parse.quote(keyword)
        p = platform.lower()
        
        # å›½å¤–å¹³å°
        if "instagram" in p:
            # Instagram ä½¿ç”¨ tag æœç´¢
            return f"https://www.instagram.com/explore/tags/{kw_encoded}/"
        elif "youtube" in p:
            return f"https://www.youtube.com/results?search_query={kw_encoded}"
        elif "tiktok" in p:
            return f"https://www.tiktok.com/search?q={kw_encoded}"
        elif "reddit" in p:
            return f"https://www.reddit.com/search/?q={kw_encoded}"
        elif "twitter" in p or p == "x":
            return f"https://twitter.com/search?q={kw_encoded}"
        elif "pinterest" in p:
            return f"https://www.pinterest.com/search/pins/?q={kw_encoded}"
        
        # ä¸­æ–‡å¹³å°ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
        elif "å°çº¢ä¹¦" in p or "xiaohongshu" in p:
            return f"https://www.xiaohongshu.com/search_result?keyword={kw_encoded}"
        elif "bç«™" in p or "bilibili" in p:
            return f"https://search.bilibili.com/all?keyword={kw_encoded}"
        elif "çŸ¥ä¹" in p or "zhihu" in p:
            return f"https://www.zhihu.com/search?type=content&q={kw_encoded}"
        elif "æŠ–éŸ³" in p or "douyin" in p:
            return f"https://www.douyin.com/search/{kw_encoded}"
        elif "å¾®åš" in p or "weibo" in p:
            return f"https://s.weibo.com/weibo?q={kw_encoded}"
        
        else:
            # é»˜è®¤ä½¿ç”¨ Google æœç´¢
            return f"https://www.google.com/search?q={kw_encoded}"
    
    def _is_homepage_url(self, url):
        """æ£€æµ‹æ˜¯å¦ä¸ºé¦–é¡µé“¾æ¥ï¼ˆéœ€è¦è¿‡æ»¤æ‰ï¼‰"""
        if not url or url == "#":
            return True
        
        # æå–è·¯å¾„éƒ¨åˆ†
        import urllib.parse
        parsed = urllib.parse.urlparse(url)
        path = parsed.path.strip('/')
        query = parsed.query
        
        # å¦‚æœæ²¡æœ‰è·¯å¾„æˆ–æŸ¥è¯¢å‚æ•°ï¼Œå¯èƒ½æ˜¯é¦–é¡µ
        if not path and not query:
            return True
        
        # å¦‚æœåªæœ‰æ ¹è·¯å¾„ä¸”æ²¡æœ‰æœç´¢å‚æ•°
        homepage_patterns = [
            r'^/?$',
            r'^/?index\.(html?|php)$',
            r'^/?home$',
        ]
        
        for pattern in homepage_patterns:
            if re.match(pattern, path):
                return True
        
        return False
    
    def _search_real_international_links(self, links_info, text_content):
        """
        ä½¿ç”¨è”ç½‘æœç´¢è·å–çœŸå®çš„å›½å¤–å¹³å°å¸–å­é“¾æ¥
        å‚è€ƒ commentproduct_generator.py çš„å®ç°
        """
        if not self.search_api_key:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            env_value = os.getenv("SEARCH_API_KEY")
            if env_value is None:
                print("âš ï¸ SEARCH_API_KEY not found in environment variables")
                print("   ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­æ˜¯å¦è®¾ç½®äº† SEARCH_API_KEY")
            elif env_value == "":
                print("âš ï¸ SEARCH_API_KEY is empty string")
                print("   ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­ SEARCH_API_KEY çš„å€¼æ˜¯å¦æ­£ç¡®")
            else:
                print(f"âš ï¸ SEARCH_API_KEY exists but is falsy (value length: {len(env_value)})")
            print("   âš ï¸ Fallback to search URLs")
            return None
        
        # æ„å»ºæœç´¢æç¤º
        links_desc = []
        for i, info in enumerate(links_info):
            title = info.get('english_title', '')
            platform = info.get('target_platform', '')
            keyword = info.get('search_keyword', '')
            links_desc.append(f"{i+1}. Platform: {platform}, Keyword: {keyword}, Expected: {title}")
        
        search_prompt = f"""Based on the English post content and recommended topics, search and return 2 **real post/video links** from ANY platform.

Post content (first 600 chars):
{text_content[:600]}

Recommended directions:
{chr(10).join(links_desc)}

Return JSON format:
[
  {{
    "title": "Actual post/video title (in English)",
    "platform": "Platform name (can be ANY: Instagram/YouTube/TikTok/Reddit/Twitter/å°çº¢ä¹¦/Bç«™/çŸ¥ä¹/å¾®åš/æŠ–éŸ³/etc.)",
    "url": "Real post/video URL (must be specific post, not homepage or search page)",
    "search_keyword": "Precise search keyword (only if really can't find any specific post link)"
  }}
]

**CRITICAL Requirements:**
1. **URL MUST be a SPECIFIC post/video/discussion link, NOT homepage or search page**
2. **Priority order:**
   - First try: Instagram, YouTube, TikTok, Reddit, Twitter (international platforms)
   - Second try: å°çº¢ä¹¦, Bç«™, çŸ¥ä¹, å¾®åš, æŠ–éŸ³ (Chinese platforms are OK if they have specific posts)
   - Last resort: Other platforms are also acceptable
3. **Title should be in English** (translate if the actual post is in Chinese)
4. **Only use search_keyword if you absolutely cannot find ANY specific post link**
5. Prefer ANY specific post URL over search pages"""

        try:
            print(f"ğŸ” Searching for real international links using {self.search_model}...")
            response = requests.post(
                f"{self.search_base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.search_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.search_model,
                    "web_search_options": {},  # Enable web search
                    "messages": [{"role": "user", "content": search_prompt}],
                    "temperature": 0.7
                },
                timeout=90
            )
            
            if response.status_code != 200:
                print(f"âš ï¸ Search API error {response.status_code}")
                return None
            
            resp_json = response.json()
            
            if "choices" not in resp_json:
                print(f"âš ï¸ Unexpected response format")
                return None
            
            content = resp_json["choices"][0]["message"]["content"]
            
            # Clean content
            content = re.sub(r'^>.*?$', '', content, flags=re.MULTILINE)
            content = re.sub(r'\*\*\[.*?\]\(.*?\)\*\*\s*Â·\s*\*.*?\*', '', content)
            content = re.sub(r'\[.*?\]\(.*?\)', '', content)
            content = re.sub(r'^```json\s*', '', content, flags=re.MULTILINE)
            content = re.sub(r'^```\s*', '', content, flags=re.MULTILINE)
            content = re.sub(r'\n{2,}', '\n', content).strip()
            
            # Extract JSON array
            json_content = None
            first_bracket = content.find('[')
            if first_bracket != -1:
                bracket_count = 0
                last_bracket = -1
                in_string = False
                escape_next = False
                
                for i in range(first_bracket, len(content)):
                    char = content[i]
                    if escape_next:
                        escape_next = False
                        continue
                    if char == '\\':
                        escape_next = True
                        continue
                    if char == '"':
                        in_string = not in_string
                        continue
                    if not in_string:
                        if char == '[':
                            bracket_count += 1
                        elif char == ']':
                            bracket_count -= 1
                            if bracket_count == 0:
                                last_bracket = i
                                break
                
                if last_bracket != -1:
                    json_content = content[first_bracket:last_bracket + 1].strip()
            
            if not json_content:
                print(f"âš ï¸ Could not extract valid JSON array")
                return None
            
            # Parse JSON
            try:
                search_results = json.loads(json_content)
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON parse error: {e}")
                return None
            
            # Validate and extract valid links (accept ANY platform)
            valid_links = []
            if isinstance(search_results, list):
                for result in search_results:
                    if not isinstance(result, dict):
                        continue
                    
                    # Prioritize real URLs (from ANY platform)
                    if result.get("url") and result.get("title"):
                        url = result.get("url", "")
                        # Validate URL format
                        if not (url.startswith("http://") or url.startswith("https://")):
                            continue
                        
                        # Filter homepage links (critical: no search pages!)
                        if self._is_homepage_url(url):
                            print(f"âš ï¸ Filtered homepage link: {url[:60]}...")
                            continue
                        
                        # Check if it's a search page (additional check)
                        if any(indicator in url.lower() for indicator in ['/search?', '/search/', 'search_query=', 'search_result']):
                            print(f"âš ï¸ Filtered search page: {url[:60]}...")
                            continue
                        
                        platform = result.get("platform", "Web")
                        valid_links.append({
                            "title": result.get("title", ""),
                            "platform": platform,
                            "url": url
                        })
                        # åŒºåˆ†å›½å¤–å’Œä¸­æ–‡å¹³å°
                        platform_type = "ğŸŒ International" if any(p in platform.lower() for p in ['instagram', 'youtube', 'tiktok', 'reddit', 'twitter']) else "ğŸ‡¨ğŸ‡³ Chinese"
                        print(f"âœ… Found real link [{platform_type}]: {result.get('title', '')[:40]}... -> {url[:60]}...")
                    
                    # Fallback: use search keyword (only if really necessary)
                    elif result.get("search_keyword") and result.get("platform"):
                        keyword = result.get("search_keyword", "")
                        platform = result.get("platform", "")
                        # Try to generate appropriate search URL based on platform
                        search_url = self._generate_international_search_url(platform, keyword)
                        
                        valid_links.append({
                            "title": result.get("title", f"{platform} search: {keyword}"),
                            "platform": platform,
                            "url": search_url
                        })
                        print(f"âš ï¸ Using search link (last resort): {result.get('title', '')[:40]}... | Keyword: {keyword}")
            
            if valid_links:
                print(f"âœ… Successfully retrieved {len(valid_links)} links")
                return valid_links
            else:
                print(f"âš ï¸ No valid links found")
                return None
        
        except Exception as e:
            print(f"âš ï¸ Search exception: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def adapt_links_to_english(self, links, english_link_titles):
        """å°†å›½å†…å¹³å°é“¾æ¥è½¬æ¢ä¸ºå›½å¤–å¹³å°å»ºè®®ï¼Œä½¿ç”¨è‹±æ–‡æ ‡é¢˜ï¼ˆä¼˜å…ˆè·å–çœŸå®é“¾æ¥ï¼‰"""
        if not links:
            return []
        
        # å¹³å°æ˜ å°„
        platform_map = {
            "å°çº¢ä¹¦": "Instagram",
            "bç«™": "YouTube",
            "bilibili": "YouTube",
            "çŸ¥ä¹": "Reddit",
            "æŠ–éŸ³": "TikTok",
            "å¾®åš": "Twitter"
        }
        
        # å‡†å¤‡é“¾æ¥ä¿¡æ¯ç”¨äºè”ç½‘æœç´¢
        links_info = []
        for i, link in enumerate(links):
            original_platform = link.get("platform", "")
            original_url = link.get("url", "")
            
            # ä½¿ç”¨è½¬æ¢åçš„è‹±æ–‡æ ‡é¢˜
            english_title = link.get("title", "")
            if english_link_titles and i < len(english_link_titles):
                english_title = english_link_titles[i]
            
            # ä»åŸå§‹ URL ä¸­æå–å…³é”®è¯
            search_keyword = ""
            if "keyword=" in original_url:
                try:
                    import urllib.parse
                    parsed = urllib.parse.urlparse(original_url)
                    params = urllib.parse.parse_qs(parsed.query)
                    if 'keyword' in params:
                        search_keyword = params['keyword'][0]
                    elif 'q' in params:
                        search_keyword = params['q'][0]
                except:
                    pass
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œä½¿ç”¨è‹±æ–‡æ ‡é¢˜ä½œä¸ºå…³é”®è¯
            if not search_keyword:
                search_keyword = english_title
            
            # è½¬æ¢å¹³å°å
            target_platform = original_platform
            for cn, en in platform_map.items():
                if cn in original_platform.lower():
                    target_platform = en
                    break
            
            links_info.append({
                "english_title": english_title,
                "target_platform": target_platform,
                "search_keyword": search_keyword
            })
        
        # å°è¯•è·å–çœŸå®é“¾æ¥
        print("ğŸ” Attempting to retrieve real international post links...")
        real_links = self._search_real_international_links(links_info, "")
        
        if real_links:
            return real_links
        
        # é™çº§ï¼šç”Ÿæˆæœç´¢é“¾æ¥
        print("âš ï¸ Web search failed, using search URLs as fallback")
        adapted_links = []
        for info in links_info:
            search_url = self._generate_international_search_url(
                info['target_platform'], 
                info['search_keyword']
            )
            
            adapted_links.append({
                "title": info['english_title'],
                "platform": info['target_platform'],
                "url": search_url,
                "search_keyword": info['search_keyword']
            })
        
        return adapted_links
    
    def generate_english_html(self, english_text, english_tags, image_paths, adapted_links, output_path):
        """ç”Ÿæˆè‹±æ–‡ç‰ˆ HTML"""
        
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        raw_paragraphs = [p for p in english_text.split('\n\n') if p.strip()]
        
        # æ¸²æŸ“ markdown
        processed_paragraphs = []
        for p in raw_paragraphs:
            # å¤„ç†åŠ ç²—
            rendered = re.sub(r'\*\*(.*?)\*\*', r'<strong style="color: #333; font-weight: 700;">\1</strong>', p)
            processed_paragraphs.append(rendered)
        
        paragraphs = processed_paragraphs
        
        # ç»„è£…å†…å®¹
        html_parts = []
        insertions = []
        
        # æ’å…¥å›¾ç‰‡
        for i, img_path in enumerate(image_paths):
            insertions.append({"type": "image", "content": os.path.basename(img_path), "index": i})
        
        # æ’å…¥é“¾æ¥
        for link in adapted_links:
            insertions.append({"type": "link", "content": link})
        
        num_paras = len(paragraphs)
        
        if num_paras == 0:
            html_parts.append(english_text)
        else:
            num_inserts = len(insertions)
            if num_inserts > 0:
                step = max(1, num_paras // (num_inserts + 1))
                current_insert_idx = 0
                
                for i, para in enumerate(paragraphs):
                    if para.startswith('<h3'):
                        html_parts.append(para)
                    else:
                        html_parts.append(f"<p>{para}</p>")
                    
                    if current_insert_idx < num_inserts:
                        if (i + 1) % step == 0 or i == num_paras - 1:
                            item = insertions[current_insert_idx]
                            if item["type"] == "image":
                                html_parts.append(self._create_image_tag(item["content"], item["index"]))
                            elif item["type"] == "link":
                                html_parts.append(self._create_link_tag(item["content"]))
                            current_insert_idx += 1
                            
                            if i == num_paras - 1:
                                while current_insert_idx < num_inserts:
                                    item = insertions[current_insert_idx]
                                    if item["type"] == "image":
                                        html_parts.append(self._create_image_tag(item["content"], item["index"]))
                                    elif item["type"] == "link":
                                        html_parts.append(self._create_link_tag(item["content"]))
                                    current_insert_idx += 1
            else:
                for para in paragraphs:
                    if para.startswith('<h3'):
                        html_parts.append(para)
                    else:
                        html_parts.append(f"<p>{para}</p>")
        
        html_content = "\n".join(html_parts)
        
        # ç”Ÿæˆæ ‡ç­¾ HTML
        tags_html = "".join([f'<span class="tag"># {tag}</span>' for tag in english_tags])
        
        # HTML æ¨¡æ¿
        html_template = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Social Media Post - English Version</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; line-height: 1.75; max-width: 800px; margin: 0 auto; padding: 15px; background: #f5f5f5; color: #333; }}
        .post-container {{ background: white; border-radius: 12px; padding: 25px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); margin: 10px 0; }}
        
        .post-content {{ font-size: 17px; color: #2c3e50; letter-spacing: 0.02em; }}
        .post-content p {{ margin: 1em 0; text-align: left; }}
        .post-content strong {{ color: #000; font-weight: 700; background: linear-gradient(to bottom, transparent 60%, #fffbe6 60%); }}
        .post-content h3 {{ font-size: 1.2em; margin-top: 1.5em; margin-bottom: 0.5em; color: #1a1a1a; }}
        
        .post-image {{ margin: 20px -25px; width: calc(100% + 50px); text-align: center; }}
        .post-image img {{ width: 100%; display: block; }}
        .image-caption {{ color: #999; font-size: 13px; margin-top: 8px; font-style: italic; padding: 0 25px; }}
        
        .post-tags {{ 
            margin: 15px 0 20px 0; 
            display: flex; 
            flex-wrap: wrap; 
            gap: 8px;
        }}
        .tag {{ 
            display: inline-block; 
            padding: 5px 12px; 
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white; 
            font-size: 13px; 
            border-radius: 15px; 
            font-weight: 500;
            box-shadow: 0 2px 4px rgba(102, 126, 234, 0.2);
        }}
        
        .link-card {{
            display: flex;
            align-items: center;
            background: #fcfcfc;
            border: 1px solid #eee;
            padding: 12px 15px;
            margin: 25px 0;
            text-decoration: none;
            border-radius: 8px;
            transition: all 0.2s;
        }}
        .link-card:hover {{ transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0,0,0,0.08); }}
        .link-card.platform-instagram {{ border-left: 5px solid #E4405F; }}
        .link-card.platform-youtube {{ border-left: 5px solid #FF0000; }}
        .link-card.platform-tiktok {{ border-left: 5px solid #000000; }}
        .link-card.platform-reddit {{ border-left: 5px solid #FF4500; }}
        .link-card.platform-twitter {{ border-left: 5px solid #1DA1F2; }}
        .link-card.platform-xiaohongshu {{ border-left: 5px solid #ff2442; }}
        .link-card.platform-bilibili {{ border-left: 5px solid #23ade5; }}
        .link-card.platform-zhihu {{ border-left: 5px solid #0084ff; }}
        .link-card.platform-douyin {{ border-left: 5px solid #1c1e21; }}
        .link-card.platform-weibo {{ border-left: 5px solid #ea5d5c; }}
        
        .link-info {{ flex: 1; }}
        .link-platform-tag {{ 
            font-size: 12px; font-weight: bold; margin-bottom: 4px; display: inline-block; padding: 2px 6px; border-radius: 4px; color: white;
        }}
        .tag-instagram {{ background: #E4405F; }}
        .tag-youtube {{ background: #FF0000; }}
        .tag-tiktok {{ background: #000; }}
        .tag-reddit {{ background: #FF4500; }}
        .tag-twitter {{ background: #1DA1F2; }}
        .tag-xiaohongshu {{ background: #ff2442; }}
        .tag-bilibili {{ background: #23ade5; }}
        .tag-zhihu {{ background: #0084ff; }}
        .tag-douyin {{ background: #000; }}
        .tag-weibo {{ background: #ea5d5c; }}
        
        .link-title {{ font-weight: bold; color: #333; font-size: 15px; margin-top: 2px; }}
        .link-action {{ color: #999; font-size: 12px; margin-top: 4px; }}
        .link-icon {{ font-size: 24px; margin-right: 15px; }}
        
        .post-header {{ display: flex; align-items: center; margin-bottom: 20px; padding-bottom: 15px; border-bottom: 1px solid #f0f0f0; }} 
        .avatar {{ width: 48px; height: 48px; border-radius: 50%; background: linear-gradient(120deg, #a1c4fd 0%, #c2e9fb 100%); margin-right: 15px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }} 
        .user-info h3 {{ margin: 0; font-size: 18px; font-weight: 600; }} 
        .post-time {{ color: #999; font-size: 13px; margin-top: 4px; }} 
        
        @media (max-width: 600px) {{ 
            .post-container {{ padding: 15px; }} 
            .post-image {{ margin: 15px -15px; width: calc(100% + 30px); }}
        }}
    </style>
</head>
<body>
    <div class="post-container">
        <div class="post-header">
            <div class="avatar"></div>
            <div class="user-info">
                <h3>AI Content Creator</h3>
                <div class="post-time">{datetime.now().strftime('%B %d, %Y at %I:%M %p')}</div>
            </div>
        </div>
        <div class="post-tags">
            {tags_html}
        </div>
        <div class="post-content">{html_content}</div>
        <div style="margin-top:30px; border-top:1px solid #eee; padding-top:15px; color:#ccc; font-size:12px; text-align:center;">
            Generated by AI â€¢ English Version â€¢ {len(image_paths)} Images
        </div>
    </div>
</body>
</html>
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_template)
        
        return output_path
    
    def _create_image_tag(self, image_filename, index):
        return f'<div class="post-image"><img src="{image_filename}"><div class="image-caption">Image {index + 1}</div></div>'
    
    def _create_link_tag(self, link_data):
        title = link_data.get('title', 'Related Content')
        platform = link_data.get('platform', 'Web').strip()
        url = link_data.get('url', '#')
        
        css_class = "platform-other"
        tag_class = "tag-other"
        icon = "ğŸ”—"
        
        p = platform.lower()
        # å›½å¤–å¹³å°
        if "instagram" in p:
            css_class = "platform-instagram"
            tag_class = "tag-instagram"
            icon = "ğŸ“·"
        elif "youtube" in p:
            css_class = "platform-youtube"
            tag_class = "tag-youtube"
            icon = "ğŸ“º"
        elif "tiktok" in p:
            css_class = "platform-tiktok"
            tag_class = "tag-tiktok"
            icon = "ğŸµ"
        elif "reddit" in p:
            css_class = "platform-reddit"
            tag_class = "tag-reddit"
            icon = "ğŸ’¬"
        elif "twitter" in p or "x" == p:
            css_class = "platform-twitter"
            tag_class = "tag-twitter"
            icon = "ğŸ¦"
        # ä¸­æ–‡å¹³å°ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
        elif "å°çº¢ä¹¦" in p or "xiaohongshu" in p:
            css_class = "platform-xiaohongshu"
            tag_class = "tag-xiaohongshu"
            icon = "ğŸ“•"
        elif "bç«™" in p or "bilibili" in p:
            css_class = "platform-bilibili"
            tag_class = "tag-bilibili"
            icon = "ğŸ“º"
        elif "çŸ¥ä¹" in p or "zhihu" in p:
            css_class = "platform-zhihu"
            tag_class = "tag-zhihu"
            icon = "â“"
        elif "æŠ–éŸ³" in p or "douyin" in p:
            css_class = "platform-douyin"
            tag_class = "tag-douyin"
            icon = "ğŸµ"
        elif "å¾®åš" in p or "weibo" in p:
            css_class = "platform-weibo"
            tag_class = "tag-weibo"
            icon = "ğŸ‘ï¸"
        
        return f'''
<a href="{url}" class="link-card {css_class}" target="_blank">
    <div class="link-icon">{icon}</div>
    <div class="link-info">
        <span class="link-platform-tag {tag_class}">{platform}</span>
        <div class="link-title">{title}</div>
        <div class="link-action">Check it out on {platform} &gt;</div>
    </div>
</a>
'''
    
    def convert_post(self, user_dir):
        """è½¬æ¢å•ä¸ªå¸–å­"""
        print(f"\n{'='*50}")
        print(f"Converting: {user_dir}")
        print(f"{'='*50}")
        
        # åŠ è½½åŸå§‹æ•°æ®
        data = self.load_post_data(user_dir)
        if not data:
            print(f"âŒ æ— æ³•åŠ è½½å¸–å­æ•°æ®")
            return None
        
        post_data = data.get("personalized_post", {})
        
        # ===== ä½¿ç”¨æœ€ç»ˆç‰ˆæœ¬çš„HTMLï¼ˆå·²ç»æ˜¯æœ€ä¼˜ç‰ˆæœ¬ï¼‰ =====
        # html_post å­—æ®µå·²ç»æ˜¯ current_html_pathï¼Œå³æœ€ç»ˆç‰ˆæœ¬
        best_html_path = post_data.get("html_post", "")
        final_version = post_data.get("final_version", "v0")
        reflection_history = post_data.get("reflection_history", [])
        
        if reflection_history:
            print(f"ğŸ“Š Reflectionå†å²:")
            for record in reflection_history:
                is_final = (record.get('version') == final_version)
                marker = "â­ (æœ€ç»ˆç‰ˆæœ¬)" if is_final else "  "
                strategy = ""
                if 'strategy' in record:
                    if record['strategy'] == 'image_regeneration':
                        strategy = " [å›¾ç‰‡é‡å»º]"
                    elif record['strategy'] == 'image_generation_rescue':
                        strategy = " [å›¾ç‰‡è¡¥æ•‘]"
                if record.get('switched_to_best'):
                    strategy += " [åŸºäºæœ€ä½³ç‰ˆæœ¬]"
                print(f"   {marker} {record['version']}: GroupScore = {record['groupscore']:.4f}{strategy}")
            print(f"âœ… ä½¿ç”¨æœ€ç»ˆç‰ˆæœ¬: {final_version} (html_postå·²æŒ‡å‘æœ€ç»ˆç‰ˆæœ¬)")
        else:
            print(f"â„¹ï¸  æ— Reflectionå†å²ï¼Œä½¿ç”¨åˆå§‹ç‰ˆæœ¬: {final_version}")
        
        # ä»é€‰ä¸­çš„HTMLä¸­æå–å†…å®¹ï¼ˆè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨åŸå§‹textï¼‰
        chinese_text, chinese_tags, images, links = self._extract_from_html(best_html_path, post_data)
        
        if not chinese_text:
            print(f"âŒ æ²¡æœ‰æ–‡æœ¬å†…å®¹")
            return None
        
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦: {len(chinese_text)} å­—ç¬¦")
        print(f"ğŸ·ï¸  åŸå§‹æ ‡ç­¾: {chinese_tags}")
        if links:
            print(f"ğŸ”— åŸå§‹é“¾æ¥: {[l.get('title', '') for l in links]}")
        
        # è½¬æ¢ä¸ºè‹±æ–‡
        print("ğŸ”„ Converting to English...")
        english_text, english_tags, english_link_titles = self.convert_to_english(chinese_text, chinese_tags, links)
        
        if not english_text:
            print(f"âŒ è½¬æ¢å¤±è´¥")
            return None
        
        print(f"âœ… è‹±æ–‡æ–‡æœ¬é•¿åº¦: {len(english_text)} å­—ç¬¦")
        print(f"ğŸ·ï¸  è‹±æ–‡æ ‡ç­¾: {english_tags}")
        if english_link_titles:
            print(f"ğŸ”— è‹±æ–‡é“¾æ¥æ ‡é¢˜: {english_link_titles}")
        
        # é€‚é…é“¾æ¥ï¼ˆä½¿ç”¨è‹±æ–‡æ ‡é¢˜ï¼‰
        adapted_links = self.adapt_links_to_english(links, english_link_titles)
        
        # ç”Ÿæˆè‹±æ–‡ç‰ˆ HTML
        user_path = os.path.join(self.generated_dir, user_dir)
        output_html = os.path.join(user_path, "image_text_english.html")
        
        print("ğŸ“„ Generating English HTML...")
        self.generate_english_html(english_text, english_tags, images, adapted_links, output_html)
        
        # ä¿å­˜è‹±æ–‡æ•°æ®
        english_data = {
            "text": english_text,
            "tags": english_tags,
            "images": images,
            "links": adapted_links,
            "html_path": output_html
        }
        
        english_json = os.path.join(user_path, "english_post.json")
        with open(english_json, 'w', encoding='utf-8') as f:
            json.dump(english_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å®Œæˆï¼æŸ¥çœ‹: {output_html}")
        
        return english_data
    
    def convert_all(self):
        """è½¬æ¢æ‰€æœ‰å¸–å­"""
        posts = self.get_available_posts()
        
        if not posts:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å·²ç”Ÿæˆçš„å¸–å­")
            return
        
        print(f"\næ‰¾åˆ° {len(posts)} ä¸ªå¸–å­")
        print(f"{'='*50}")
        
        success_count = 0
        for post_dir in posts:
            try:
                result = self.convert_post(post_dir)
                if result:
                    success_count += 1
            except Exception as e:
                print(f"âŒ è½¬æ¢ {post_dir} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"\n{'='*50}")
        print(f"ğŸ‰ å®Œæˆï¼æˆåŠŸè½¬æ¢ {success_count}/{len(posts)} ä¸ªå¸–å­")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å°†å°çº¢ä¹¦å¸–å­è½¬æ¢ä¸ºè‹±æ–‡ç‰ˆï¼ˆTikTok/YouTubeé£æ ¼ï¼‰')
    parser.add_argument('--user', type=str, help='æŒ‡å®šè¦è½¬æ¢çš„ç”¨æˆ·ç›®å½•å')
    parser.add_argument('--all', action='store_true', help='è½¬æ¢æ‰€æœ‰å¸–å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not os.getenv("CHAT_API_KEY"):
        os.environ.update({
            "CHAT_API_KEY": "sk-dVaSXmTEMBh0Gygx49ResSvaONvErml5QV8McBAGkbPmX2mG",
            "CHAT_BASE_URL": "https://yunwu.ai/v1",
            "CHAT_MODEL": "gpt-4o",
        })
    
    converter = RedBookEnglishConverter()
    
    if args.all:
        converter.convert_all()
    elif args.user:
        converter.convert_post(args.user)
    else:
        # äº¤äº’å¼é€‰æ‹©
        posts = converter.get_available_posts()
        if not posts:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å·²ç”Ÿæˆçš„å¸–å­")
            return
        
        print(f"\n{'='*50}")
        print("å¯ç”¨çš„å¸–å­:")
        print(f"{'='*50}")
        for idx, post in enumerate(posts):
            print(f"[{idx}] {post}")
        print(f"{'='*50}")
        
        user_input = input("\nè¯·è¾“å…¥è¦è½¬æ¢çš„å¸–å­åºå·ï¼ˆå¤šä¸ªç”¨ç©ºæ ¼åˆ†éš”ï¼Œæˆ–è¾“å…¥ 'all' è½¬æ¢æ‰€æœ‰ï¼‰: ").strip()
        
        if user_input.lower() == 'all':
            converter.convert_all()
        else:
            # è§£æè¾“å…¥çš„åºå·åˆ—è¡¨
            selected_posts = []
            for item in user_input.split():
                item = item.strip()
                if item.isdigit():
                    idx = int(item)
                    if 0 <= idx < len(posts):
                        if posts[idx] not in selected_posts:
                            selected_posts.append(posts[idx])
                    else:
                        print(f"âš ï¸ è­¦å‘Š: åºå· [{idx}] è¶…å‡ºèŒƒå›´ (0-{len(posts)-1})")
                else:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ•ˆçš„è¾“å…¥ '{item}'")
            
            if not selected_posts:
                print("âŒ æœªé€‰æ‹©æœ‰æ•ˆçš„å¸–å­")
                return
            
            print(f"\nâœ… å³å°†è½¬æ¢ {len(selected_posts)} ä¸ªå¸–å­:")
            for post in selected_posts:
                print(f"  {post}")
            print(f"{'='*50}\n")
            
            # æ‰¹é‡è½¬æ¢
            success_count = 0
            for post_dir in selected_posts:
                try:
                    result = converter.convert_post(post_dir)
                    if result:
                        success_count += 1
                except Exception as e:
                    print(f"âŒ è½¬æ¢ {post_dir} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
            
            print(f"\n{'='*50}")
            print(f"ğŸ‰ å®Œæˆï¼æˆåŠŸè½¬æ¢ {success_count}/{len(selected_posts)} ä¸ªå¸–å­")


if __name__ == "__main__":
    main()

