import base64
import os
import json
import requests
import urllib.parse
import re  # <--- æ–°å¢æ­£åˆ™åº“ï¼Œç”¨äºå¤„ç† Markdown
from datetime import datetime
from pathlib import Path

# Handle imports that work both from main.py (absolute) and from agents/ directory (relative)
try:
    # Try absolute import first (when running from main.py)
    from agents.rag_embedding_helper import RAGEmbeddingHelper
    from agents.html_parser_for_reflection import HTMLParserForReflection
    from agents.reflection_advisor import ReflectionAdvisor
    from agents.evaluate_groupscore import evaluate_file, CLIPEvaluator
except ImportError:
    # Fallback to relative import (when running from agents/ directory)
    from rag_embedding_helper import RAGEmbeddingHelper
    from html_parser_for_reflection import HTMLParserForReflection
    from reflection_advisor import ReflectionAdvisor
    from evaluate_groupscore import evaluate_file, CLIPEvaluator

# å°è¯•åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åŠ è½½ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv æœªå®‰è£…ï¼Œè·³è¿‡


class AIRefusalError(Exception):
    """AIè¿ç»­æ‹’ç»ç”Ÿæˆå†…å®¹çš„å¼‚å¸¸"""
    pass


class ITProductGenerator:
    def __init__(self, ideas, examples_dir="agents/redbook", enable_links=False):
        # æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
        self.chat_api_key = os.getenv("CHAT_API_KEY")
        self.chat_base_url = os.getenv("CHAT_BASE_URL")
        self.chat_model = os.getenv("CHAT_MODEL")

        # å›¾åƒç”Ÿæˆæ¨¡å‹
        self.generate_api_key = os.getenv("IMAGE_API_KEY")
        self.generate_base_url = os.getenv("IMAGE_BASE_URL")
        self.generate_model = os.getenv("IMAGE_MODEL")
        
        # é“¾æ¥ç”Ÿæˆå¼€å…³ï¼ˆé»˜è®¤å…³é—­ï¼‰
        self.enable_links = enable_links
        
        # è”ç½‘æœç´¢æ¨¡å‹ï¼ˆç”¨äºè·å–çœŸå®é“¾æ¥ï¼‰
        self.search_api_key = os.getenv("SEARCH_API_KEY")
        self.search_base_url = os.getenv("SEARCH_BASE_URL", "https://yunwu.ai/v1")
        self.search_model = os.getenv("SEARCH_MODEL", "gpt-5-all")
        
        # æ‰“å°é“¾æ¥ç”ŸæˆçŠ¶æ€
        if self.enable_links:
            print(f"ğŸ”— é“¾æ¥ç”ŸæˆåŠŸèƒ½: âœ… å·²å¯ç”¨")
            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡
            if not self.search_api_key:
                print(f"   âš ï¸  è­¦å‘Š: SEARCH_API_KEYæœªè®¾ç½®ï¼Œé“¾æ¥ç”Ÿæˆå¯èƒ½å¤±è´¥")
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯èƒ½çš„ API key åç§°
                alternative_keys = ["GENERATE_API_KEY", "IMAGE_API_KEY", "CHAT_API_KEY"]
                found_alternatives = []
                for key in alternative_keys:
                    if os.getenv(key):
                        found_alternatives.append(key)
                if found_alternatives:
                    print(f"   ğŸ’¡ æç¤º: æ‰¾åˆ°äº†å…¶ä»– API key: {', '.join(found_alternatives)}")
                    print(f"      ğŸ’¡ ä½†éœ€è¦çš„æ˜¯ SEARCH_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        else:
            print(f"ğŸ”— é“¾æ¥ç”ŸæˆåŠŸèƒ½: âš ï¸  å·²ç¦ç”¨ (enable_links=False)")
        
        # åˆ›æ„ç”Ÿæˆå™¨
        self.ideas = ideas
        
        # æ ·ä¾‹ç›®å½•
        self.examples_dir = examples_dir
        
        # RAG settings
        self.rag_enabled = True  # Enable RAG-based example retrieval
        self.dataset_name = "redbook"  # Dataset name for cache identification
        self.data_root = os.path.join(os.path.dirname(__file__), "..", "download", "redbook")
        self._rag_cache = {}  # Cache for RAG retrieval results (key: user_id)
        
        # Initialize RAG embedding helper
        self.rag_helper = RAGEmbeddingHelper(
            api_key=self.search_api_key,
            api_base=self.search_base_url,
            cache_dir=os.path.join(os.path.dirname(__file__), "..", "embeddings_cache")
        )
        
        # åŠ è½½å°çº¢ä¹¦æ ·ä¾‹ï¼ˆå°†æ”¹ä¸ºæ”¯æŒ RAGï¼‰
        self.examples = []  # åˆå§‹åŒ–ä¸ºç©ºï¼Œå°†åœ¨éœ€è¦æ—¶åŠ¨æ€åŠ è½½
        
        # Reflection mechanism settings
        self.reflection_enabled = True  # Enable reflection by default
        self.reflection_threshold = float(os.getenv("REFLECTION_THRESHOLD_IT", "0.65"))
        # Maximum reflection iterations (can be configured via environment variable)
        # First 3 iterations use specific strategies, iterations >= 3 all use iteration 2's strategy
        self.max_reflection_iterations = int(os.getenv("MAX_REFLECTION_ITERATIONS", "3"))
        self.reflection_strict_mode = os.getenv("REFLECTION_STRICT_MODE", "true").lower() == "true"  # ä¸¥æ ¼æ¨¡å¼ï¼šåªæ¥å—æå‡scoreçš„ä¿®æ”¹
        
        # Initialize reflection components
        try:
            self.html_parser = HTMLParserForReflection()
            self.reflection_advisor = ReflectionAdvisor()
            
            # Initialize CLIP evaluator for GroupScore calculation
            clip_model = os.getenv("CLIP_MODEL", "ViT-B/32")
            clip_device = os.getenv("CLIP_DEVICE", "cuda")
            self.clip_evaluator = CLIPEvaluator(model_name=clip_model, device=clip_device)
            
            strict_mode_status = "âœ… å¼€å¯" if self.reflection_strict_mode else "âš ï¸  å…³é—­"
            print(f"âœ… Reflectionæœºåˆ¶å·²å¯ç”¨ (é˜ˆå€¼: {self.reflection_threshold}, æœ€å¤š{self.max_reflection_iterations}æ¬¡è¿­ä»£, ä¸¥æ ¼æ¨¡å¼: {strict_mode_status})")
            if self.max_reflection_iterations > 3:
                print(f"   â„¹ï¸  å‰3æ¬¡ä½¿ç”¨ç‰¹å®šç­–ç•¥ï¼Œç¬¬4-{self.max_reflection_iterations}æ¬¡é‡å¤ä½¿ç”¨ç¬¬3æ¬¡ç­–ç•¥")
        except Exception as e:
            print(f"âš ï¸  Reflectionæœºåˆ¶åˆå§‹åŒ–å¤±è´¥: {e}, å°†è·³è¿‡reflection")
            self.reflection_enabled = False
    
    def extract_top1_preference(self, profile_data):
        """
        Extract top1 preference from user profile
        
        Args:
            profile_data: User profile dict or string
            
        Returns:
            String containing the top1 preference description
        """
        if isinstance(profile_data, dict):
            profile_text = profile_data.get("profile_text", json.dumps(profile_data, ensure_ascii=False))
        else:
            profile_text = str(profile_data)
        
        # Try to extract "1. Preference 1:" or "Preference 1:" pattern
        patterns = [
            r'1\.\s*Preference\s*1:\s*([^\n]+(?:\n\s+Reason:[^\n]+)?)',
            r'Preference\s*1:\s*([^\n]+(?:\n\s+Reason:[^\n]+)?)',
            r'1\.\s*([^\n]+(?:\n\s+Reason:[^\n]+)?)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, profile_text, re.IGNORECASE)
            if match:
                preference = match.group(1).strip()
                print(f"ğŸ“‹ Extracted Top1 Preference: {preference[:100]}...")
                return preference
        
        # Fallback: return first 500 chars of profile
        print(f"âš ï¸ Could not extract structured preference, using profile preview")
        return profile_text[:500]
    
    def extract_user_id_from_path(self, file_path):
        """
        Extract user ID from file path
        
        Args:
            file_path: Path to user profile or output directory
            
        Returns:
            User ID string, or None if not found
        """
        # Try to extract from path pattern like: generated_it/0_5435e123d6e4a965e190095a/
        # or download/redbook/{user_id}/
        
        # Pattern 1: {number}_{alphanumeric_id}
        match = re.search(r'[\\/](\d+_[a-f0-9]+)[\\/]', file_path)
        if match:
            full_id = match.group(1)
            # Extract the alphanumeric part after underscore
            user_id = full_id.split('_', 1)[1] if '_' in full_id else full_id
            print(f"ğŸ“Œ Extracted user_id from path: {user_id}")
            return user_id
        
        # Pattern 2: pure numeric ID
        match = re.search(r'[\\/](\d{10,})[\\/]', file_path)
        if match:
            user_id = match.group(1)
            print(f"ğŸ“Œ Extracted user_id from path: {user_id}")
            return user_id
        
        print(f"âš ï¸ Could not extract user_id from path: {file_path}")
        return None
    
    def load_examples_with_rag(self, user_id, top1_preference, top_k=3):
        """
        Load examples using RAG (Retrieval-Augmented Generation)
        Retrieves most relevant examples from user's historical and recommended posts
        
        Args:
            user_id: User ID
            top1_preference: Top1 preference text for similarity matching
            top_k: Number of examples to retrieve
            
        Returns:
            List of example dicts with content
        """
        # Check cache
        cache_key = f"{user_id}_{top_k}"
        if cache_key in self._rag_cache:
            print(f"âœ… Using cached RAG examples ({len(self._rag_cache[cache_key])} posts)")
            return self._rag_cache[cache_key]
        
        print(f"ğŸ” RAG Mode: Retrieving relevant examples for user {user_id}...")
        
        try:
            # Step 1: Build embeddings for user files
            print(f"ğŸ“Š Building embeddings for user files...")
            embeddings_data = self.rag_helper.build_embeddings_for_user(
                dataset_name=self.dataset_name,
                dataset_root=self.data_root,
                user_id=user_id,
                max_workers=10,
                use_cache=True
            )
            
            if not embeddings_data["embeddings"]:
                print(f"âš ï¸ No embeddings found, falling back to default examples")
                return self._load_examples_fallback()
            
            # Step 2: Retrieve top-k similar files
            print(f"ğŸ¯ Retrieving top-{top_k} similar examples based on preference...")
            similar_files = self.rag_helper.retrieve_top_k_similar(
                query_text=top1_preference,
                embeddings_data=embeddings_data,
                top_k=top_k
            )
            
            if not similar_files:
                print(f"âš ï¸ No similar files found, falling back to default examples")
                return self._load_examples_fallback()
            
            # Step 3: Load file contents and parse (including images)
            examples = []
            for i, file_info in enumerate(similar_files):
                try:
                    content = self.rag_helper.get_file_content(file_info["path"])
                    
                    if not content:
                        continue
                    
                    # Add text content
                    examples.append({
                        "type": "text",
                        "content": content,
                        "filename": file_info["filename"],
                        "similarity": file_info["similarity"],
                        "folder": file_info["folder"],
                        "post_id": file_info["post_id"]
                    })
                    
                    print(f"   âœ… Retrieved: {file_info['filename']} (similarity: {file_info['similarity']:.3f})")
                    
                    # Try to load the first image from this post as a style reference
                    try:
                        # Get the post directory (parent of note.txt)
                        post_dir = Path(file_info["path"]).parent
                        images_dir = post_dir / "images"
                        
                        if images_dir.exists():
                            # Find the first image
                            image_files = sorted(images_dir.glob("*.jpg")) + sorted(images_dir.glob("*.png")) + sorted(images_dir.glob("*.webp"))
                            if image_files:
                                first_image = image_files[0]
                                with open(first_image, 'rb') as img_f:
                                    image_data = img_f.read()
                                    base64_image = base64.b64encode(image_data).decode('utf-8')
                                    
                                    # Determine image format
                                    ext = first_image.suffix.lower().strip('.')
                                    if ext == 'jpg':
                                        ext = 'jpeg'
                                    
                                    examples.append({
                                        "type": "image",
                                        "content": base64_image,
                                        "format": ext,
                                        "filename": first_image.name,
                                        "post_id": file_info["post_id"],
                                        "similarity": file_info["similarity"]
                                    })
                                    print(f"      ğŸ“¸ Loaded image: {first_image.name}")
                    except Exception as img_err:
                        # Image loading is optional, don't fail the whole process
                        pass
                        
                except Exception as e:
                    print(f"   âš ï¸ Failed to load {file_info['path']}: {e}")
            
            # Cache the results
            self._rag_cache[cache_key] = examples
            
            # Count text and image examples
            text_count = sum(1 for e in examples if e["type"] == "text")
            image_count = sum(1 for e in examples if e["type"] == "image")
            print(f"ğŸ’¾ Cached {len(examples)} RAG example(s): {text_count} æ–‡æœ¬, {image_count} å›¾ç‰‡")
            
            return examples
            
        except Exception as e:
            print(f"âš ï¸ RAG retrieval failed: {e}")
            import traceback
            traceback.print_exc()
            return self._load_examples_fallback()
    
    def _load_examples_fallback(self):
        """
        Fallback method: Load examples from fixed directory (agents/redbook/)
        This is the original _load_examples logic
        """
        examples = []
        
        if not os.path.exists(self.examples_dir):
            print(f"âš ï¸ æ ·ä¾‹ç›®å½•ä¸å­˜åœ¨: {self.examples_dir}ï¼Œå°†ä¸ä½¿ç”¨æ ·ä¾‹")
            return examples
        
        try:
            # éå†ç›®å½•ä¸­çš„æ–‡ä»¶
            for filename in os.listdir(self.examples_dir):
                filepath = os.path.join(self.examples_dir, filename)
                
                # å¤„ç†æ–‡æœ¬æ–‡ä»¶
                if filename.endswith('.txt'):
                    with open(filepath, 'r', encoding='utf-8') as f:
                        text_content = f.read().strip()
                        if text_content:
                            examples.append({
                                "type": "text",
                                "content": text_content,
                                "filename": filename
                            })
                
                # å¤„ç†å›¾ç‰‡æ–‡ä»¶
                elif filename.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                    with open(filepath, 'rb') as f:
                        image_data = f.read()
                        base64_image = base64.b64encode(image_data).decode('utf-8')
                        
                        # ç¡®å®šå›¾ç‰‡æ ¼å¼
                        ext = filename.lower().split('.')[-1]
                        if ext == 'jpg':
                            ext = 'jpeg'
                        
                        examples.append({
                            "type": "image",
                            "content": base64_image,
                            "format": ext,
                            "filename": filename
                        })
            
            if examples:
                text_count = sum(1 for e in examples if e["type"] == "text")
                image_count = sum(1 for e in examples if e["type"] == "image")
                print(f"âœ… åŠ è½½äº†RAG {len(examples)} ä¸ªæ ·ä¾‹: {text_count} ä¸ªæ–‡æœ¬, {image_count} å¼ å›¾ç‰‡")
            else:
                print(f"âš ï¸ {self.examples_dir} ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°æ ·ä¾‹æ–‡ä»¶")
        
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ ·ä¾‹æ—¶å‡ºé”™: {e}")
        
        return examples
    
    def _load_examples(self, user_profile_path=None, profile_data=None):
        """
        Load example posts - supports both RAG mode and fallback mode
        
        Args:
            user_profile_path: Path to user profile (for extracting user_id)
            profile_data: User profile data (for extracting top1 preference)
            
        Returns:
            List of example dicts with content
        """
        # If RAG is enabled and we have necessary info, use RAG
        if self.rag_enabled and user_profile_path and profile_data:
            try:
                # Extract user_id from path
                user_id = self.extract_user_id_from_path(user_profile_path)
                
                if user_id:
                    # Extract top1 preference
                    top1_preference = self.extract_top1_preference(profile_data)
                    
                    # Use RAG to retrieve examples (top-3 for consistency with reflection)
                    examples = self.load_examples_with_rag(user_id, top1_preference, top_k=3)
                    
                    if examples:
                        return examples
                    else:
                        print("âš ï¸ RAG returned no examples, using fallback")
                else:
                    print("âš ï¸ Could not extract user_id, using fallback")
            except Exception as e:
                print(f"âš ï¸ RAG mode failed: {e}, using fallback")
                import traceback
                traceback.print_exc()
        
        # Fallback: use fixed examples
        return self._load_examples_fallback()

    def _is_ai_refusal(self, text):
        """
        æ£€æµ‹AIæ˜¯å¦æ‹’ç»ç”Ÿæˆå†…å®¹
        
        Args:
            text: AIè¿”å›çš„æ–‡æœ¬
            
        Returns:
            bool: å¦‚æœæ˜¯æ‹’ç»å“åº”è¿”å›True
        """
        if not text or len(text) < 10:
            return True
        
        # å¸¸è§çš„æ‹’ç»å“åº”æ¨¡å¼
        refusal_patterns = [
            "I'm sorry, I can't assist with that",
            "I cannot assist with that",
            "I can't help with that",
            "I'm unable to assist",
            "I cannot provide",
            "I'm sorry, but I can't",
            "I apologize, but I cannot",
            "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ååŠ©",
            "æŠ±æ­‰ï¼Œæˆ‘ä¸èƒ½",
            "å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•",
        ]
        
        text_lower = text.lower().strip()
        
        for pattern in refusal_patterns:
            if pattern.lower() in text_lower:
                return True
        
        # æ£€æµ‹è¿‡çŸ­çš„å“åº”ï¼ˆé€šå¸¸æ˜¯æ‹’ç»ï¼‰
        if len(text) < 50:
            return True
        
        return False
    
    def generate_text(self, user_profile, user_profile_path=None, profile_data=None):
        """æ ¹æ®ç”¨æˆ·ç”»åƒç”Ÿæˆæ–‡æ¡ˆï¼Œæ”¯æŒ RAG æ¨¡å¼åŠ¨æ€åŠ è½½æ ·ä¾‹
        
        Args:
            user_profile: User profile text
            user_profile_path: Path to user profile (for RAG)
            profile_data: Profile data dict (for RAG)
        """
        
        # åŠ è½½æ ·ä¾‹ï¼ˆæ”¯æŒ RAG æ¨¡å¼ï¼‰
        examples = self._load_examples(user_profile_path=user_profile_path, profile_data=profile_data)
        
        # æ„å»ºåˆ›æ„æç¤º
        idea_prompt = f"""
        **åˆ›æ„æŒ‡å¯¼ï¼š**
        {json.dumps(self.ideas, ensure_ascii=False, indent=2)}
        """
        
        # æ„å»ºæ ·ä¾‹æç¤ºï¼ˆå¦‚æœæœ‰æ ·ä¾‹ï¼‰
        examples_prompt = ""
        image_examples = []
        
        if examples:
            examples_prompt = "\n**å‚è€ƒæ ·ä¾‹ï¼š**\n"
            
            # Extract text examples (åªä¿ç•™1ä¸ªï¼Œä¸”æ›´çŸ­)
            text_examples = [e for e in examples if e["type"] == "text"]
            if text_examples:
                examples_prompt += f"\næ ·ä¾‹:\n{text_examples[0]['content'][:200]}\n"  # åªä¿ç•™ç¬¬ä¸€ä¸ªï¼Œä¸”åª200å­—
            
            # Extract image examples for multimodal reference (ä¿ç•™ä½†ç®€åŒ–è¯´æ˜)
            image_examples = [e for e in examples if e["type"] == "image"]
            if image_examples:
                examples_prompt += f"\n**é…å›¾æ ·ä¾‹ï¼šå·²æä¾› {len(image_examples)} å¼ å‚è€ƒå›¾ç‰‡**\n"
        
        # Build prompt text
        prompt_text = f"""
            ä½ æ˜¯ä¸€ä½æ´»è·ƒåœ¨å°çº¢ä¹¦å¹³å°çš„çœŸå®åšä¸»ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·ç”»åƒå’Œåˆ›æ„æŒ‡å¯¼åˆ›ä½œä¸€ç¯‡å°çº¢ä¹¦é£æ ¼çš„å›¾æ–‡å¸–å­ã€‚    

            **ç”¨æˆ·ç”»åƒç‰¹å¾ï¼š**
            {user_profile}

            {idea_prompt}
            
            {examples_prompt}
            
            â­ **æ ¸å¿ƒåˆ›ä½œåŸåˆ™**ï¼š
            - åƒçœŸäººä¸€æ ·å†™ä½œï¼Œè¡¨è¾¾è¦æœ‰å˜åŒ–å’Œå¤šæ ·æ€§
            - é¿å…AIå¼çš„é‡å¤ç”¨è¯ï¼ˆå¦‚åå¤è¯´"ç»ç»å­"ã€"å®å­ä»¬"ï¼‰
            - åŒæ ·çš„æ„æ€ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼
            - ä¿æŒè‡ªç„¶ã€çœŸè¯šã€æœ‰ä¸ªæ€§
            
            **åˆ›ä½œè¦æ±‚ï¼š**
            ä¸€ã€æ ¼å¼è¦æ±‚ï¼š
            1. **æ ‡ç­¾ï¼ˆTagsï¼‰- å¿…é¡»é¦–å…ˆè¾“å‡º**ï¼š
               - åœ¨æ­£æ–‡å¼€å§‹ä¹‹å‰ï¼Œå…ˆè¾“å‡º 1-4 ä¸ªå°çº¢ä¹¦é£æ ¼çš„æ ‡ç­¾
               - æ ¼å¼ï¼šåœ¨ç¬¬ä¸€è¡Œè¾“å…¥ "===TAGS===" åæ¢è¡Œï¼Œæ¯ä¸ªæ ‡ç­¾å•ç‹¬ä¸€è¡Œ
               - æ ‡ç­¾è¦è´´è¿‘å°çº¢ä¹¦çœŸå®é£æ ¼ï¼Œä¾‹å¦‚ï¼š
                 * ç¾é£Ÿç±»ï¼šç¾é£Ÿæ¢åº—ã€ç«é”…ã€æˆéƒ½ç¾é£Ÿã€äººå‡100ä»¥ä¸‹ã€å·èœ
                 * æ—…è¡Œç±»ï¼šæ—…è¡Œvlogã€æ­å·æ—…æ¸¸ã€å‘¨æœ«å»å“ªç©ã€æ±Ÿå—æ°´ä¹¡ã€å°ä¼—æ™¯ç‚¹
                 * å¥½ç‰©ç±»ï¼šå¥½ç‰©åˆ†äº«ã€æ•°ç æµ‹è¯„ã€iPhoneã€æ€§ä»·æ¯”ä¹‹é€‰ã€ç§‘æŠ€å¥½ç‰©
                 * ç”Ÿæ´»ç±»ï¼šæ—¥å¸¸ç”Ÿæ´»ã€å‘¨æœ«æ—¥è®°ã€å’–å•¡åº—ã€æ°›å›´æ„Ÿã€æ¾å¼›æ„Ÿç”Ÿæ´»
               - æ ‡ç­¾è¦å…·ä½“ï¼Œèƒ½ä½“ç°å†…å®¹æ ¸å¿ƒï¼Œé¿å…è¿‡äºå®½æ³›
               - è¾“å‡ºå®Œtagsåæ¢è¡Œè¾“å…¥ "===CONTENT===" å†å¼€å§‹æ­£æ–‡
            
            2. **æ­£æ–‡**ï¼šè¯­è¨€è‡ªç„¶ã€ç»†èŠ‚ä¸°å¯Œã€å£è¯­åŒ–ï¼ˆä¸­æ–‡ï¼‰ï¼Œ300-800å­—ä¸ºå®œï¼Œä½†ä¸Šä¸è®¾é™ã€‚
            
            3. **æ’ç‰ˆè¦æ±‚ï¼ˆé‡è¦ï¼‰**ï¼š
               - è¯·é€‚å½“ä½¿ç”¨ **åŠ ç²—** (markdownè¯­æ³•) æ¥æ ‡è®°å…³é”®è¯æˆ–é‡ç‚¹ï¼Œè¿™èƒ½æå‡é˜…è¯»ä½“éªŒã€‚
               - æ®µè½ä¹‹é—´è¦åˆ†æ˜ï¼Œå–„ç”¨emojiåˆ†éš”æˆ–è£…é¥°ï¼ˆå¦‚âœ¨ğŸ”¥ğŸ’•ç­‰ï¼‰ã€‚
               - ä¸è¦ä½¿ç”¨ä¸€çº§æˆ–äºŒçº§æ ‡é¢˜ï¼ˆ# æˆ– ##ï¼‰ï¼Œä½¿ç”¨emojiæˆ–åŠ ç²—æ¥å¼•å¯¼å°èŠ‚ã€‚
            
            4. **é“¾æ¥æ¨è**ï¼š
               - åœ¨æ­£æ–‡ç»“æŸåï¼Œæ¢è¡Œè¾“å…¥ "===LINKS==="ã€‚
               - æ¨è 2 ä¸ªä¸å¸–å­å†…å®¹é«˜åº¦ç›¸å…³çš„**å›½å†…å¹³å°**å»¶ä¼¸é˜…è¯»å†…å®¹çš„æœç´¢å…³é”®è¯ã€‚
               - æ ¼å¼å¿…é¡»ä¸ºï¼š**æ¨èè¯­/æ ‡é¢˜ | å¹³å°åç§° | æœç´¢å…³é”®è¯**
               - å¹³å°ä»…é™ï¼š**å°çº¢ä¹¦ã€Bç«™ã€çŸ¥ä¹ã€æŠ–éŸ³ã€å¾®åš**ã€‚
               - æœç´¢å…³é”®è¯è¦ç²¾ç¡®ï¼ˆå¦‚"iPhone 15 Pro æµ‹è¯„"è€Œé"æ‰‹æœºæµ‹è¯„"ï¼‰ã€‚
            
            äºŒã€å†…å®¹è¦æ±‚ï¼ˆå°çº¢ä¹¦é£æ ¼ï¼‰ï¼š
            0. **æ–‡æœ¬å½¢å¼**ï¼šè§†åˆ›æ„å’Œç”»åƒè€Œå®šï¼Œå¯ä»¥æ˜¯ç§è‰åˆ†äº«ã€æ¢åº—æ—¥è®°ã€æ•™ç¨‹æ”»ç•¥ã€å¥½ç‰©æ¨èã€æ—…è¡Œvlogç­‰å°çº¢ä¹¦å¸¸è§å½¢å¼
            
            1. **è¯­è¨€é£æ ¼ - è´´è¿‘å°çº¢ä¹¦ç”¨æˆ·ï¼ˆæ³¨æ„å¤šæ ·åŒ–è¡¨è¾¾ï¼‰**ï¼š
               
               ğŸ“¢ **ç§°å‘¼æ–¹å¼ï¼ˆæ¢ç€ç”¨ï¼Œä¸è¦æ€»ç”¨åŒä¸€ä¸ªï¼‰**ï¼š
               - å¼€å¤´ç§°å‘¼ï¼šå§å¦¹ä»¬ / å®å­ä»¬ / é›†ç¾ä»¬ / å®¶äººä»¬ / æœ‹å‹ä»¬ / å¤§å®¶
               - æˆ–è€…ç›´æ¥å¼€é—¨è§å±±ï¼Œä¸ç”¨ç§°å‘¼
               
               âœ¨ **è¡¨è¾¾èµç¾ï¼ˆé¿å…é‡å¤"ç»ç»å­"ï¼‰**ï¼š
               - è¶…çº§å¥½ï¼šç»äº† / å¤ªæ£’äº† / çˆ±äº†çˆ±äº† / çœŸé¦™ / æ— æ•Œäº† / å¤ªå¯äº† / å¥½çˆ± / å¤ªèµäº†
               - å¾ˆæ¨èï¼šyyds / å¼ºæ¨ / å¿…å†² / å€¼å¾— / ä¸è¸©é›· / é—­çœ¼å…¥ / å¯ä»¥è¯•è¯• / çœŸå¿ƒæ¨è
               - æƒŠè‰³ï¼šæƒŠè‰³åˆ°æˆ‘äº† / å¤ªç¾äº† / è¢«åœˆç²‰äº† / æ²¦é™·äº† / ä¸Šå¤´äº† / æ¬²ç½¢ä¸èƒ½
               
               ğŸ’• **è¡¨è¾¾æ„Ÿå—ï¼ˆè‡ªç„¶çœŸå®ï¼‰**ï¼š
               - å¥½è¯„ï¼šå¥½åƒå“­äº† / çˆ±ä¸é‡Šæ‰‹ / å¿µå¿µä¸å¿˜ / å›è´­æ— æ•°æ¬¡ / ç»ˆäºæ‰¾åˆ°äº†
               - æ°›å›´æ„Ÿï¼šæ¾å¼›æ„Ÿæ»¡æ»¡ / æ°›å›´æ„Ÿæ‹‰æ»¡ / æ²»æ„ˆç³» / å¾ˆchill / å¾ˆèˆ’æœ / å²æœˆé™å¥½
               - æƒ…ç»ªï¼šè°æ‡‚å•Š / ç ´é˜²äº† / emoäº† / æœ‰è¢«æ„ŸåŠ¨åˆ° / å¤ªæ²»æ„ˆäº†
               
               ğŸ¯ **æ¨èè¡¨è¾¾ï¼ˆçµæ´»è¿ç”¨ï¼‰**ï¼š
               - æ¨èï¼šç§è‰äº† / å®‰åˆ©ç»™ä½ ä»¬ / åˆ†äº«ç»™ä½ ä»¬ / å¢™è£‚æ¨è / èµ¶ç´§å†² / å€¼å¾—æ‹¥æœ‰
               - åŠé€€ï¼šé¿é›· / ä¸æ¨è / è¸©é›·äº† / ç¿»è½¦äº† / åˆ«ä¹° / æ…å…¥
               
               ğŸ”¥ **ç½‘ç»œç”¨è¯­ï¼ˆé€‚åº¦ä½¿ç”¨ï¼Œä¸è¦å †ç Œï¼‰**ï¼š
               - ç¨‹åº¦è¯ï¼šç‹ ç‹ åœ° / ç–¯ç‹‚ / æ— é™ / æ­»ç£•
               - çŠ¶æ€è¯ï¼šDNAåŠ¨äº† / ç ´é˜² / ä¸Šå¤´ / æ²¦é™· / ç ´å¤§é˜²
               - å½¢å®¹è¯ï¼šç»ç¾ / å¥ˆæ–¯ / å·¨å¥½ / å¤ªå¯¹äº† / å°±æ˜¯å®ƒ
               
               ğŸ’¬ **è¯­è¨€æŠ€å·§**ï¼š
               - å£è¯­åŒ–ã€å¹´è½»åŒ–ï¼Œä½¿ç”¨æ„Ÿå¹å·è¡¨è¾¾çƒ­æƒ…ï¼
               - é€‚å½“ä½¿ç”¨emojiï¼ˆä½†ä¸è¦è¿‡åº¦ï¼‰
               - å¤šç”¨çŸ­å¥ï¼ŒèŠ‚å¥è½»å¿«
               - å¶å°”ç”¨ç–‘é—®å¥å¢åŠ äº’åŠ¨æ„Ÿ
               - é¿å…åŒä¸€è¯æ±‡åå¤å‡ºç°ï¼ˆå¦‚è¿ç»­3æ¬¡"ç»ç»å­"ï¼‰
               - ä¿æŒçœŸè¯šï¼Œä¸è¦åˆ»æ„å †ç Œç½‘ç»œç”¨è¯­
               
            2. **æƒ…æ„ŸçœŸå®ã€çƒ­æƒ…åˆ†äº«**ï¼š
               - è¡¨è¾¾çœŸå®çš„æƒ…ç»ªï¼Œå¯ä»¥ç”¨"çœŸçš„è¶…å¥½åƒï¼"ã€"æˆ‘çˆ±äº†ï¼"ã€"å¼ºçƒˆæ¨èï¼"ç­‰
               - åƒç»™æœ‹å‹å®‰åˆ©ä¸€æ ·çš„è¯­æ°”
            
            3. **ç»“æ„è‡ªç”±ä½†è¦æœ‰é‡ç‚¹**ï¼š
               - å¼€å¤´æŠ“çœ¼çƒï¼ˆå¯ä»¥ç›´æ¥è¯´ç»“è®ºï¼Œå¦‚"è¿™å®¶åº—æˆ‘è¦å¹çˆ†ï¼"ï¼‰
               - ä¸­é—´è¯¦ç»†ä»‹ç»
               - ç»“å°¾å¯ä»¥äº’åŠ¨ï¼ˆå¦‚"ä½ ä»¬å»è¿‡å—ï¼Ÿ"ã€"è¯„è®ºåŒºè¯´è¯´ä½ çš„æœ€çˆ±"ï¼‰
            
            **é¿å…ä»¥ä¸‹AIå¸¸è§é—®é¢˜ï¼š**
            - âŒ ä¸è¦ä½¿ç”¨ä¸¥è‚ƒçš„è®®è®ºæ–‡å£å»
            - âŒ ä¸è¦è¿‡åº¦å †ç Œç½‘ç»œç”¨è¯­ï¼Œä¿æŒè‡ªç„¶
            - âŒ ä¸è¦å†™æˆå¹¿å‘Šè½¯æ–‡ï¼Œè¦åƒçœŸå®åˆ†äº«
            - âŒ **ä¸è¦é‡å¤ä½¿ç”¨åŒæ ·çš„è¯æ±‡**ï¼ˆå¦‚ä¸è¦3æ¬¡éƒ½è¯´"ç»ç»å­"ï¼Œè¦æ¢ç€ç”¨"ç»äº†"ã€"å¤ªæ£’äº†"ã€"çˆ±äº†"ç­‰ï¼‰
            - âŒ **ä¸è¦æ¯æ®µå¼€å¤´éƒ½ç”¨åŒæ ·çš„ç§°å‘¼**ï¼ˆå¦‚ä¸è¦æ®µæ®µéƒ½æ˜¯"å®å­ä»¬"ï¼‰
            - âœ… ä¿æŒè¡¨è¾¾çš„å¤šæ ·æ€§å’Œè‡ªç„¶æ€§ï¼ŒåƒçœŸäººå†™ä½œä¸€æ ·æœ‰å˜åŒ–

            è¯·å¼€å§‹åˆ›ä½œå°çº¢ä¹¦é£æ ¼çš„å›¾æ–‡å¸–å­ï¼š
            """
        
        # Build message content (support multimodal if images are provided)
        message_content = []
        
        # Add image examples first (if available) for visual style reference
        if image_examples:
            print(f"   ğŸ“¸ æä¾› {len(image_examples)} å¼ å›¾ç‰‡æ ·ä¾‹ä½œä¸ºè§†è§‰é£æ ¼å‚è€ƒ")
            for i, img_ex in enumerate(image_examples[:2], 1):  # Max 2 images to avoid token limits
                message_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/{img_ex['format']};base64,{img_ex['content']}"
                    }
                })
            message_content.append({
                "type": "text",
                "text": f"ä»¥ä¸Šæ˜¯ {len(image_examples[:2])} å¼ å°çº¢ä¹¦çœŸå®å¸–å­çš„é…å›¾æ ·ä¾‹ï¼Œè¯·å‚è€ƒå®ƒä»¬çš„è§†è§‰é£æ ¼ã€æ„å›¾å’Œè‰²è°ƒã€‚\n\n"
            })
        
        # Add main prompt text
        message_content.append({
            "type": "text",
            "text": prompt_text
        })
        
        # Make API call with retry mechanism for refusal responses
        max_retries = 3
        full_content = None
        
        for attempt in range(1, max_retries + 1):
            try:
                resp = requests.post(
                    f"{self.chat_base_url}/chat/completions",
                    headers={"Authorization": f"Bearer {self.chat_api_key}"},
                    json={
                        "model": self.chat_model,
                        "messages": [{"role": "user", "content": message_content}],
                        "temperature": 0.3 + (attempt - 1) * 0.05,  # ä½temperatureè®©åˆå§‹ç”Ÿæˆæ›´ç®€å•åŸºç¡€
                        "top_p": 0.9,
                    },
                    timeout=60
                )
                
                if resp.status_code != 200:
                    print(f"   âš ï¸  APIé”™è¯¯ {resp.status_code}: {resp.text[:200]}")
                    if attempt < max_retries:
                        print(f"   ğŸ”„ é‡è¯• {attempt + 1}/{max_retries}...")
                        import time
                        time.sleep(2)
                        continue
                    else:
                        raise Exception(f"APIè°ƒç”¨å¤±è´¥: {resp.status_code}")
                
                full_content = resp.json()["choices"][0]["message"]["content"].strip()
                
                # æ£€æµ‹AIæ˜¯å¦æ‹’ç»ç”Ÿæˆ
                if self._is_ai_refusal(full_content):
                    print(f"   âš ï¸  æ£€æµ‹åˆ°AIæ‹’ç»å“åº”ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰")
                    print(f"       å†…å®¹: {full_content[:100]}...")
                    
                    if attempt < max_retries:
                        print(f"   ğŸ”„ é‡è¯•ç”Ÿæˆ...")
                        import time
                        time.sleep(1)
                        continue
                    else:
                        print(f"   âŒ {max_retries}æ¬¡å°è¯•åä»ç„¶è¢«æ‹’ç»")
                        print(f"   ğŸ’¡ è·³è¿‡è¯¥ç”¨æˆ·...")
                        raise AIRefusalError(f"AIè¿ç»­{max_retries}æ¬¡æ‹’ç»ç”Ÿæˆå†…å®¹")
                else:
                    # æˆåŠŸç”Ÿæˆæ­£å¸¸å†…å®¹
                    if attempt > 1:
                        print(f"   âœ… ç¬¬ {attempt} æ¬¡å°è¯•æˆåŠŸç”Ÿæˆå†…å®¹")
                    break
                    
            except Exception as e:
                print(f"   âŒ ç”Ÿæˆæ–‡æ¡ˆå¼‚å¸¸ (å°è¯• {attempt}/{max_retries}): {str(e)[:100]}")
                if attempt < max_retries:
                    import time
                    time.sleep(2)
                else:
                    raise
        
        # ç¡®ä¿æœ‰å†…å®¹
        if not full_content:
            raise Exception("ç”Ÿæˆæ–‡æ¡ˆå¤±è´¥ï¼šAIæœªè¿”å›æœ‰æ•ˆå†…å®¹")
        
        # --- è§£ææ ‡ç­¾ã€æ­£æ–‡å’Œé“¾æ¥ ---
        tags = []
        text_body = full_content
        links = []
        
        # 1. æå–æ ‡ç­¾
        if "===TAGS===" in full_content:
            parts = full_content.split("===TAGS===", 1)
            remaining = parts[1]
            
            if "===CONTENT===" in remaining:
                tag_section, remaining = remaining.split("===CONTENT===", 1)
                # è§£ææ ‡ç­¾
                for line in tag_section.strip().split('\n'):
                    tag = line.strip()
                    if tag:
                        tags.append(tag)
                
                text_body = remaining
            else:
                # æ²¡æœ‰ ===CONTENT===ï¼Œç›´æ¥ä» ===TAGS=== åé¢æ‰¾æ­£æ–‡
                text_body = remaining
        
        # 2. æå–é“¾æ¥ï¼ˆå…ˆæå–å…³é”®è¯ï¼‰
        link_keywords = []
        if "===LINKS===" in text_body:
            parts = text_body.split("===LINKS===")
            text_body = parts[0].strip()
            link_section = parts[1].strip()
            
            # è§£ææ¯ä¸€è¡Œé“¾æ¥ï¼Œå…ˆæ”¶é›†å…³é”®è¯
            for line in link_section.split('\n'):
                line = line.strip()
                if '|' in line:
                    try:
                        segments = [s.strip() for s in line.split('|')]
                        if len(segments) >= 3:
                            # æ¸…æ´—æ ‡é¢˜ä¸­çš„ç‰¹æ®Šç¬¦å·
                            raw_title = segments[0]
                            clean_title = raw_title.replace('**', '').replace('__', '').replace('*', '')
                            clean_title = re.sub(r'^[\-\â€¢\d\.\s]+', '', clean_title).strip()

                            platform = segments[1]
                            keyword = segments[2]
                            
                            link_keywords.append({
                                "title": clean_title,
                                "platform": platform,
                                "keyword": keyword
                            })
                    except Exception as e:
                        print(f"è§£æé“¾æ¥è¡Œå‡ºé”™: {line} - {e}")
            
            # ä¿æŒ links ä¸ºç©ºåˆ—è¡¨ï¼Œç¨åæ ¹æ®æœç´¢ç»“æœå¡«å……
            # ä¸è¦å°† link_keywords èµ‹å€¼ç»™ linksï¼Œé¿å…å…±äº«å¼•ç”¨
        
        # 3. å¦‚æœæœ‰é“¾æ¥å…³é”®è¯ï¼Œä½¿ç”¨è”ç½‘æœç´¢è·å–çœŸå®é“¾æ¥ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        if link_keywords and self.enable_links:
            print(f"ğŸ” ä½¿ç”¨è”ç½‘æœç´¢è·å–çœŸå®å¸–å­é“¾æ¥ï¼ˆæœ€å¤šé‡è¯•2æ¬¡ï¼‰...")
            real_links = self._search_real_links_with_retry(link_keywords, text_body, max_retries=2)
            if real_links:
                links = real_links
                print(f"âœ… æˆåŠŸè·å– {len(links)} ä¸ªçœŸå®é“¾æ¥")
            else:
                print(f"âš ï¸ è”ç½‘æœç´¢å¤±è´¥ï¼Œé™çº§åˆ°æœç´¢é“¾æ¥")
                # é™çº§ï¼šä½¿ç”¨å…³é”®è¯ç”Ÿæˆæœç´¢é“¾æ¥
                links = []
                for link_kw in link_keywords:
                    search_url = self._generate_search_url(link_kw['platform'], link_kw['keyword'])
                    links.append({
                        "title": link_kw['title'],
                        "platform": link_kw['platform'],
                        "url": search_url
                    })
                print(f"   âœ… å·²ç”Ÿæˆ {len(links)} ä¸ªæœç´¢é“¾æ¥ä½œä¸ºå¤‡é€‰")
        elif link_keywords and not self.enable_links:
            print(f"â„¹ï¸  é“¾æ¥ç”ŸæˆåŠŸèƒ½å·²ç¦ç”¨ï¼ˆenable_links=Falseï¼‰ï¼Œè·³è¿‡ {len(link_keywords)} ä¸ªé“¾æ¥å…³é”®è¯")
        
        return text_body, tags, links
    
    def _search_real_links_with_retry(self, link_keywords, text_content, max_retries=2):
        """
        ä½¿ç”¨è”ç½‘æœç´¢è·å–çœŸå®çš„å¸–å­é“¾æ¥ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            link_keywords: é“¾æ¥å…³é”®è¯åˆ—è¡¨
            text_content: æ–‡æœ¬å†…å®¹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤2æ¬¡ï¼‰
        
        Returns:
            çœŸå®é“¾æ¥åˆ—è¡¨ï¼Œå¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        import time
        
        for attempt in range(1, max_retries + 1):
            print(f"\nğŸ”„ å°è¯• {attempt}/{max_retries} è·å–çœŸå®é“¾æ¥...")
            
            real_links = self._search_real_links(link_keywords, text_content)
            
            if real_links and len(real_links) > 0:
                print(f"âœ… ç¬¬ {attempt} æ¬¡å°è¯•æˆåŠŸï¼Œè·å–åˆ° {len(real_links)} ä¸ªçœŸå®é“¾æ¥")
                return real_links
            else:
                if attempt < max_retries:
                    wait_time = 1  # ç­‰å¾…æ—¶é—´ï¼š1ç§’
                    print(f"âš ï¸ ç¬¬ {attempt} æ¬¡å°è¯•æœªæ‰¾åˆ°çœŸå®é“¾æ¥ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ æ‰€æœ‰ {max_retries} æ¬¡å°è¯•éƒ½æœªæ‰¾åˆ°çœŸå®é“¾æ¥")
        
        return []
    
    def _search_real_links(self, link_keywords, text_content):
        """
        ä½¿ç”¨è”ç½‘æœç´¢è·å–çœŸå®çš„å¸–å­é“¾æ¥ï¼ˆå•æ¬¡å°è¯•ï¼‰
        å‚è€ƒ commentproduct_generator.py çš„å®ç°
        """
        if not self.search_api_key:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            env_value = os.getenv("SEARCH_API_KEY")
            if env_value is None:
                print("âš ï¸ SEARCH_API_KEY not found in environment variables")
                print("   ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­æ˜¯å¦è®¾ç½®äº† SEARCH_API_KEY")
            elif env_value == "":
                print("âš ï¸ SEARCH_API_KEY is empty string")
                print("   ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­ SEARCH_API_KEY çš„å€¼æ˜¯å¦æ­£ç¡®")
            else:
                print(f"âš ï¸ SEARCH_API_KEY exists but is falsy (value length: {len(env_value)})")
            print("   âš ï¸ Fallback to search URLs")
            return None
        
        # æ„å»ºæœç´¢æç¤º
        links_desc = []
        for i, link in enumerate(link_keywords):
            title = link.get('title', '')
            platform = link.get('platform', '')
            keyword = link.get('keyword', '')
            links_desc.append(f"{i+1}. å¹³å°: {platform}, å…³é”®è¯: {keyword}, æœŸæœ›å†…å®¹: {title}")
        
        search_prompt = f"""åŸºäºä»¥ä¸‹å°çº¢ä¹¦å¸–å­å†…å®¹å’Œæ¨èå…³é”®è¯ï¼Œæœç´¢å¹¶è¿”å›2ä¸ª**çœŸå®å­˜åœ¨çš„å¸–å­é“¾æ¥**ã€‚

å¸–å­å†…å®¹ï¼ˆå‰600å­—ï¼‰ï¼š
{text_content[:600]}

æ¨èæ–¹å‘ï¼š
{chr(10).join(links_desc)}

è¿”å›JSONæ ¼å¼ï¼ˆå¿…é¡»æ˜¯çº¯JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š
[
  {{
    "title": "å…·ä½“å¸–å­çš„æ ‡é¢˜",
    "platform": "å¹³å°åç§°ï¼ˆå°çº¢ä¹¦/Bç«™/çŸ¥ä¹/æŠ–éŸ³/å¾®åšï¼‰",
    "url": "çœŸå®çš„å¸–å­URLï¼ˆå¿…é¡»æ˜¯å…·ä½“å¸–å­è€Œéé¦–é¡µæˆ–çƒ­æ¦œï¼‰"
  }}
]

**æ ¸å¿ƒè¦æ±‚ï¼ˆé‡è¦ï¼‰**ï¼š
1. **URL å¿…é¡»æ˜¯å…·ä½“çš„å¸–å­/è§†é¢‘/æ–‡ç« é“¾æ¥ï¼Œä¸èƒ½æ˜¯é¦–é¡µæˆ–æœç´¢é¡µ**
2. **ç›¸å…³æ€§è¦æ±‚å®½æ¾**ï¼šåªè¦ä¸»é¢˜ç›¸å…³å³å¯ï¼Œä¸éœ€è¦å®Œå…¨åŒ¹é…
   - ä¾‹å¦‚ï¼šèˆè¹ˆå›¾æ–‡å¯ä»¥æ¨èã€Šåªæ­¤é’ç»¿ã€‹ç›¸å…³å¸–å­
   - ä¾‹å¦‚ï¼šç¾é£Ÿå›¾æ–‡å¯ä»¥æ¨èåŒç±»å‹ç¾é£Ÿçš„å¸–å­
   - ä¾‹å¦‚ï¼šæ—…è¡Œå›¾æ–‡å¯ä»¥æ¨èåŒç›®çš„åœ°çš„å¸–å­
3. **ä¼˜å…ˆæœç´¢å°çº¢ä¹¦ã€Bç«™ã€çŸ¥ä¹ã€æŠ–éŸ³ã€å¾®åšç­‰å¹³å°çš„ç›¸å…³å†…å®¹**
4. **æ ‡é¢˜è¦çœŸå®å®Œæ•´**
5. **åªè¿”å›èƒ½æ‰¾åˆ°çœŸå®URLçš„é“¾æ¥ï¼Œå¦‚æœæ‰¾ä¸åˆ°å°±è¿”å›ç©ºæ•°ç»„ []**
6. **ä¸è¦è¿”å› search_keywordï¼Œåªè¿”å›æœ‰çœŸå®URLçš„é“¾æ¥**

**è¾“å‡ºè¦æ±‚**ï¼š
- åªè¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—
- å¦‚æœæ‰¾ä¸åˆ°çœŸå®é“¾æ¥ï¼Œè¿”å›ç©ºæ•°ç»„ []
- ç¡®ä¿URLæ˜¯çœŸå®å¯è®¿é—®çš„å…·ä½“å¸–å­é“¾æ¥"""

        try:
            print(f"ğŸ“¡ è°ƒç”¨ {self.search_model} è¿›è¡Œè”ç½‘æœç´¢...")
            response = requests.post(
                f"{self.search_base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.search_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.search_model,
                    "web_search_options": {},  # å¯ç”¨è”ç½‘æœç´¢
                    "messages": [{"role": "user", "content": search_prompt}],
                    "temperature": 0.7
                },
                timeout=90
            )
            
            if response.status_code != 200:
                print(f"âš ï¸ æœç´¢APIé”™è¯¯ {response.status_code}")
                return None
            
            resp_json = response.json()
            
            if "choices" not in resp_json:
                print(f"âš ï¸ å“åº”æ ¼å¼å¼‚å¸¸")
                return None
            
            content = resp_json["choices"][0]["message"]["content"]
            
            # æ¸…ç† content - ç§»é™¤å¼•ç”¨å—å’Œ markdown é“¾æ¥
            content = re.sub(r'^>.*?$', '', content, flags=re.MULTILINE)
            content = re.sub(r'\*\*\[.*?\]\(.*?\)\*\*\s*Â·\s*\*.*?\*', '', content)
            content = re.sub(r'\[.*?\]\(.*?\)', '', content)
            content = re.sub(r'^```json\s*', '', content, flags=re.MULTILINE)
            content = re.sub(r'^```\s*', '', content, flags=re.MULTILINE)
            content = re.sub(r'\n{2,}', '\n', content).strip()
            
            print(f"ğŸ“„ æœç´¢ç»“æœï¼ˆå‰500å­—ç¬¦ï¼‰: {content[:500]}...")
            
            # å°è¯•å¤šç§æ–¹å¼æå– JSON
            search_results = None
            
            # æ–¹æ³•1: å°è¯•ç›´æ¥è§£ææ•´ä¸ª contentï¼ˆå¯èƒ½æ˜¯çº¯JSONï¼‰
            try:
                parsed = json.loads(content.strip())
                if isinstance(parsed, list):
                    search_results = parsed
                    print("âœ… æ–¹æ³•1: ç›´æ¥è§£ææˆåŠŸï¼ˆæ•°ç»„æ ¼å¼ï¼‰")
                elif isinstance(parsed, dict):
                    # å¯èƒ½æ˜¯ {"search_query": [...]} æˆ–å…¶ä»–æ ¼å¼
                    # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„æ•°ç»„å­—æ®µ
                    for key in ['results', 'links', 'data', 'items']:
                        if key in parsed and isinstance(parsed[key], list):
                            search_results = parsed[key]
                            print(f"âœ… æ–¹æ³•1: ä»å­—å…¸ä¸­æå–æ•°ç»„ï¼ˆå­—æ®µ: {key}ï¼‰")
                            break
            except:
                pass
            
            # æ–¹æ³•2: æå– JSON æ•°ç»„ï¼ˆä½¿ç”¨æ‹¬å·åŒ¹é…ï¼‰
            if not search_results:
                first_bracket = content.find('[')
                if first_bracket != -1:
                    bracket_count = 0
                    last_bracket = -1
                    in_string = False
                    escape_next = False
                    
                    for i in range(first_bracket, len(content)):
                        char = content[i]
                        if escape_next:
                            escape_next = False
                            continue
                        if char == '\\':
                            escape_next = True
                            continue
                        if char == '"':
                            in_string = not in_string
                            continue
                        if not in_string:
                            if char == '[':
                                bracket_count += 1
                            elif char == ']':
                                bracket_count -= 1
                                if bracket_count == 0:
                                    last_bracket = i
                                    break
                    
                    if last_bracket != -1:
                        json_content = content[first_bracket:last_bracket + 1].strip()
                        try:
                            search_results = json.loads(json_content)
                            print("âœ… æ–¹æ³•2: æ‹¬å·åŒ¹é…æå–æˆåŠŸ")
                        except:
                            pass
            
            # æ–¹æ³•3: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ JSON æ•°ç»„
            if not search_results:
                json_pattern = r'\[\s*\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}(?:\s*,\s*\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})*\s*\]'
                match = re.search(json_pattern, content, re.DOTALL)
                if match:
                    try:
                        search_results = json.loads(match.group(0))
                        print("âœ… æ–¹æ³•3: æ­£åˆ™è¡¨è¾¾å¼æå–æˆåŠŸ")
                    except:
                        pass
            
            if not search_results:
                print(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON æ•°ç»„ï¼ŒåŸå§‹å†…å®¹å‰500å­—ç¬¦: {content[:500]}")
                return []
            
            if not isinstance(search_results, list):
                print(f"âš ï¸ è§£æç»“æœä¸æ˜¯æ•°ç»„æ ¼å¼: {type(search_results)}")
                return []
            
            # è°ƒè¯•ï¼šæ‰“å°è§£æåçš„ç»“æœç»“æ„
            print(f"ğŸ” è§£æåçš„ç»“æœæ•°é‡: {len(search_results)}")
            if search_results:
                first_result = search_results[0]
                print(f"ğŸ” ç¬¬ä¸€ä¸ªç»“æœç±»å‹: {type(first_result)}")
                if isinstance(first_result, dict):
                    print(f"ğŸ” ç¬¬ä¸€ä¸ªç»“æœå­—æ®µ: {list(first_result.keys())}")
                    print(f"ğŸ” ç¬¬ä¸€ä¸ªç»“æœå†…å®¹: {json.dumps(first_result, ensure_ascii=False, indent=2)[:400]}...")
            
            # éªŒè¯å’Œæå–æœ‰æ•ˆé“¾æ¥ï¼ˆé™ä½æ ‡å‡†ï¼Œåªè¦çœŸå®URLå°±æ¥å—ï¼‰
            valid_links = []
            if isinstance(search_results, list):
                for idx, result in enumerate(search_results):
                    if not isinstance(result, dict):
                        print(f"âš ï¸ ç»“æœ {idx} ä¸æ˜¯å­—å…¸æ ¼å¼: {type(result)}")
                        continue
                    
                    # è°ƒè¯•ï¼šæ‰“å°æ¯ä¸ªç»“æœçš„å­—æ®µ
                    result_keys = list(result.keys())
                    has_url = 'url' in result_keys
                    print(f"ğŸ” å¤„ç†ç»“æœ {idx}: å­—æ®µ={result_keys}, æ˜¯å¦æœ‰urlå­—æ®µ={has_url}")
                    
                    # å¦‚æœæ²¡æœ‰urlå­—æ®µï¼Œæ‰“å°å®Œæ•´å†…å®¹ä»¥ä¾¿è°ƒè¯•
                    if not has_url:
                        print(f"   âš ï¸ ç»“æœ {idx} æ²¡æœ‰urlå­—æ®µï¼Œå®Œæ•´å†…å®¹: {json.dumps(result, ensure_ascii=False)[:200]}...")
                    
                    # åªæ¥å—æœ‰çœŸå® URL çš„é“¾æ¥
                    # å°è¯•å¤šç§å¯èƒ½çš„URLå­—æ®µå
                    url = None
                    for url_field in ['url', 'link', 'href', 'web_url', 'post_url', 'article_url']:
                        if result.get(url_field):
                            url = result.get(url_field, "").strip()
                            if url:
                                print(f"   âœ… ä»å­—æ®µ '{url_field}' æ‰¾åˆ°URL: {url[:60]}...")
                                break
                    
                    if url:
                        
                        # åŸºæœ¬éªŒè¯ï¼šå¿…é¡»æ˜¯ http/https é“¾æ¥
                        if not (url.startswith("http://") or url.startswith("https://")):
                            continue
                        
                        # è¿‡æ»¤æ˜æ˜¾çš„é¦–é¡µå’Œæœç´¢é¡µ
                        if self._is_homepage_url(url):
                            print(f"âš ï¸ è¿‡æ»¤é¦–é¡µé“¾æ¥: {url[:60]}...")
                            continue
                        
                        # è¿‡æ»¤æœç´¢é¡µï¼ˆå…³é”®ï¼šä¸å…è®¸æœç´¢é“¾æ¥ï¼‰
                        if any(indicator in url.lower() for indicator in [
                            '/search?', '/search/', 'search_query=', 'search_result', 
                            '?q=', '&q=', 'keyword=', '&keyword='
                        ]):
                            print(f"âš ï¸ è¿‡æ»¤æœç´¢é¡µé“¾æ¥: {url[:60]}...")
                            continue
                        
                        # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šç¬¦å·
                        raw_title = result.get("title", "ç›¸å…³å¸–å­")
                        clean_title = raw_title.replace('**', '').replace('__', '').replace('*', '')
                        clean_title = re.sub(r'^[\-\â€¢\d\.\s]+', '', clean_title).strip()
                        if not clean_title:
                            clean_title = "ç›¸å…³å¸–å­"
                        
                        valid_links.append({
                            "title": clean_title,
                            "platform": result.get("platform", "ç½‘é¡µ"),
                            "url": url
                        })
                        print(f"âœ… æ‰¾åˆ°çœŸå®é“¾æ¥: {clean_title[:40]}... -> {url[:60]}...")
            
            if valid_links:
                print(f"âœ… æˆåŠŸè·å– {len(valid_links)} ä¸ªçœŸå®é“¾æ¥")
                return valid_links
            else:
                print(f"âš ï¸ æœ¬æ¬¡å°è¯•æœªæ‰¾åˆ°æœ‰æ•ˆé“¾æ¥")
                print(f"   ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
                print(f"      1. APIè¿”å›çš„ç»“æœä¸­æ²¡æœ‰ 'url' å­—æ®µ")
                print(f"      2. æ‰€æœ‰URLéƒ½è¢«è¿‡æ»¤ï¼ˆé¦–é¡µ/æœç´¢é¡µï¼‰")
                print(f"      3. URLæ ¼å¼ä¸æ­£ç¡®ï¼ˆä¸æ˜¯http/httpsï¼‰")
                return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¾¿äºé‡è¯•æœºåˆ¶åˆ¤æ–­
        
        except Exception as e:
            print(f"âš ï¸ è”ç½‘æœç´¢å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¾¿äºé‡è¯•æœºåˆ¶åˆ¤æ–­
    
    def _is_homepage_url(self, url):
        """æ£€æµ‹æ˜¯å¦ä¸ºé¦–é¡µé“¾æ¥ï¼ˆéœ€è¦è¿‡æ»¤æ‰ï¼‰"""
        if not url or url == "#":
            return True
        
        # æå–è·¯å¾„éƒ¨åˆ†
        parsed = urllib.parse.urlparse(url)
        path = parsed.path.strip('/')
        query = parsed.query
        
        # å¦‚æœæ²¡æœ‰è·¯å¾„æˆ–æŸ¥è¯¢å‚æ•°ï¼Œå¯èƒ½æ˜¯é¦–é¡µ
        if not path and not query:
            return True
        
        # å¦‚æœåªæœ‰æ ¹è·¯å¾„ä¸”æ²¡æœ‰æœç´¢å‚æ•°
        homepage_patterns = [
            r'^/?$',  # ç©ºè·¯å¾„æˆ–åªæœ‰æ–œæ 
            r'^/?index\.(html?|php)$',  # index é¡µé¢
            r'^/?home$',  # home é¡µé¢
        ]
        
        for pattern in homepage_patterns:
            if re.match(pattern, path):
                return True
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯çƒ­æ¦œèšåˆç½‘ç«™
        hotlist_domains = [
            "tophub.today",
            "remenla.com",
            "shenmehuole.com",
            "imshuai.com",
            "v2hot.com"
        ]
        
        if any(domain in url for domain in hotlist_domains):
            return True
        
        return False

    def _generate_search_url(self, platform, keyword):
        """æ ¹æ®å¹³å°å’Œå…³é”®è¯ç”ŸæˆçœŸå®çš„æœç´¢é“¾æ¥"""
        kw_encoded = urllib.parse.quote(keyword)
        p = platform.lower()
        
        if "bç«™" in p or "bilibili" in p:
            return f"https://search.bilibili.com/all?keyword={kw_encoded}"
        elif "å°çº¢ä¹¦" in p:
            return f"https://www.xiaohongshu.com/search_result?keyword={kw_encoded}"
        elif "çŸ¥ä¹" in p:
            return f"https://www.zhihu.com/search?type=content&q={kw_encoded}"
        elif "æŠ–éŸ³" in p:
            return f"https://www.douyin.com/search/{kw_encoded}"
        elif "å¾®åš" in p:
            return f"https://s.weibo.com/weibo?q={kw_encoded}"
        else:
            return f"https://www.baidu.com/s?wd={kw_encoded}"

    def _must_include_people(self, text_content, user_profile):
        """åˆ¤æ–­æ˜¯å¦å¿…é¡»åŒ…å«äººç‰©ï¼ˆåªæ£€æµ‹éå¸¸ç¡®å®šçš„åœºæ™¯ï¼‰"""
        text_lower = text_content.lower()
        profile_lower = str(user_profile).lower()
        
        # åªä¿ç•™ç»å¯¹ç¡®å®šéœ€è¦äººç‰©çš„å…³é”®è¯
        must_people_keywords = [
            'è‡ªæ‹', 'ootd', 'ç©¿æ­', 'å¦†å®¹', 'åŒ–å¦†', 
            'å‘å‹', 'å¥èº«', 'ç‘œä¼½', 'èˆè¹ˆ', 'è·³èˆ',
            'è¯•è‰²', 'ä¸Šèº«', 'æ˜¾ç˜¦', 'æ˜¾é«˜',
            'æ¼”å”±ä¼š', 'æ‰“å¡ç…§'  # æ¼”å”±ä¼šé€šå¸¸æ˜¯æƒ³æ‹è‡ªå·±
        ]
        
        for keyword in must_people_keywords:
            if keyword in text_lower or keyword in profile_lower:
                return True
        
        return False
    
    def _must_include_pets(self, text_content, user_profile):
        """åˆ¤æ–­æ˜¯å¦å¿…é¡»åŒ…å«å® ç‰©ï¼ˆåªæ£€æµ‹éå¸¸ç¡®å®šçš„åœºæ™¯ï¼‰"""
        text_lower = text_content.lower()
        profile_lower = str(user_profile).lower()
        
        # åªä¿ç•™ç»å¯¹ç¡®å®šæ˜¯å® ç‰©å†…å®¹çš„å…³é”®è¯
        must_pet_keywords = [
            'æˆ‘çš„çŒ«', 'æˆ‘çš„ç‹—', 'æˆ‘å®¶çŒ«', 'æˆ‘å®¶ç‹—',
            'é“²å±å®˜', 'æ¯›å­©å­', 'é›ç‹—', 'æ’¸çŒ«',
            'çŒ«å’ªæ—¥å¸¸', 'ç‹—ç‹—æ—¥å¸¸'
        ]
        
        for keyword in must_pet_keywords:
            if keyword in text_lower or keyword in profile_lower:
                return True
        
        return False
    
    def _get_focus_subject(self, text_content, user_profile):
        """åˆ†æå†…å®¹ä¸»é¢˜ï¼Œç¡®å®šå›¾ç‰‡èšç„¦å¯¹è±¡"""
        text_lower = text_content.lower()
        profile_lower = str(user_profile).lower()
        combined = text_lower + ' ' + profile_lower
        
        # åˆ†æä¸»é¢˜
        if any(kw in combined for kw in ['ç¾é£Ÿ', 'é¤å…', 'ç«é”…', 'çƒ§çƒ¤', 'å’–å•¡', 'å¥¶èŒ¶', 'ç”œå“', 'æ–™ç†', 'èœ']):
            return "ç¾é£Ÿ", "é£Ÿç‰©æœ¬èº«ï¼ˆç‰¹å†™ã€æ‘†ç›˜ã€è´¨æ„Ÿï¼‰"
        
        elif any(kw in combined for kw in ['æ—…è¡Œ', 'æ—…æ¸¸', 'æ™¯ç‚¹', 'é£æ™¯', 'å»ºç­‘', 'å¯ºåº™', 'å…¬å›­', 'æµ·è¾¹', 'å±±']):
            return "æ—…è¡Œ", "æ™¯è‰²å’Œå»ºç­‘ï¼ˆé£å…‰ã€æ°›å›´ã€ç‰¹è‰²ï¼‰"
        
        elif any(kw in combined for kw in ['æ•°ç ', 'æ‰‹æœº', 'ç”µè„‘', 'è€³æœº', 'ç›¸æœº', 'iphone', 'ipad', 'é”®ç›˜', 'é¼ æ ‡']):
            return "æ•°ç äº§å“", "äº§å“ç»†èŠ‚ï¼ˆå¤–è§‚ã€è®¾è®¡ã€åŠŸèƒ½å±•ç¤ºï¼‰"
        
        elif any(kw in combined for kw in ['å¥½ç‰©', 'ç‰©å“', 'æ–‡å…·', 'å®¶å±…', 'ç”¨å“', 'å·¥å…·']):
            return "å¥½ç‰©æ¨è", "äº§å“å®ç‰©ï¼ˆæ‘†æ‹ã€ä½¿ç”¨åœºæ™¯ï¼‰"
        
        elif any(kw in combined for kw in ['å’–å•¡åº—', 'ä¹¦åº—', 'å•†åœº', 'åº—é“º', 'ç©ºé—´', 'ç¯å¢ƒ']):
            return "ç©ºé—´ç¯å¢ƒ", "åº—é“ºç¯å¢ƒå’Œæ°›å›´ï¼ˆè£…ä¿®ã€å¸ƒå±€ã€ç»†èŠ‚ï¼‰"
        
        elif any(kw in combined for kw in ['ä¹¦', 'ç”µå½±', 'éŸ³ä¹', 'æ¸¸æˆ', 'å‰§']):
            return "æ–‡å¨±æ¨è", "ç›¸å…³è§†è§‰å…ƒç´ ï¼ˆå°é¢ã€æµ·æŠ¥ã€åœºæ™¯ï¼‰"
        
        else:
            return "ç”Ÿæ´»åˆ†äº«", "ç”Ÿæ´»åœºæ™¯å’Œç»†èŠ‚"
    
    def _extract_all_captions_from_text(self, text_content, num_images):
        """
        ä¸€æ¬¡æ€§æå–æ‰€æœ‰å›¾ç‰‡çš„captionï¼Œç¡®ä¿å®ƒä»¬ä¸åŒï¼ˆåˆæ¬¡ç”Ÿæˆæ—¶ä½¿ç”¨ï¼Œä¸åˆ†æå›¾ç‰‡ï¼‰
        
        Args:
            text_content: æ–‡æ¡ˆå†…å®¹
            num_images: å›¾ç‰‡æ•°é‡
            
        Returns:
            captionåˆ—è¡¨ï¼Œæ¯ä¸ªéƒ½æ˜¯dict: {{"zh": "ä¸­æ–‡caption", "en": "English caption"}}
        """
        if num_images == 1:
            # å•å¼ å›¾ç‰‡ï¼Œç›´æ¥è°ƒç”¨å•ä¸ªæå–å‡½æ•°
            caption = self._extract_caption_from_text(text_content, image_index=0)
            return [caption]
        
        try:
            # ä½¿ç”¨AIä¸€æ¬¡æ€§æå–æ‰€æœ‰å›¾ç‰‡çš„captionï¼Œç¡®ä¿å®ƒä»¬ä¸åŒï¼ŒåŒæ—¶ç”Ÿæˆä¸­è‹±æ–‡
            prompt = f"""Extract {num_images} DIFFERENT, SPECIFIC product/item names from this Xiaohongshu post text as image captions. For each image, provide BOTH Chinese and English captions.

Post text:
{text_content[:1000]}

Requirements:
1. **PRIORITY: Specific Product/Item Name**: Extract CONCRETE products, foods, drinks, or items mentioned in the post
   - âœ… Good: "æ‹¿é“å’–å•¡" / "latte coffee", "æŠ¹èŒ¶é¥®å“" / "matcha drink", "çº¢è‰²å£çº¢" / "red lipstick"
   - âŒ Bad: "å’–å•¡åº—" / "coffee shop" (location, too vague), "ä¸Šæµ·ç”œå“åº—" / "Shanghai dessert shop" (location, too vague)
2. **DIVERSITY IS CRITICAL**: Each caption must be DIFFERENT from others
   - If post mentions multiple items â†’ extract different items for each image
   - If post mentions one main item â†’ extract different aspects/details
3. **Chinese Caption**: Must be natural, idiomatic Chinese (NOT machine translation). Use appropriate Chinese terms.
4. **English Caption**: 2-6 English words, for CLIP model evaluation
5. **Concrete over Abstract**: Prefer specific items over general categories or locations
6. **Image-focused**: What would be the main subject in each image? Extract that specific item.

Output format: For each image, provide one line with format: "Image N: [Chinese] | [English]"

Example outputs for 2 images:
- Post about "æŠ¹èŒ¶èŒ‰è‰æ‹¿é“" â†’ 
  Image 1: æŠ¹èŒ¶æ‹¿é“ | matcha latte
  Image 2: èŒ‰è‰èŒ¶ | jasmine tea
  
- Post about "æ‹¿é“å’–å•¡å’Œè›‹ç³•" â†’ 
  Image 1: æ‹¿é“å’–å•¡ | latte coffee
  Image 2: è›‹ç³• | cake dessert

- Post about "çº¢è‰²å£çº¢è¯•è‰²" â†’ 
  Image 1: çº¢è‰²å£çº¢ | red lipstick
  Image 2: è¯•è‰²æ•ˆæœ | lip swatch

Output (one line per image):"""

            resp = requests.post(
                f"{self.chat_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.chat_api_key}"},
                json={
                    "model": self.chat_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.5,  # ç¨é«˜æ¸©åº¦ä»¥å¢åŠ å¤šæ ·æ€§
                    "max_tokens": 150
                },
                timeout=30
            )
            
            if resp.status_code == 200:
                response_text = resp.json()["choices"][0]["message"]["content"].strip()
                # è§£æå¤šè¡Œè¾“å‡ºï¼Œæå–ä¸­è‹±æ–‡
                lines = response_text.split('\n')
                captions = []
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è§£ææ ¼å¼: "Image N: [Chinese] | [English]" æˆ– "1. [Chinese] | [English]"
                    if '|' in line:
                        # åˆ†ç¦»ä¸­è‹±æ–‡
                        parts = line.split('|', 1)
                        if len(parts) == 2:
                            # ç§»é™¤å‰ç¼€ï¼ˆImage N: æˆ– 1. ç­‰ï¼‰
                            chinese_part = parts[0].strip()
                            english_part = parts[1].strip()
                            
                            # æ¸…ç†ä¸­æ–‡éƒ¨åˆ†
                            if ':' in chinese_part:
                                chinese_part = chinese_part.split(':', 1)[1].strip()
                            if chinese_part and chinese_part[0].isdigit():
                                chinese_part = chinese_part.split('.', 1)[1].strip() if '.' in chinese_part else chinese_part[1:].strip()
                            
                            # æ¸…ç†è‹±æ–‡éƒ¨åˆ†
                            english_part = english_part.strip()
                            # é™åˆ¶è‹±æ–‡é•¿åº¦ï¼ˆ2-6ä¸ªå•è¯ï¼‰
                            words = english_part.split()
                            if len(words) > 6:
                                english_part = " ".join(words[:6])
                            
                            if chinese_part and english_part and len(words) >= 2:
                                captions.append({"zh": chinese_part, "en": english_part})
                
                # ç¡®ä¿æ•°é‡æ­£ç¡®
                if len(captions) == num_images:
                    return captions
                elif len(captions) > num_images:
                    return captions[:num_images]
                else:
                    # å¦‚æœæå–çš„æ•°é‡ä¸å¤Ÿï¼Œç”¨ç¬¬ä¸€ä¸ªcaptionçš„å˜ä½“å¡«å……
                    while len(captions) < num_images:
                        if captions:
                            # åŸºäºå·²æœ‰captionç”Ÿæˆå˜ä½“
                            base_caption = captions[0]
                            variant_zh = f"{base_caption['zh']}ç»†èŠ‚" if len(captions) == 1 else f"{base_caption['zh']}{len(captions)+1}"
                            variant_en = f"{base_caption['en']} detail" if len(captions) == 1 else f"{base_caption['en']} {len(captions)+1}"
                            captions.append({"zh": variant_zh, "en": variant_en})
                        else:
                            fallback_zh = self._simple_keyword_extraction(text_content) if text_content else "ç”Ÿæ´»åˆ†äº«"
                            fallback_en = self._translate_to_english(fallback_zh)
                            captions.append({"zh": fallback_zh, "en": fallback_en})
                    return captions[:num_images]
            else:
                # é™çº§æ–¹æ¡ˆï¼šé€ä¸ªæå–
                print(f"   âš ï¸  æ‰¹é‡æå–captionå¤±è´¥ï¼Œä½¿ç”¨é€ä¸ªæå–æ–¹æ¡ˆ")
                return [self._extract_caption_from_text(text_content, i) for i in range(num_images)]
                
        except Exception as e:
            print(f"   âš ï¸  æ‰¹é‡æå–captionå¤±è´¥: {e}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            # é™çº§æ–¹æ¡ˆï¼šé€ä¸ªæå–
            return [self._extract_caption_from_text(text_content, i) for i in range(num_images)]
    
    def _extract_caption_from_text(self, text_content, image_index=0):
        """
        åŸºäºæ–‡æ¡ˆæå–å•ä¸ªcaptionï¼ˆåˆæ¬¡ç”Ÿæˆæ—¶ä½¿ç”¨ï¼Œä¸åˆ†æå›¾ç‰‡ï¼‰
        
        Args:
            text_content: æ–‡æ¡ˆå†…å®¹
            image_index: å›¾ç‰‡ç´¢å¼•ï¼ˆ0-basedï¼‰
            
        Returns:
            dict: {{"zh": "ä¸­æ–‡caption", "en": "English caption"}}
        """
        try:
            # ä½¿ç”¨AIä»æ–‡æ¡ˆä¸­æå–å…³é”®è¯ï¼ŒåŒæ—¶ç”Ÿæˆä¸­è‹±æ–‡
            prompt = f"""Extract a concise, SPECIFIC product/item name from this Xiaohongshu post text as image caption. Provide BOTH Chinese and English versions.

Post text:
{text_content[:800]}

Requirements:
1. **PRIORITY: Specific Product/Item Name**: Extract the CONCRETE product, food, drink, or item mentioned in the post
   - âœ… Good: "æ‹¿é“å’–å•¡" / "latte coffee", "æŠ¹èŒ¶é¥®å“" / "matcha drink", "çº¢è‰²å£çº¢" / "red lipstick"
   - âŒ Bad: "å’–å•¡åº—" / "coffee shop" (location, too vague), "ä¸Šæµ·ç”œå“åº—" / "Shanghai dessert shop" (location, too vague)
2. **Chinese Caption**: Must be natural, idiomatic Chinese (NOT machine translation). Use appropriate Chinese terms.
3. **English Caption**: 2-6 English words, for CLIP model evaluation
4. **Concrete over Abstract**: Prefer specific items over general categories or locations
5. **Image-focused**: What would be the main subject in the image? Extract that specific item.

Output format: "[Chinese] | [English]"

Example outputs:
- Post about "æŠ¹èŒ¶èŒ‰è‰æ‹¿é“" â†’ æŠ¹èŒ¶æ‹¿é“ | matcha latte
- Post about "æ‹¿é“å’–å•¡" â†’ æ‹¿é“å’–å•¡ | latte coffee
- Post about "çº¢è‰²å£çº¢" â†’ çº¢è‰²å£çº¢ | red lipstick
- Post about "ç™½è‰²è¿åŠ¨é‹" â†’ ç™½è‰²è¿åŠ¨é‹ | white sneakers

Output:"""

            resp = requests.post(
                f"{self.chat_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.chat_api_key}"},
                json={
                    "model": self.chat_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.3,
                    "max_tokens": 80
                },
                timeout=30
            )
            
            if resp.status_code == 200:
                response_text = resp.json()["choices"][0]["message"]["content"].strip()
                # è§£æä¸­è‹±æ–‡
                if '|' in response_text:
                    parts = response_text.split('|', 1)
                    chinese = parts[0].strip()
                    english = parts[1].strip()
                    # é™åˆ¶è‹±æ–‡é•¿åº¦ï¼ˆ2-6ä¸ªå•è¯ï¼‰
                    words = english.split()
                    if len(words) > 6:
                        english = " ".join(words[:6])
                    if chinese and english and len(words) >= 2:
                        return {"zh": chinese, "en": english}
                
                # å¦‚æœè§£æå¤±è´¥ï¼Œé™çº§æ–¹æ¡ˆ
                fallback_zh = self._simple_keyword_extraction(text_content) if text_content else "ç”Ÿæ´»åˆ†äº«"
                fallback_en = self._translate_to_english(fallback_zh)
                return {"zh": fallback_zh, "en": fallback_en}
            else:
                fallback_zh = self._simple_keyword_extraction(text_content) if text_content else "ç”Ÿæ´»åˆ†äº«"
                fallback_en = self._translate_to_english(fallback_zh)
                return {"zh": fallback_zh, "en": fallback_en}
                
        except Exception as e:
            print(f"   âš ï¸  æ–‡æ¡ˆæå–captionå¤±è´¥: {e}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            fallback_zh = self._simple_keyword_extraction(text_content) if text_content else "ç”Ÿæ´»åˆ†äº«"
            fallback_en = self._translate_to_english(fallback_zh)
            return {"zh": fallback_zh, "en": fallback_en}
    
    def _extract_keywords_for_caption(self, image_path, text_content=None):
        """
        åŸºäºå›¾ç‰‡å†…å®¹æå–æ ¸å¿ƒä¸»ä½“ä½œä¸ºcaptionï¼ˆä½¿ç”¨vision modelåˆ†æå›¾ç‰‡ï¼‰
        
        Args:
            image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            text_content: æ–‡æ¡ˆå†…å®¹ï¼ˆå¯é€‰ï¼Œä½œä¸ºä¸Šä¸‹æ–‡å‚è€ƒï¼‰
            
        Returns:
            dict: {{"zh": "ä¸­æ–‡caption", "en": "English caption"}}
        """
        if not image_path or not os.path.exists(image_path):
            print(f"   âš ï¸  å›¾ç‰‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            fallback_zh = self._simple_keyword_extraction(text_content) if text_content else "ç”Ÿæ´»åˆ†äº«"
            fallback_en = self._translate_to_english(fallback_zh)
            return {"zh": fallback_zh, "en": fallback_en}
        
        try:
            # è¯»å–å›¾ç‰‡å¹¶ç¼–ç ä¸ºbase64
            with open(image_path, 'rb') as f:
                img_data = f.read()
            
            # åˆ¤æ–­å›¾ç‰‡æ ¼å¼
            ext = Path(image_path).suffix.lower()
            mime_type = {
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.webp': 'image/webp',
                '.gif': 'image/gif'
            }.get(ext, 'image/jpeg')
            
            img_base64 = base64.b64encode(img_data).decode('utf-8')
            
            # æ„å»ºvision promptï¼ˆä¸“æ³¨äºæå–æ ¸å¿ƒä¸»ä½“ï¼ŒåŒæ—¶ç”Ÿæˆä¸­è‹±æ–‡ï¼‰
            context_hint = ""
            if text_content:
                context_hint = f"""
**æ–‡æ¡ˆä¸Šä¸‹æ–‡ï¼ˆä»…ä¾›å‚è€ƒï¼‰**ï¼š
{text_content[:200]}
"""
            
            vision_prompt = f"""Analyze this image and extract a concise core subject as caption. Provide BOTH Chinese and English versions.

{context_hint}

**Requirements**:
1. **Core Subject**: Extract only the most prominent visual element in the image
2. **Chinese Caption**: Must be natural, idiomatic Chinese (NOT machine translation). Use appropriate Chinese terms.
3. **English Caption**: 2-6 English words, for CLIP model evaluation
4. **Specific but not verbose**:
   - âœ… Good: "æ‹¿é“å’–å•¡" / "latte coffee", "æŠ¹èŒ¶é¥®å“" / "matcha drink"
   - âŒ Bad: "é£Ÿç‰©" / "food" (too vague)
5. **Based on image content**: Must accurately reflect what's actually shown in the image, don't guess from text

**Output format**: "[Chinese] | [English]"

Example outputs:
- Image shows coffee and cup â†’ æ‹¿é“å’–å•¡ | latte coffee
- Image shows shop interior â†’ å’–å•¡åº— | coffee shop
- Image shows product display â†’ äº§å“å±•ç¤º | product review
- Image shows scenery/building â†’ æ—…è¡Œé£æ™¯ | travel vlog

Output:"""

            # ä½¿ç”¨vision modelï¼ˆå¦‚æœæœ‰é…ç½®ï¼‰æˆ–chat model
            vision_model = os.getenv("VISION_MODEL", "claude-sonnet-4-5-20250929")
            search_api_key = os.getenv("SEARCH_API_KEY", self.chat_api_key)
            search_base_url = os.getenv("SEARCH_BASE_URL", self.chat_base_url)
            
            response = requests.post(
                f"{search_base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {search_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": vision_model,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": vision_prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:{mime_type};base64,{img_base64}"
                                    }
                                }
                            ]
                        }
                    ],
                    "temperature": 0.3,
                    "max_tokens": 50
                },
                timeout=30
            )
            
            if response.status_code == 200:
                response_text = response.json()["choices"][0]["message"]["content"].strip()
                # è§£æä¸­è‹±æ–‡
                if '|' in response_text:
                    parts = response_text.split('|', 1)
                    chinese = parts[0].strip()
                    english = parts[1].strip()
                    # æ¸…ç†å¯èƒ½çš„æ ¼å¼
                    chinese = chinese.replace("å…³é”®è¯ï¼š", "").replace("å…³é”®è¯:", "").strip()
                    english = english.replace("Keywords:", "").replace("Keywordsï¼š", "").strip()
                    # é™åˆ¶è‹±æ–‡é•¿åº¦ï¼ˆ2-6ä¸ªå•è¯ï¼‰
                    words = english.split()
                    if len(words) > 6:
                        english = " ".join(words[:6])
                    if chinese and english and len(words) >= 2:
                        return {"zh": chinese, "en": english}
                
                # å¦‚æœè§£æå¤±è´¥ï¼Œé™çº§æ–¹æ¡ˆ
                fallback_zh = self._simple_keyword_extraction(text_content) if text_content else "ç”Ÿæ´»åˆ†äº«"
                fallback_en = self._translate_to_english(fallback_zh)
                return {"zh": fallback_zh, "en": fallback_en}
            else:
                print(f"   âš ï¸  Vision APIé”™è¯¯ {response.status_code}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                fallback_zh = self._simple_keyword_extraction(text_content) if text_content else "ç”Ÿæ´»åˆ†äº«"
                fallback_en = self._translate_to_english(fallback_zh)
                return {"zh": fallback_zh, "en": fallback_en}
                
        except Exception as e:
            print(f"   âš ï¸  å›¾ç‰‡åˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            fallback_zh = self._simple_keyword_extraction(text_content) if text_content else "ç”Ÿæ´»åˆ†äº«"
            fallback_en = self._translate_to_english(fallback_zh)
            return {"zh": fallback_zh, "en": fallback_en}
    
    def _simple_keyword_extraction(self, text_content):
        """ç®€å•çš„å…³é”®è¯æå–ï¼ˆé™çº§æ–¹æ¡ˆï¼Œè¿”å›ä¸­æ–‡ï¼‰"""
        # æå–å¸¸è§å…³é”®è¯
        keywords_map = {
            "å’–å•¡": ["å’–å•¡", "æ‹¿é“", "ç¾å¼", "å¡å¸ƒ", "æ‰‹å†²"],
            "ç¾é£Ÿ": ["ç¾é£Ÿ", "é¤å…", "ç«é”…", "çƒ§çƒ¤", "æ–™ç†"],
            "æ—…è¡Œ": ["æ—…è¡Œ", "æ—…æ¸¸", "æ™¯ç‚¹", "é£æ™¯"],
            "å¥½ç‰©": ["å¥½ç‰©", "åˆ†äº«", "æ¨è", "æµ‹è¯„"],
            "ç”Ÿæ´»": ["æ—¥å¸¸", "ç”Ÿæ´»", "å‘¨æœ«", "æ—¥è®°"]
        }
        
        text_lower = text_content.lower() if text_content else ""
        for category, keywords in keywords_map.items():
            for kw in keywords:
                if kw in text_lower:
                    return category
        
        return "ç”Ÿæ´»åˆ†äº«"
    
    def _translate_to_english(self, chinese_text):
        """å°†ä¸­æ–‡å…³é”®è¯ç¿»è¯‘æˆè‹±æ–‡ï¼ˆç”¨äºCLIPè¯„ä¼°ï¼‰"""
        translation_map = {
            "å’–å•¡": "coffee drink",
            "ç¾é£Ÿ": "food",
            "æ—…è¡Œ": "travel",
            "å¥½ç‰©": "product review",
            "ç”Ÿæ´»åˆ†äº«": "lifestyle",
            "ç”Ÿæ´»": "lifestyle",
            "æ¢åº—": "cafe visit",
            "å’–å•¡åº—": "coffee shop",
            "æŠ¹èŒ¶é¥®å“": "matcha drink",
            "æ—…è¡Œvlog": "travel vlog"
        }
        
        # ç›´æ¥åŒ¹é…
        if chinese_text in translation_map:
            return translation_map[chinese_text]
        
        # å°è¯•éƒ¨åˆ†åŒ¹é…
        for key, value in translation_map.items():
            if key in chinese_text:
                return value
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç®€å•ç¿»è¯‘
        try:
            # ä½¿ç”¨AIå¿«é€Ÿç¿»è¯‘
            prompt = f"Translate this Chinese keyword to English (2-6 words, for image caption): {chinese_text}\n\nOutput ONLY the English translation, no other text:"
            
            resp = requests.post(
                f"{self.chat_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.chat_api_key}"},
                json={
                    "model": self.chat_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1,
                    "max_tokens": 20
                },
                timeout=10
            )
            
            if resp.status_code == 200:
                translation = resp.json()["choices"][0]["message"]["content"].strip()
                # æ¸…ç†æ ¼å¼
                translation = translation.replace("Translation:", "").replace("ç¿»è¯‘:", "").strip()
                return translation
        except:
            pass
        
        # æœ€ç»ˆé™çº§ï¼šè¿”å›é€šç”¨è‹±æ–‡
        return "lifestyle"
    
    def _extract_generalized_theme(self, text_content):
        """
        ä»å®Œæ•´æ–‡æ¡ˆä¸­æå–æåº¦æ³›åŒ–çš„ä¸»é¢˜
        åªä¿ç•™æœ€å®½æ³›çš„ä¸»é¢˜ç±»åˆ«ï¼Œç§»é™¤æ‰€æœ‰å…·ä½“ç»†èŠ‚ï¼Œè®©å›¾ç‰‡ç”Ÿæˆæ—¶å®Œå…¨çœ‹ä¸åˆ°å…·ä½“è§†è§‰æè¿°
        """
        import re
        
        # ç¬¬ä¸€æ­¥ï¼šç§»é™¤æ‰€æœ‰å…·ä½“çš„è§†è§‰æè¿°è¯æ±‡
        text = text_content
        
        # ç§»é™¤æ‰€æœ‰é¢œè‰²æè¿°
        text = re.sub(r'\b(ç™½è‰²|é»‘è‰²|çº¢è‰²|è“è‰²|ç»¿è‰²|é»„è‰²|ç²‰è‰²|ç´«è‰²|æ£•è‰²|ç°è‰²|ç±³è‰²|å’–å•¡è‰²|æ·±è‰²|æµ…è‰²|äº®è‰²|æš—è‰²|é‡‘è‰²|é“¶è‰²|é€æ˜|åŠé€æ˜)\s*(çš„|)?', '', text)
        
        # ç§»é™¤æ‰€æœ‰å…·ä½“ç‰©å“ç»†èŠ‚
        detail_patterns = [
            r'å¿ƒå½¢[çš„]?æ‹‰èŠ±', r'æ‹‰èŠ±å›¾æ¡ˆ', r'å›¾æ¡ˆ', r'èŠ±çº¹', r'çº¹ç†',
            r'é™¶ç“·æ¯', r'ç»ç’ƒæ¯', r'é©¬å…‹æ¯', r'å’–å•¡æ¯', r'æ¯å­',
            r'æœ¨æ¡Œ', r'æœ¨åˆ¶', r'æœ¨è´¨', r'æ¡Œå­', r'æ¡Œé¢',
            r'ç‰¹å†™', r'ç»†èŠ‚', r'è´¨æ„Ÿ', r'æè´¨',
            r'æ‘†ç›˜', r'è£…é¥°', r'ç‚¹ç¼€', r'æ­é…',
            r'æ‹¿é“', r'ç¾å¼', r'å¡å¸ƒå¥‡è¯º', r'æ‰‹å†²',  # å…·ä½“å’–å•¡ç±»å‹
            r'iPhone', r'MacBook', r'AirPods',  # å…·ä½“äº§å“å‹å·
            r'æœé˜³', r'ä¸‰é‡Œå±¯', r'å¤ªå¤é‡Œ',  # å…·ä½“åœ°ç‚¹
            r'äººå‡\d+', r'\d+å…ƒ', r'ä»·æ ¼', r'æŠ˜æ‰£',  # ä»·æ ¼ä¿¡æ¯
        ]
        for pattern in detail_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ç¬¬äºŒæ­¥ï¼šåªæå–æœ€å®½æ³›çš„ä¸»é¢˜ç±»åˆ«ï¼ˆæåº¦ç®€åŒ–ï¼‰
        # åªä¿ç•™æœ€æ ¸å¿ƒçš„ä¸»é¢˜è¯ï¼Œä¸åŒ…å«ä»»ä½•å…·ä½“æè¿°
        theme_keywords = []
        text_lower = text.lower()
        
        # ä½¿ç”¨æ›´å®½æ³›çš„åˆ†ç±»
        if any(kw in text_lower for kw in ['å’–å•¡', 'å’–å•¡åº—', 'é¥®å“', 'å¥¶èŒ¶', 'èŒ¶']):
            theme_keywords.append('é¥®å“åœºæ™¯')  # ä¸è¯´æ˜¯å’–å•¡ï¼Œåªè¯´æ˜¯é¥®å“
        elif any(kw in text_lower for kw in ['ç¾é£Ÿ', 'é¤å…', 'ç«é”…', 'çƒ§çƒ¤', 'é£Ÿç‰©', 'èœ', 'æ–™ç†']):
            theme_keywords.append('é¤é¥®åœºæ™¯')  # ä¸è¯´æ˜¯ç«é”…ï¼Œåªè¯´æ˜¯é¤é¥®
        elif any(kw in text_lower for kw in ['æ—…è¡Œ', 'æ—…æ¸¸', 'æ™¯ç‚¹', 'é£æ™¯', 'å»ºç­‘']):
            theme_keywords.append('æ—…è¡Œåœºæ™¯')
        elif any(kw in text_lower for kw in ['æ•°ç ', 'æ‰‹æœº', 'ç”µè„‘', 'ç”µå­', 'ç§‘æŠ€']):
            theme_keywords.append('ç§‘æŠ€äº§å“')  # ä¸è¯´æ˜¯iPhoneï¼Œåªè¯´æ˜¯ç§‘æŠ€äº§å“
        elif any(kw in text_lower for kw in ['å¥½ç‰©', 'ç‰©å“', 'äº§å“', 'å•†å“']):
            theme_keywords.append('äº§å“å±•ç¤º')
        elif any(kw in text_lower for kw in ['ä¹¦', 'ç”µå½±', 'éŸ³ä¹', 'æ¸¸æˆ', 'å‰§', 'å¨±ä¹']):
            theme_keywords.append('æ–‡å¨±å†…å®¹')
        else:
            theme_keywords.append('ç”Ÿæ´»åœºæ™¯')  # æœ€å®½æ³›çš„åˆ†ç±»
        
        # ç¬¬ä¸‰æ­¥ï¼šç§»é™¤æƒ…æ„Ÿè¯ï¼Œåªä¿ç•™æœ€å®½æ³›çš„ä¸»é¢˜
        # ä¸åŒ…å«æƒ…æ„Ÿæè¿°ï¼Œè®©å›¾ç‰‡ç”Ÿæˆæ›´éšæœº
        
        # æ„å»ºæåº¦æ³›åŒ–çš„æè¿°ï¼ˆåªåŒ…å«æœ€å®½æ³›çš„ä¸»é¢˜ï¼Œä¸åŒ…å«ä»»ä½•å…·ä½“ä¿¡æ¯ï¼‰
        generalized = theme_keywords[0]  # åªä¿ç•™ä¸»é¢˜ï¼Œä¸åŒ…å«æƒ…æ„Ÿ
        
        return generalized

    def generate_images(self, user_profile, text_content, output_dir, tags=None, image_captions=None):
        """ç”Ÿæˆé…å›¾ï¼Œæ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦äººç‰©ï¼Œä¿æŒç³»åˆ—å›¾ç‰‡ä¸€è‡´æ€§
        
        Args:
            user_profile: ç”¨æˆ·ç”»åƒ
            text_content: æ–‡æ¡ˆå†…å®¹ï¼ˆç”¨äºåˆ¤æ–­å›¾ç‰‡æ•°é‡å’Œç”Ÿæˆç›¸å…³å›¾ç‰‡ï¼‰
            output_dir: è¾“å‡ºç›®å½•
            tags: ç¬”è®°æ ‡ç­¾åˆ—è¡¨
            image_captions: é¢„æå–çš„captionåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”Ÿæˆå›¾ç‰‡æ—¶ä¸å‚è€ƒï¼Œä»…ç”¨äºåç»­HTMLç”Ÿæˆï¼‰
        """
        word_count = len(text_content.strip())
        num_images = 1 if word_count <= 300 else (2 if word_count <= 800 else 3)
        print(f"æ–‡æ¡ˆå­—æ•°: {word_count}, éœ€è¦ç”Ÿæˆ {num_images} å¼ å›¾ç‰‡")

        # åˆ†ææ–‡æ¡ˆå†…å®¹ï¼Œç¡®å®šå›¾ç‰‡ä¸»é¢˜å’Œèšç„¦å¯¹è±¡
        theme, focus_desc = self._get_focus_subject(text_content, user_profile)
        must_people = self._must_include_people(text_content, user_profile)
        must_pets = self._must_include_pets(text_content, user_profile)
        
        # æ„å»ºèšç„¦æŒ‡å¼•
        if must_people:
            focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
âš ï¸ **å¿…é¡»åŒ…å«äººç‰©**ï¼ˆå¦‚ç©¿æ­å±•ç¤ºã€å¥èº«ã€è‡ªæ‹ç­‰åœºæ™¯ï¼‰
èšç„¦å¯¹è±¡ï¼šäººç‰©å±•ç¤ºï¼Œé…åˆ {focus_desc.lower()}
"""
            consistency_mode = "äººç‰©"
        elif must_pets:
            focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
âš ï¸ **å¿…é¡»åŒ…å«å® ç‰©**
èšç„¦å¯¹è±¡ï¼šå® ç‰©ç‰¹å†™å’Œæ—¥å¸¸
"""
            consistency_mode = "å® ç‰©"
        else:
            focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
èšç„¦å¯¹è±¡ï¼š{focus_desc}
"""
            consistency_mode = "é€šç”¨"
        
        style_reference = f"\nå‚è€ƒé£æ ¼ï¼šå°çº¢ä¹¦çœŸå®å›¾ç‰‡é£æ ¼ï¼Œè‡ªç„¶ã€ç”Ÿæ´»åŒ–ã€æœ‰è´¨æ„Ÿã€‚"
        
        # å¦‚æœä¼ å…¥äº†é¢„æå–çš„captionsï¼Œè¯´æ˜æ˜¯åˆæ¬¡ç”Ÿæˆï¼Œåªä½¿ç”¨tagsï¼Œä¸ä¼ å…¥æ–‡æ¡ˆ
        # å¦‚æœæ²¡æœ‰ä¼ å…¥ï¼Œåˆ™ä½¿ç”¨æ–‡æ¡ˆå†…å®¹ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        use_pre_extracted_captions = image_captions is not None and len(image_captions) >= num_images
        
        if use_pre_extracted_captions:
            print(f"ğŸ¨ ç”Ÿæˆç­–ç•¥ï¼šåˆæ¬¡ç”Ÿæˆ - ä»…ä½¿ç”¨tagsï¼Œä¸ä¼ å…¥æ–‡æ¡ˆä¸Šä¸‹æ–‡")
        else:
            print(f"ğŸ¨ å›¾ç‰‡ä¸»é¢˜ï¼š{theme}")
            print(f"ğŸ“¸ ç”Ÿæˆç­–ç•¥ï¼šåŸºäºæ–‡æ¡ˆå†…å®¹ç”Ÿæˆç›¸å…³å›¾ç‰‡")

        image_paths = []
        max_retries = 5  # å¢åŠ é‡è¯•æ¬¡æ•°ï¼ˆä»3å¢åŠ åˆ°5ï¼‰
        reference_image = None  # ç”¨äºä¿æŒä¸€è‡´æ€§

        for i in range(num_images):
            image_path = os.path.join(output_dir, f"personalized_post_{i+1}.png")
            success = False
            
            for attempt in range(1, max_retries + 1):
                if attempt > 1:
                    print(f"   ğŸ”„ ç¬¬ {i+1} å¼ å›¾ç‰‡é‡è¯• {attempt}/{max_retries}...")
                
                # ğŸ¯ æ„å»ºæç¤ºè¯ï¼šåˆæ¬¡ç”Ÿæˆæ—¶åªä½¿ç”¨tagsï¼Œä¸ä¼ å…¥æ–‡æ¡ˆ
                if use_pre_extracted_captions:
                    # åˆæ¬¡ç”Ÿæˆï¼šåªä½¿ç”¨tagsï¼Œä¸ä¼ å…¥æ–‡æ¡ˆä¸Šä¸‹æ–‡
                    tags_section = ""
                    if tags:
                        tags_str = ", ".join(tags)
                        tags_section = f"""
**ç¬”è®°æ ‡ç­¾ï¼ˆå”¯ä¸€å‚è€ƒï¼‰**ï¼š
{tags_str}

âš ï¸ **é‡è¦**ï¼šåªæ ¹æ®æ ‡ç­¾ç”Ÿæˆå›¾ç‰‡ï¼Œä¸è¦å‚è€ƒå…¶ä»–å†…å®¹ã€‚æ ‡ç­¾æ˜¯ï¼š{tags_str}
"""
                    
                    if i == 0:
                        prompt_text = f"""
ç”Ÿæˆä¸€å¼ å°çº¢ä¹¦é£æ ¼çš„å›¾ç‰‡ï¼ˆç¬¬ {i+1}/{num_images} å¼ ï¼‰ï¼š

{tags_section}

{style_reference}

ğŸ’¡ **ç”ŸæˆæŒ‡å¯¼**ï¼š
- æ ¹æ®æ ‡ç­¾ä¸»é¢˜ç”Ÿæˆç›¸å…³çš„å›¾ç‰‡
- ä¿æŒå°çº¢ä¹¦çœŸå®å›¾ç‰‡é£æ ¼ï¼Œè‡ªç„¶ã€ç”Ÿæ´»åŒ–ã€æœ‰è´¨æ„Ÿ

è¦æ±‚ï¼š
- çœŸå®è‡ªç„¶çš„ç¤¾äº¤åª’ä½“ç…§ç‰‡é£æ ¼
- æœ€å¥½ä¸è¦å‡ºç°æ–‡å­—ï¼Œåƒä¸‡ä¸è¦å‡ºç°ä¸­æ–‡æˆ–ä»€ä¹ˆå¥‡æ€ªçš„å­—ç¬¦ï¼å®åœ¨éœ€è¦æ–‡å­—è¯·ä½¿ç”¨è‹±æ–‡ï¼Œå¹¶ä¸”å°½é‡æ”¾åœ¨å›¾ç‰‡çš„è§’è½ï¼Œä¸è¦å½±å“å›¾ç‰‡ä¸»ä½“
- ç¬¦åˆå°çº¢ä¹¦å›¾ç‰‡ç¾å­¦
- ç”Ÿæ´»åŒ–ã€æœ‰è´¨æ„Ÿã€æœ‰æ•…äº‹æ„Ÿ
- æ„å›¾æ¸…æ™°ï¼Œä¸»ä½“çªå‡º
- è‰²å½©è‡ªç„¶ï¼Œå…‰çº¿æŸ”å’Œ
- å¦‚æœç”»é¢ä¸­å‡ºç°äº†äººç‰©æˆ–åŠ¨ç‰©ï¼Œè¯·ç¡®ä¿å…¶ç‰¹å¾æ˜ç¡®ï¼Œä¾¿äºåç»­å›¾ç‰‡ä¿æŒä¸€è‡´
- åç»­å›¾ç‰‡å…³é”®äººç‰©æˆ–åŠ¨ç‰©ä¿æŒä¸€è‡´ï¼Œä½†æ˜¯çªå‡ºçš„äº‹ç‰©éœ€è¦æ”¹å˜ï¼Œé¿å…å›¾ç‰‡é—´è¿‡åº¦ç›¸ä¼¼
"""
                    else:
                        prompt_text = f"""
ç”Ÿæˆä¸€å¼ å°çº¢ä¹¦é£æ ¼çš„å›¾ç‰‡ï¼ˆç¬¬ {i+1}/{num_images} å¼ ï¼Œå‚è€ƒå‰ä¸€å¼ å›¾ç‰‡ï¼‰ï¼š

{tags_section}

{style_reference}

âš ï¸ **é£æ ¼ä¸€è‡´æ€§è¦æ±‚**
- ä¸å‚è€ƒå›¾ç‰‡ä¿æŒæ•´ä½“é£æ ¼å’Œæ„å›¾æ€è·¯ä¸€è‡´
- å¦‚æœå‚è€ƒå›¾ç‰‡ä¸­æœ‰äººç‰©æˆ–åŠ¨ç‰©ï¼Œä¿æŒå…¶å¤–è²Œç‰¹å¾å®Œå…¨ä¸€è‡´
- å¯ä»¥æ”¹å˜ï¼šåœºæ™¯ã€è§’åº¦ã€åŠ¨ä½œã€èƒŒæ™¯ã€æ‹æ‘„è·ç¦»
- çªå‡ºçš„äº‹ç‰©éœ€è¦æ”¹å˜ï¼Œé¿å…å›¾ç‰‡é—´è¿‡åº¦ç›¸ä¼¼
- ä¿æŒè‰²è°ƒã€æ°›å›´å’Œæ‹æ‘„é£æ ¼çš„è¿è´¯æ€§

è¦æ±‚ï¼š
- çœŸå®è‡ªç„¶çš„ç¤¾äº¤åª’ä½“ç…§ç‰‡é£æ ¼
- **ç»å¯¹ç¦æ­¢åœ¨å›¾ç‰‡ä¸Šç”Ÿæˆä»»ä½•æ–‡å­—ï¼Œç‰¹åˆ«æ˜¯ä¸­æ–‡æ–‡å­—ï¼** å›¾ç‰‡å¿…é¡»æ˜¯çº¯è§†è§‰å†…å®¹ï¼Œä¸èƒ½æœ‰ä»»ä½•æ–‡å­—å åŠ 
- ç”Ÿæ´»åŒ–ã€æœ‰è´¨æ„Ÿã€æœ‰æ•…äº‹æ„Ÿ
"""
                else:
                    # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨æ–‡æ¡ˆå†…å®¹ï¼ˆå¦‚æœæ²¡æœ‰é¢„æå–çš„captionsï¼‰
                    if i == 0:
                        prompt_text = f"""
ç”Ÿæˆä¸€å¼ å°çº¢ä¹¦é£æ ¼çš„å›¾ç‰‡ï¼ˆç¬¬ {i+1}/{num_images} å¼ ï¼‰ï¼š

**æ–‡æ¡ˆå†…å®¹ï¼ˆå‚è€ƒï¼‰**ï¼š
{text_content[:500]}

{style_reference}

{focus_guidance}

ğŸ’¡ **ç”ŸæˆæŒ‡å¯¼**ï¼š
- æ ¹æ®æ–‡æ¡ˆå†…å®¹ç”Ÿæˆç›¸å…³çš„å›¾ç‰‡
- å›¾ç‰‡åº”è¯¥å±•ç¤ºæ–‡æ¡ˆä¸­æåˆ°çš„åœºæ™¯ã€ç‰©å“æˆ–ä¸»é¢˜
- ä¿æŒå°çº¢ä¹¦çœŸå®å›¾ç‰‡é£æ ¼ï¼Œè‡ªç„¶ã€ç”Ÿæ´»åŒ–ã€æœ‰è´¨æ„Ÿ

è¦æ±‚ï¼š
- çœŸå®è‡ªç„¶çš„ç¤¾äº¤åª’ä½“ç…§ç‰‡é£æ ¼
- æœ€å¥½ä¸è¦å‡ºç°æ–‡å­—ï¼Œåƒä¸‡ä¸è¦å‡ºç°ä¸­æ–‡æˆ–ä»€ä¹ˆå¥‡æ€ªçš„å­—ç¬¦ï¼å®åœ¨éœ€è¦æ–‡å­—è¯·ä½¿ç”¨è‹±æ–‡ï¼Œå¹¶ä¸”å°½é‡æ”¾åœ¨å›¾ç‰‡çš„è§’è½ï¼Œä¸è¦å½±å“å›¾ç‰‡ä¸»ä½“
- ç¬¦åˆå°çº¢ä¹¦å›¾ç‰‡ç¾å­¦
- ç”Ÿæ´»åŒ–ã€æœ‰è´¨æ„Ÿã€æœ‰æ•…äº‹æ„Ÿ
- æ„å›¾æ¸…æ™°ï¼Œä¸»ä½“çªå‡º
- è‰²å½©è‡ªç„¶ï¼Œå…‰çº¿æŸ”å’Œ
- å¦‚æœç”»é¢ä¸­å‡ºç°äº†äººç‰©æˆ–åŠ¨ç‰©ï¼Œè¯·ç¡®ä¿å…¶ç‰¹å¾æ˜ç¡®ï¼Œä¾¿äºåç»­å›¾ç‰‡ä¿æŒä¸€è‡´
- åç»­å›¾ç‰‡å…³é”®äººç‰©æˆ–åŠ¨ç‰©ä¿æŒä¸€è‡´ï¼Œä½†æ˜¯çªå‡ºçš„äº‹ç‰©éœ€è¦æ”¹å˜ï¼Œé¿å…å›¾ç‰‡é—´è¿‡åº¦ç›¸ä¼¼
"""
                    else:
                        prompt_text = f"""
ç”Ÿæˆä¸€å¼ å°çº¢ä¹¦é£æ ¼çš„å›¾ç‰‡ï¼ˆç¬¬ {i+1}/{num_images} å¼ ï¼Œå‚è€ƒå‰ä¸€å¼ å›¾ç‰‡ï¼‰ï¼š

**æ–‡æ¡ˆå†…å®¹ï¼ˆå‚è€ƒï¼‰**ï¼š
{text_content[:500]}

{style_reference}

âš ï¸ **é£æ ¼ä¸€è‡´æ€§è¦æ±‚**
- ä¸å‚è€ƒå›¾ç‰‡ä¿æŒæ•´ä½“é£æ ¼å’Œæ„å›¾æ€è·¯ä¸€è‡´
- å¦‚æœå‚è€ƒå›¾ç‰‡ä¸­æœ‰äººç‰©æˆ–åŠ¨ç‰©ï¼Œä¿æŒå…¶å¤–è²Œç‰¹å¾å®Œå…¨ä¸€è‡´
- å¯ä»¥æ”¹å˜ï¼šåœºæ™¯ã€è§’åº¦ã€åŠ¨ä½œã€èƒŒæ™¯ã€æ‹æ‘„è·ç¦»
- çªå‡ºçš„äº‹ç‰©éœ€è¦æ”¹å˜ï¼Œé¿å…å›¾ç‰‡é—´è¿‡åº¦ç›¸ä¼¼
- ä¿æŒè‰²è°ƒã€æ°›å›´å’Œæ‹æ‘„é£æ ¼çš„è¿è´¯æ€§

{focus_guidance}

ğŸ’¡ **ç”ŸæˆæŒ‡å¯¼**ï¼š
- æ ¹æ®æ–‡æ¡ˆå†…å®¹ç”Ÿæˆç›¸å…³çš„å›¾ç‰‡
- å›¾ç‰‡åº”è¯¥å±•ç¤ºæ–‡æ¡ˆä¸­æåˆ°çš„åœºæ™¯ã€ç‰©å“æˆ–ä¸»é¢˜
- ä¿æŒä¸å‰ä¸€å¼ å›¾ç‰‡çš„é£æ ¼ä¸€è‡´æ€§

è¦æ±‚ï¼š
- çœŸå®è‡ªç„¶çš„ç¤¾äº¤åª’ä½“ç…§ç‰‡é£æ ¼
- **ç»å¯¹ç¦æ­¢åœ¨å›¾ç‰‡ä¸Šç”Ÿæˆä»»ä½•æ–‡å­—ï¼Œç‰¹åˆ«æ˜¯ä¸­æ–‡æ–‡å­—ï¼** å›¾ç‰‡å¿…é¡»æ˜¯çº¯è§†è§‰å†…å®¹ï¼Œä¸èƒ½æœ‰ä»»ä½•æ–‡å­—å åŠ 
- ç”Ÿæ´»åŒ–ã€æœ‰è´¨æ„Ÿã€æœ‰æ•…äº‹æ„Ÿ
"""
                
                try:
                    # æ„å»ºè¯·æ±‚å†…å®¹
                    parts = [{"text": prompt_text}]
                    
                    # å¦‚æœæœ‰å‚è€ƒå›¾ç‰‡ä¸”ä¸æ˜¯ç¬¬ä¸€å¼ ï¼Œæ·»åŠ å‚è€ƒ
                    if reference_image and i > 0:
                        parts.insert(0, {
                            "inlineData": {
                                "mimeType": "image/png",
                                "data": reference_image
                            }
                        })
                    
                    resp = requests.post(
                        f"{self.generate_base_url}/models/gemini-2.5-flash-image-preview:generateContent",
                        headers={"Content-Type": "application/json", "Authorization": f"Bearer {self.generate_api_key}"},
                        json={
                            "contents": [{"parts": parts}],
                            "generationConfig": {"temperature": 0.9}  # æ­£å¸¸temperatureï¼Œç”Ÿæˆè‡ªç„¶ç›¸å…³çš„å›¾ç‰‡
                        },
                        timeout=60
                    )
                    
                    if resp.status_code == 200:
                        result_parts = resp.json().get("candidates", [{}])[0].get("content", {}).get("parts", [])
                        for part in result_parts:
                            if "inlineData" in part:
                                # ä¿å­˜å›¾ç‰‡
                                image_data = base64.b64decode(part["inlineData"]["data"])
                                with open(image_path, "wb") as f:
                                    f.write(image_data)
                                
                                image_paths.append(image_path)
                                
                                # ğŸ¯ å¦‚æœå·²æœ‰é¢„æå–çš„captionï¼Œä¸é‡æ–°æå–
                                # å¦‚æœæ²¡æœ‰ï¼Œåˆ™åˆ†æå›¾ç‰‡æå–captionï¼ˆé™çº§æ–¹æ¡ˆï¼‰
                                if not use_pre_extracted_captions:
                                    caption_keyword = self._extract_keywords_for_caption(image_path, text_content=text_content)
                                    if image_captions is None:
                                        image_captions = []
                                    image_captions.append(caption_keyword)
                                    if isinstance(caption_keyword, dict):
                                        print(f"   ğŸ“ åˆ†æå›¾ç‰‡æå–caption: {caption_keyword.get('zh', '')} ({caption_keyword.get('en', '')})")
                                    else:
                                        print(f"   ğŸ“ åˆ†æå›¾ç‰‡æå–caption: {caption_keyword}")
                                
                                # ä¿å­˜ä¸ºä¸‹ä¸€å¼ çš„å‚è€ƒ
                                reference_image = part["inlineData"]["data"]
                                
                                success = True
                                if i == 0:
                                    print(f"âœ… ç¬¬ {i+1} å¼ å›¾ç‰‡å·²ä¿å­˜ï¼ˆåŸºå‡†å›¾ç‰‡ï¼‰")
                                else:
                                    if consistency_mode in ["äººç‰©", "å® ç‰©"]:
                                        print(f"âœ… ç¬¬ {i+1} å¼ å›¾ç‰‡å·²ä¿å­˜ï¼ˆä¿æŒ{consistency_mode}ä¸€è‡´ï¼‰")
                                    else:
                                        print(f"âœ… ç¬¬ {i+1} å¼ å›¾ç‰‡å·²ä¿å­˜ï¼ˆä¿æŒé£æ ¼ä¸€è‡´ï¼‰")
                                break
                    
                    if success:
                        break
                    else:
                        # è¯·æ±‚æˆåŠŸä½†æ²¡æœ‰ç”Ÿæˆå›¾ç‰‡ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
                        if resp.status_code != 200:
                            print(f"   âš ï¸  APIè¿”å›é”™è¯¯: {resp.status_code}")
                            if resp.text:
                                error_msg = resp.text[:200]
                                print(f"       é”™è¯¯ä¿¡æ¯: {error_msg}")
                    
                except Exception as e:
                    print(f"   âš ï¸  å›¾ç‰‡ç”Ÿæˆå¼‚å¸¸ (å°è¯• {attempt}/{max_retries}): {str(e)[:100]}")
                    import time
                    if attempt < max_retries:
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
            
            if not success:
                print(f"âŒ ç¬¬ {i+1} å¼ å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰")
                if i == 0:
                    print("âš ï¸ åŸºå‡†å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                    print("ğŸ’¡ æç¤ºï¼šå°†åœ¨Reflectioné˜¶æ®µå°è¯•é‡æ–°ç”Ÿæˆå›¾ç‰‡")
                    break  # ä¸å†å°è¯•åç»­å›¾ç‰‡
        
        # å¦‚æœä½¿ç”¨é¢„æå–çš„captionsï¼Œç›´æ¥è¿”å›å®ƒä»¬
        # å¦‚æœæ²¡æœ‰é¢„æå–çš„captionsï¼Œç¡®ä¿image_pathså’Œimage_captionsé•¿åº¦ä¸€è‡´
        if not use_pre_extracted_captions:
            if image_captions is None:
                image_captions = []
            while len(image_captions) < len(image_paths):
                image_captions.append(None)
        
        # è¿”å›å›¾ç‰‡è·¯å¾„ï¼ˆcaptionså·²åœ¨å¤–éƒ¨é¢„æå–æˆ–å·²ç”Ÿæˆï¼‰
        return image_paths

    def _render_markdown(self, text):
        """
        ç®€å•çš„ Markdown è½¬ HTML è½¬æ¢å™¨
        è§£å†³ç½‘é¡µä¸Šæ˜¾ç¤º **text** ç¬¦å·çš„é—®é¢˜ï¼Œå°†å…¶è½¬æ¢ä¸ºåŠ ç²—æ ·å¼
        """
        # 1. å¤„ç†åŠ ç²— **text** -> <strong>text</strong>
        # ä½¿ç”¨ ? éè´ªå©ªåŒ¹é…ï¼Œé˜²æ­¢è·¨è¡ŒåŒ¹é…è¿‡å¤š
        text = re.sub(r'\*\*(.*?)\*\*', r'<strong style="color: #333; font-weight: 700;">\1</strong>', text)
        
        # 2. å¤„ç†ç®€å•çš„æ ‡é¢˜ (ä»¥é˜²ä¸‡ä¸€ AI è¾“å‡ºäº†æ ‡é¢˜)
        # ### Title -> <h3>Title</h3>
        text = re.sub(r'^###\s+(.*?)$', r'<h3>\1</h3>', text, flags=re.MULTILINE)
        
        # 3. å¤„ç†åˆ—è¡¨é¡¹ (ç®€å•çš„ - item)
        text = re.sub(r'^\-\s+(.*?)$', r'â€¢ \1', text, flags=re.MULTILINE)

        return text

    def generate_html_post(self, text_content, image_paths, links, tags, output_path="post.html", image_captions=None):
        """ç”ŸæˆHTMLå¸–å­ï¼ŒåŒ…å« Markdown æ¸²æŸ“é€»è¾‘
        
        Args:
            text_content: æ–‡æœ¬å†…å®¹
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            links: é“¾æ¥åˆ—è¡¨
            tags: æ ‡ç­¾åˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„
            image_captions: å›¾ç‰‡captionåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        """
        
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        raw_paragraphs = [p for p in text_content.split('\n\n') if p.strip()]
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ¸²æŸ“æ¯ä¸€æ®µçš„ Markdown ---
        processed_paragraphs = []
        for p in raw_paragraphs:
            rendered_p = self._render_markdown(p)
            processed_paragraphs.append(rendered_p)
            
        paragraphs = processed_paragraphs
        # ------------------------------------

        insertions = []
        
        # æ’å…¥å›¾ç‰‡ï¼ˆå¸¦captionï¼‰
        for i, img_path in enumerate(image_paths):
            caption = image_captions[i] if image_captions and i < len(image_captions) else None
            insertions.append({"type": "image", "content": img_path, "index": i, "caption": caption})
        # æ’å…¥é“¾æ¥
        for link in links:
            insertions.append({"type": "link", "content": link})
            
        html_parts = []
        num_paras = len(paragraphs)
        
        if num_paras == 0:
            html_parts.append(text_content)
        else:
            num_inserts = len(insertions)
            if num_inserts > 0:
                step = max(1, num_paras // (num_inserts + 1))
                current_insert_idx = 0
                for i, para in enumerate(paragraphs):
                    # åˆ¤æ–­æ˜¯å¦å·²ç»æ˜¯æ ‡é¢˜æ ‡ç­¾ï¼ˆå¦‚æœæ˜¯h3å°±ä¸åŠ pæ ‡ç­¾äº†ï¼‰
                    if para.startswith('<h3'):
                         html_parts.append(para)
                    else:
                         html_parts.append(f"<p>{para}</p>")
                         
                    if current_insert_idx < num_inserts:
                        if (i + 1) % step == 0 or i == num_paras - 1:
                            item = insertions[current_insert_idx]
                            if item["type"] == "image":
                                caption = item.get("caption")
                                html_parts.append(self._create_image_tag(item["content"], item["index"], caption=caption))
                            elif item["type"] == "link":
                                html_parts.append(self._create_link_tag(item["content"]))
                            current_insert_idx += 1
                            if i == num_paras - 1:
                                while current_insert_idx < num_inserts:
                                    item = insertions[current_insert_idx]
                                    if item["type"] == "image":
                                        caption = item.get("caption")
                                        html_parts.append(self._create_image_tag(item["content"], item["index"], caption=caption))
                                    elif item["type"] == "link":
                                        html_parts.append(self._create_link_tag(item["content"]))
                                    current_insert_idx += 1
            else:
                for para in paragraphs:
                    if para.startswith('<h3'):
                        html_parts.append(para)
                    else:
                        html_parts.append(f"<p>{para}</p>")

        html_content = "\n".join(html_parts)
                
        html_template = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>ä¸ªæ€§åŒ–ç¤¾äº¤åª’ä½“å¸–å­</title>
            <style>
                body {{ font-family: 'Helvetica Neue', Helvetica, 'Microsoft YaHei', sans-serif; line-height: 1.75; max-width: 800px; margin: 0 auto; padding: 15px; background: #f5f5f5; color: #333; }}
                .post-container {{ background: white; border-radius: 12px; padding: 25px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); margin: 10px 0; }}
                
                /* æ­£æ–‡æ ·å¼ä¼˜åŒ– */
                .post-content {{ font-size: 17px; color: #2c3e50; letter-spacing: 0.02em; }}
                .post-content p {{ margin: 1em 0; text-align: justify; }}
                .post-content strong {{ color: #000; font-weight: 700; background: linear-gradient(to bottom, transparent 60%, #fffbe6 60%); }} /* æ¨¡æ‹Ÿé«˜äº®ç¬”æ•ˆæœ */
                .post-content h3 {{ font-size: 1.2em; margin-top: 1.5em; margin-bottom: 0.5em; color: #1a1a1a; }}
                
                .post-image {{ margin: 20px -25px; width: calc(100% + 50px); text-align: center; }}
                .post-image img {{ width: 100%; display: block; }}
                .image-caption {{ color: #999; font-size: 13px; margin-top: 8px; font-style: italic; padding: 0 25px; }}

                /* æ ‡ç­¾æ ·å¼ */
                .post-tags {{ 
                    margin: 15px 0 20px 0; 
                    display: flex; 
                    flex-wrap: wrap; 
                    gap: 8px;
                }}
                .tag {{ 
                    display: inline-block; 
                    padding: 5px 12px; 
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white; 
                    font-size: 13px; 
                    border-radius: 15px; 
                    font-weight: 500;
                    box-shadow: 0 2px 4px rgba(102, 126, 234, 0.2);
                }}

                /* é“¾æ¥å¡ç‰‡æ ·å¼ (ä¿æŒä¸å˜) */
                .link-card {{
                    display: flex;
                    align-items: center;
                    background: #fcfcfc;
                    border: 1px solid #eee;
                    padding: 12px 15px;
                    margin: 25px 0;
                    text-decoration: none;
                    border-radius: 8px;
                    transition: all 0.2s;
                }}
                .link-card:hover {{ transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0,0,0,0.08); }}
                .link-card.platform-bç«™ {{ border-left: 5px solid #23ade5; }}
                .link-card.platform-å°çº¢ä¹¦ {{ border-left: 5px solid #ff2442; }}
                .link-card.platform-çŸ¥ä¹ {{ border-left: 5px solid #0084ff; }}
                .link-card.platform-æŠ–éŸ³ {{ border-left: 5px solid #1c1e21; }}
                .link-card.platform-å¾®åš {{ border-left: 5px solid #ea5d5c; }}
                
                .link-info {{ flex: 1; }}
                .link-platform-tag {{ 
                    font-size: 12px; font-weight: bold; margin-bottom: 4px; display: inline-block; padding: 2px 6px; border-radius: 4px; color: white;
                }}
                .tag-bç«™ {{ background: #23ade5; }}
                .tag-å°çº¢ä¹¦ {{ background: #ff2442; }}
                .tag-çŸ¥ä¹ {{ background: #0084ff; }}
                .tag-æŠ–éŸ³ {{ background: #000; }}
                .tag-å¾®åš {{ background: #ea5d5c; }}
                
                .link-title {{ font-weight: bold; color: #333; font-size: 15px; margin-top: 2px; }}
                .link-action {{ color: #999; font-size: 12px; margin-top: 4px; }}
                .link-icon {{ font-size: 24px; margin-right: 15px; }}

                .post-header {{ display: flex; align-items: center; margin-bottom: 20px; padding-bottom: 15px; border-bottom: 1px solid #f0f0f0; }} 
                .avatar {{ width: 48px; height: 48px; border-radius: 50%; background: linear-gradient(120deg, #a1c4fd 0%, #c2e9fb 100%); margin-right: 15px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }} 
                .user-info h3 {{ margin: 0; font-size: 18px; font-weight: 600; }} 
                .post-time {{ color: #999; font-size: 13px; margin-top: 4px; }} 

                @media (max-width: 600px) {{ 
                    .post-container {{ padding: 15px; }} 
                    .post-image {{ margin: 15px -15px; width: calc(100% + 30px); }}
                }}
            </style>
        </head>
        <body>
            <div class="post-container">
                <div class="post-header">
                    <div class="avatar"></div>
                    <div class="user-info">
                        <h3>AI åˆ›æ„åŠ©ç†</h3>
                        <div class="post-time">{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}</div>
                    </div>
                </div>
                <div class="post-tags">
                    {"".join([f'<span class="tag"># {tag}</span>' for tag in tags])}
                </div>
                <div class="post-content">{html_content}</div>
                <div style="margin-top:30px; border-top:1px solid #eee; padding-top:15px; color:#ccc; font-size:12px; text-align:center;">
                    Generated by AI â€¢ {len(image_paths)} Images
                </div>
            </div>
        </body>
        </html>
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_template)
        return output_path

    def _create_image_tag(self, image_path, index, caption=None):
        """
        åˆ›å»ºå›¾ç‰‡æ ‡ç­¾ï¼Œä½¿ç”¨æå–çš„caption
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            index: å›¾ç‰‡ç´¢å¼•ï¼ˆ0-basedï¼‰
            caption: å›¾ç‰‡captionï¼Œå¯ä»¥æ˜¯dict {{"zh": "ä¸­æ–‡", "en": "English"}} æˆ–å­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        if caption:
            # å¤„ç†æ–°çš„dictæ ¼å¼æˆ–æ—§çš„å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            if isinstance(caption, dict):
                caption_zh = caption.get("zh", "")
                caption_en = caption.get("en", "")
                # æ˜¾ç¤ºä¸­æ–‡captionï¼Œç”¨dataå±æ€§å­˜å‚¨è‹±æ–‡captionï¼ˆç”¨äºCLIPè®¡ç®—ï¼‰
                caption_text = f"å›¾{index + 1}: {caption_zh}"
                if caption_en:
                    return f'<div class="post-image"><img src="{os.path.basename(image_path)}"><div class="image-caption" data-caption-en="{caption_en}">{caption_text}</div></div>'
                else:
                    return f'<div class="post-image"><img src="{os.path.basename(image_path)}"><div class="image-caption">{caption_text}</div></div>'
            else:
                # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå‡è®¾æ˜¯è‹±æ–‡
                caption_text = f"å›¾{index + 1}: {caption}"
                return f'<div class="post-image"><img src="{os.path.basename(image_path)}"><div class="image-caption" data-caption-en="{caption}">{caption_text}</div></div>'
        else:
            # é™çº§ï¼šä½¿ç”¨é»˜è®¤æ ¼å¼
            caption_text = f"å›¾ {index + 1}"
            return f'<div class="post-image"><img src="{os.path.basename(image_path)}"><div class="image-caption">{caption_text}</div></div>'

    def _create_link_tag(self, link_data):
        title = link_data.get('title', 'ç›¸å…³å†…å®¹')
        platform = link_data.get('platform', 'ç½‘é¡µ').strip()
        url = link_data.get('url', '#')
        
        css_class = "platform-other"
        tag_class = "tag-other"
        icon = "ğŸ”—"
        
        p = platform.lower()
        if "bç«™" in p or "bilibili" in p:
            css_class = "platform-bç«™"
            tag_class = "tag-bç«™"
            icon = "ğŸ“º"
        elif "å°çº¢ä¹¦" in p:
            css_class = "platform-å°çº¢ä¹¦"
            tag_class = "tag-å°çº¢ä¹¦"
            icon = "ğŸ“•"
        elif "çŸ¥ä¹" in p:
            css_class = "platform-çŸ¥ä¹"
            tag_class = "tag-çŸ¥ä¹"
            icon = "â“"
        elif "æŠ–éŸ³" in p:
            css_class = "platform-æŠ–éŸ³"
            tag_class = "tag-æŠ–éŸ³"
            icon = "ğŸµ"
        elif "å¾®åš" in p:
            css_class = "platform-å¾®åš"
            tag_class = "tag-å¾®åš"
            icon = "ğŸ‘ï¸"
            
        return f'''
        <a href="{url}" class="link-card {css_class}" target="_blank">
            <div class="link-icon">{icon}</div>
            <div class="link-info">
                <span class="link-platform-tag {tag_class}">{platform}</span>
                <div class="link-title">{title}</div>
                <div class="link-action">ç‚¹å‡»å» {platform} æŸ¥çœ‹è¯¦æƒ… &gt;</div>
            </div>
        </a>
        '''

    def _extract_captions_from_html(self, html_path):
        """
        ä»HTMLä¸­æå–ç°æœ‰çš„captions
        
        Returns:
            captionåˆ—è¡¨ï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        try:
            from bs4 import BeautifulSoup
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            content_div = soup.find('div', class_='post-content')
            
            if not content_div:
                return []
            
            captions = []
            image_divs = content_div.find_all('div', class_='post-image')
            
            for img_div in image_divs:
                caption_div = img_div.find('div', class_='image-caption')
                if caption_div:
                    # ä¼˜å…ˆä»data-caption-enè¯»å–è‹±æ–‡ï¼Œä»æ–‡æœ¬è¯»å–ä¸­æ–‡
                    caption_en = caption_div.get('data-caption-en', '')
                    caption_text = caption_div.get_text(strip=True)
                    # ç§»é™¤"å›¾X:"å‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
                    if caption_text.startswith("å›¾") and ":" in caption_text:
                        caption_text = caption_text.split(":", 1)[1].strip()
                    
                    # è¿”å›dictæ ¼å¼ï¼ˆåŒ…å«ä¸­è‹±æ–‡ï¼‰
                    if caption_en:
                        captions.append({"zh": caption_text, "en": caption_en})
                    else:
                        # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰è‹±æ–‡ï¼Œå‡è®¾æ–‡æœ¬æ˜¯è‹±æ–‡
                        captions.append({"zh": caption_text, "en": caption_text})
                else:
                    captions.append(None)
            
            return captions
        except Exception as e:
            print(f"   âš ï¸  æå–captionå¤±è´¥: {e}")
            return []
    
    def _regenerate_images_for_reflection(self, html_path, text_content, user_profile, output_dir, iteration, tags=None):
        """
        åœ¨Reflectionè¿‡ç¨‹ä¸­é‡æ–°ç”Ÿæˆå›¾ç‰‡
        
        Args:
            html_path: å½“å‰HTMLæ–‡ä»¶è·¯å¾„
            text_content: å½“å‰æ–‡æœ¬å†…å®¹
            user_profile: ç”¨æˆ·ç”»åƒ
            output_dir: è¾“å‡ºç›®å½•
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            tags: æ ‡ç­¾åˆ—è¡¨ï¼ˆç”¨äºç¬¬ä¸€æ¬¡åæ€æ—¶å‚è€ƒï¼‰
            
        Returns:
            æ–°å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            print(f"\nğŸ¨ é‡æ–°ç”Ÿæˆå›¾ç‰‡ï¼ˆç¬¬{iteration+1}æ¬¡Reflection - å›¾ç‰‡é‡å»ºç­–ç•¥ï¼‰...")
            
            # 1. è§£æå½“å‰HTMLè·å–å›¾ç‰‡ä¿¡æ¯
            from bs4 import BeautifulSoup
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            content_div = soup.find('div', class_='post-content')
            
            if not content_div:
                print("   âš ï¸  æ— æ³•æ‰¾åˆ°post-content div")
                return None
            
            # è·å–å½“å‰å›¾ç‰‡æ•°é‡
            current_images = content_div.find_all('div', class_='post-image')
            num_images = len(current_images)
            
            if num_images == 0:
                print("   âš ï¸  æ²¡æœ‰å›¾ç‰‡éœ€è¦é‡æ–°ç”Ÿæˆ")
                return None
            
            print(f"   ğŸ“¸ éœ€è¦é‡æ–°ç”Ÿæˆ {num_images} å¼ å›¾ç‰‡")
            print(f"   ğŸ’¡ ç­–ç•¥ï¼šåŸºäºå½“å‰æ–‡æœ¬å†…å®¹ï¼Œé‡æ–°ç”Ÿæˆä¸æ–‡å­—æ›´åŒ¹é…çš„å›¾ç‰‡")
            
            # 2. è¯»å–æ—§å›¾ç‰‡ä½œä¸ºå‚è€ƒï¼ˆé£æ ¼å’Œæ„å›¾ï¼‰
            old_image_paths = []
            for img_div in current_images:
                img_tag = img_div.find('img')
                if img_tag and img_tag.get('src'):
                    old_img_path = os.path.join(os.path.dirname(html_path), img_tag['src'])
                    if os.path.exists(old_img_path):
                        old_image_paths.append(old_img_path)
            
            # 3. ç”Ÿæˆæ–°å›¾ç‰‡ï¼ˆä½¿ç”¨æ”¹è¿›çš„promptï¼Œå¼ºè°ƒä¸æ–‡æœ¬çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼‰
            theme, focus_desc = self._get_focus_subject(text_content, user_profile)
            must_people = self._must_include_people(text_content, user_profile)
            must_pets = self._must_include_pets(text_content, user_profile)
            
            # æ„å»ºå¢å¼ºçš„èšç„¦æŒ‡å¼•ï¼ˆå¼ºè°ƒæ–‡æœ¬åŒ¹é…ï¼‰
            if must_people:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
âš ï¸ **å¿…é¡»åŒ…å«äººç‰©**ï¼ˆå¦‚ç©¿æ­å±•ç¤ºã€å¥èº«ã€è‡ªæ‹ç­‰åœºæ™¯ï¼‰
èšç„¦å¯¹è±¡ï¼šäººç‰©å±•ç¤ºï¼Œé…åˆ {focus_desc.lower()}
ğŸ¯ **å…³é”®è¦æ±‚**ï¼šå›¾ç‰‡å†…å®¹å¿…é¡»ä¸æ–‡æ¡ˆå¼ºç›¸å…³ï¼Œå±•ç¤ºæ–‡æ¡ˆä¸­æåˆ°çš„å…·ä½“åœºæ™¯ã€åŠ¨ä½œã€ç‰©å“
"""
            elif must_pets:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
âš ï¸ **å¿…é¡»åŒ…å«å® ç‰©**
èšç„¦å¯¹è±¡ï¼šå® ç‰©ç‰¹å†™å’Œæ—¥å¸¸
ğŸ¯ **å…³é”®è¦æ±‚**ï¼šå›¾ç‰‡å†…å®¹å¿…é¡»ä¸æ–‡æ¡ˆå¼ºç›¸å…³ï¼Œå±•ç¤ºæ–‡æ¡ˆä¸­æåˆ°çš„å…·ä½“åœºæ™¯ã€åŠ¨ä½œ
"""
            else:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
èšç„¦å¯¹è±¡ï¼š{focus_desc}
ğŸ¯ **å…³é”®è¦æ±‚**ï¼šå›¾ç‰‡å¿…é¡»ç²¾ç¡®å±•ç¤ºæ–‡æ¡ˆä¸­æè¿°çš„å†…å®¹ï¼ˆäº§å“ã€åœºæ™¯ã€ç»†èŠ‚ã€é¢œè‰²ã€æ°›å›´ï¼‰
"""
            
            new_image_paths = []
            reference_image = None
            
            # ç¬¬äºŒè½®åæ€ï¼ˆiteration == 1ï¼‰æ—¶ä¸ä½¿ç”¨æ—§å›¾ç‰‡å‚è€ƒï¼Œåªçœ‹æ–‡æœ¬ä¸Šä¸‹æ–‡
            # è¿™æ ·å¯ä»¥å®Œå…¨åŸºäºæ–‡æœ¬å†…å®¹é‡æ–°ç”Ÿæˆï¼Œä¸å—æ—§å›¾ç‰‡é£æ ¼å½±å“
            if iteration == 1:
                print(f"   ğŸ“ ç¬¬äºŒè½®åæ€ï¼šä¸ä½¿ç”¨æ—§å›¾ç‰‡å‚è€ƒï¼Œå®Œå…¨åŸºäºæ–‡æœ¬ä¸Šä¸‹æ–‡é‡æ–°ç”Ÿæˆ")

            
            for i in range(num_images):
                image_path = os.path.join(output_dir, f"personalized_post_{i+1}_v{iteration+1}.png")
                success = False
                max_retries = 3
                
                # ç¬¬ä¸€æ¬¡åæ€ï¼ˆiteration == 0ï¼‰ï¼šä¸»è¦å‚è€ƒä¸Šä¸‹æ–‡å’Œæ ‡ç­¾
                if iteration == 0:
                    tags_section = ""
                    if tags:
                        tags_str = ", ".join(tags)
                        tags_section = f"""
**Tags (Primary Reference):**
{tags_str}

**Text Context (Secondary Reference):**
{text_content[:500]}
"""
                    else:
                        tags_section = f"""
**Text Context:**
{text_content[:500]}
"""
                    
                    prompt_text = f"""
Generate image {i+1}/{num_images} for a Xiaohongshu post (1st Reflection - regenerate based on context and tags).

{tags_section}

**User Profile:**
{user_profile[:300]}

{focus_guidance}

**CRITICAL Requirements (1st Reflection - regenerate images based on context and tags)**:
1. **Primary focus on tags**: The image should strongly reflect the core themes and keywords from the tags
2. **Text context alignment**: Incorporate key visual elements mentioned in the text context
3. **Natural Xiaohongshu photo style**: Realistic, life-like, social media aesthetic
4. **Clear composition**: Main subject should be prominent and clearly visible
5. **Soft lighting and natural colors**: Maintain authentic, appealing visual quality
6. **ABSOLUTELY NO TEXT OVERLAYS**: Strictly prohibit any text on the image, especially Chinese characters! The image must be pure visual content without any text overlays
7. **If characters/pets appear**: Keep their features consistent across all images

**Style Reference**: Natural Xiaohongshu lifestyle photography with high quality and storytelling

**REMEMBER**: This is the 1st reflection - regenerate images primarily based on tags and context to improve image-text matching.
"""
                # æ„å»ºå¼ºè°ƒæ–‡æœ¬åŒ¹é…çš„promptï¼ˆä½¿ç”¨å®Œæ•´æ–‡æ¡ˆï¼Œç¡®ä¿å›¾æ–‡é«˜åº¦å…³è”ï¼‰
                elif i == 0:
                    prompt_text = f"""
Generate image {i+1}/{num_images} for a Xiaohongshu post with MAXIMUM semantic consistency with the text.

**Text Content (MUST match this exactly - extract ALL visual elements):**
{text_content}

**User Profile:**
{user_profile[:300]}

{focus_guidance}

**CRITICAL Requirements (for optimal CLIP score - this is REFLECTION optimization)**:
1. **SEMANTIC ALIGNMENT IS PARAMOUNT**: The image MUST visually represent EVERY key concept, object, scene, color, and detail mentioned in the text
2. **Extract and visualize ALL keywords from text**:
   - Objects: Extract all concrete nouns (e.g., "latte", "wooden table", "white cup", "heart pattern")
   - Colors: Extract all color descriptions (e.g., "white", "brown", "golden")
   - Actions: Extract all action verbs (e.g., "drinking", "sitting", "holding")
   - Scenes: Extract all scene descriptions (e.g., "coffee shop", "outdoor", "morning light")
   - Details: Extract all specific details (e.g., "heart latte art", "ceramic cup", "wooden texture")
3. **Visualize text descriptions literally**: If text says "heart-shaped latte art", show exactly that. If text says "white ceramic cup", show exactly that.
4. **Natural Xiaohongshu photo style**: Realistic, life-like, social media aesthetic
5. **Clear composition**: Main subject should be prominent and clearly visible
6. **Soft lighting and natural colors**: Maintain authentic, appealing visual quality
7. **ABSOLUTELY NO TEXT OVERLAYS**: Strictly prohibit any text on the image, especially Chinese characters! The image must be pure visual content without any text overlays
8. **If characters/pets appear**: Keep their features consistent across all images

**Style Reference**: Natural Xiaohongshu lifestyle photography with high quality and storytelling

**REMEMBER**: This is a REFLECTION optimization - the goal is to MAXIMIZE image-text semantic matching for higher CLIP score. Extract EVERY visual element from the text and make it visible in the image.
"""
                else:
                    # ç¬¬äºŒè½®åæ€æ—¶ä¸ä½¿ç”¨å‚è€ƒå›¾ç‰‡ï¼Œæ‰€ä»¥ä¸éœ€è¦ä¸€è‡´æ€§è¦æ±‚
                    if iteration == 1:
                        consistency_section = """
**Note**: This is the 2nd reflection - generating completely new images based on text content only, no reference images used.
"""
                    else:
                        consistency_section = """
**Consistency Requirements**:
- Maintain the same style, color tone, and atmosphere as the reference image
- If there are characters/pets in previous images, keep their appearance identical
- Change: scene angle, specific objects/details shown, but maintain overall theme
- Ensure variety while keeping semantic consistency with text
"""
                    
                    prompt_text = f"""
Generate image {i+1}/{num_images} for a Xiaohongshu post (continuing from previous image).

**Text Content (MUST match this exactly - extract ALL visual elements):**
{text_content}

{focus_guidance}

{consistency_section}

**CRITICAL Requirements (for optimal CLIP score - this is REFLECTION optimization)**:
1. **Extract and visualize keywords from text**: Identify ALL concrete nouns, colors, actions, scenes, and details in the text and ensure they appear in the image
2. **Visualize text descriptions literally**: If text mentions specific objects, colors, or details, show them exactly
3. **The image MUST visually represent different aspects of the text content** - extract visual elements that haven't been shown in previous images yet
4. **Maintain semantic alignment**: Every element in the image should correspond to something mentioned in the text
5. **ABSOLUTELY NO TEXT OVERLAYS**: Strictly prohibit any text on the image, especially Chinese characters! The image must be pure visual content without any text overlays

**REMEMBER**: This is a REFLECTION optimization - maximize image-text semantic matching for higher CLIP score.
"""
                
                for attempt in range(1, max_retries + 1):
                    try:
                        parts = [{"text": prompt_text}]
                        
                        # ç¬¬äºŒè½®åæ€ï¼ˆiteration == 1ï¼‰æ—¶ä¸ä½¿ç”¨æ—§å›¾ç‰‡å‚è€ƒï¼Œå®Œå…¨åŸºäºæ–‡æœ¬ä¸Šä¸‹æ–‡
                        # å…¶ä»–è½®æ¬¡å¯ä»¥ä½¿ç”¨å‚è€ƒå›¾ç‰‡ä¿æŒä¸€è‡´æ€§
                        if iteration == 1:
                            # ç¬¬äºŒè½®åæ€ï¼šä¸ä½¿ç”¨ä»»ä½•æ—§å›¾ç‰‡å‚è€ƒï¼Œåªçœ‹æ–‡æœ¬ä¸Šä¸‹æ–‡
                            pass  # ä¸æ·»åŠ ä»»ä½•å‚è€ƒå›¾ç‰‡
                        elif reference_image and i > 0:
                            # åç»­å›¾ç‰‡ï¼šå‚è€ƒå‰ä¸€å¼ æ–°ç”Ÿæˆçš„å›¾ç‰‡ä¿æŒä¸€è‡´æ€§
                            parts.insert(0, {
                                "inlineData": {
                                    "mimeType": "image/png",
                                    "data": reference_image
                                }
                            })
                        elif reference_image and i == 0:
                            # ç¬¬ä¸€å¼ å›¾ï¼šå‚è€ƒæ—§å›¾ç‰‡çš„é£æ ¼ï¼ˆä»…éç¬¬äºŒè½®åæ€æ—¶ï¼‰
                            parts.append({
                                "inlineData": {
                                    "mimeType": "image/png", 
                                    "data": reference_image
                                }
                            })
                            parts.append({"text": "\n(Above is style reference - generate new image matching the text while maintaining similar photographic style)"})
                        
                        resp = requests.post(
                            f"{self.generate_base_url}/models/gemini-2.5-flash-image-preview:generateContent",
                            headers={"Content-Type": "application/json", "Authorization": f"Bearer {self.generate_api_key}"},
                            json={
                                "contents": [{"parts": parts}],
                                "generationConfig": {"temperature": 0.6}  # ç¨ä½çš„temperatureä»¥æé«˜ä¸€è‡´æ€§
                            },
                            timeout=60
                        )
                        
                        if resp.status_code == 200:
                            result_parts = resp.json().get("candidates", [{}])[0].get("content", {}).get("parts", [])
                            for part in result_parts:
                                if "inlineData" in part:
                                    image_data = base64.b64decode(part["inlineData"]["data"])
                                    with open(image_path, "wb") as f:
                                        f.write(image_data)
                                    
                                    new_image_paths.append(image_path)
                                    reference_image = part["inlineData"]["data"]
                                    
                                    print(f"   âœ… ç¬¬ {i+1}/{num_images} å¼ æ–°å›¾ç‰‡å·²ç”Ÿæˆ")
                                    success = True
                                    break
                            
                            if success:
                                break  # æˆåŠŸç”Ÿæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                            else:
                                if attempt < max_retries:
                                    print(f"   âš ï¸  ç¬¬ {i+1} å¼ å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œé‡è¯• {attempt+1}/{max_retries}...")
                        else:
                            print(f"   âš ï¸  APIé”™è¯¯: {resp.status_code}")
                            if attempt < max_retries:
                                print(f"   ğŸ”„ é‡è¯• {attempt+1}/{max_retries}...")
                            
                    except Exception as e:
                        print(f"   âŒ ç¬¬ {i+1} å¼ å›¾ç‰‡ç”Ÿæˆå¼‚å¸¸ (å°è¯• {attempt}/{max_retries}): {str(e)[:100]}")
                        if attempt < max_retries:
                            import time
                            time.sleep(2)  # ç­‰å¾…åé‡è¯•
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸç”Ÿæˆ
                if not success:
                    print(f"   âŒ ç¬¬ {i+1} å¼ å›¾ç‰‡æœ€ç»ˆç”Ÿæˆå¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰")
                    # å¦‚æœæ˜¯ç¬¬ä¸€å¼ å›¾ç‰‡å¤±è´¥ï¼Œåç»­æ— æ³•ç»§ç»­
                    if i == 0:
                        print(f"   âš ï¸  ç¬¬ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
                        break
            
            if len(new_image_paths) != num_images:
                print(f"   âš ï¸  å›¾ç‰‡ç”Ÿæˆä¸å®Œæ•´ï¼ˆ{len(new_image_paths)}/{num_images}ï¼‰ï¼Œæ”¾å¼ƒé‡å»º")
                return None
            
            print(f"   ğŸ‰ æˆåŠŸé‡æ–°ç”Ÿæˆ {len(new_image_paths)} å¼ å›¾ç‰‡")
            return new_image_paths
            
        except Exception as e:
            print(f"   âŒ å›¾ç‰‡é‡æ–°ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _regenerate_paragraph(self, old_text, issue, suggestion, image_context=""):
        """
        æ ¹æ®issueå’Œsuggestionï¼Œè°ƒç”¨AIé‡æ–°ç”Ÿæˆæ”¹è¿›åçš„æ®µè½
        
        Args:
            old_text: åŸå§‹æ–‡æœ¬æ®µè½
            issue: é—®é¢˜æè¿°
            suggestion: æ”¹è¿›å»ºè®®
            image_context: ç›¸å…³å›¾ç‰‡çš„æè¿°ï¼ˆä»multimodalåˆ†æè·å–ï¼‰
            
        Returns:
            æ”¹è¿›åçš„æ–‡æœ¬ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # æ·»åŠ å›¾ç‰‡ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
            image_context_section = ""
            if image_context:
                image_context_section = f"""
**ç›¸å…³å›¾ç‰‡æè¿°ï¼ˆå‚è€ƒï¼Œç¡®ä¿æ–‡å­—ä¸å›¾ç‰‡åŒ¹é…ï¼‰ï¼š**
{image_context}
"""
            
            prompt = f"""You are an AI content optimizer for Xiaohongshu (RedNote) posts. Your goal is to rewrite text to MAXIMIZE semantic consistency with related images while maintaining natural expression.

**Original Text:**
{old_text}

**Identified Issue:**
{issue}

**Improvement Suggestion:**
{suggestion}

{image_context_section}

**Critical Requirements (in order of priority - REFLECTION optimization for higher CLIP score):**
1. **SEMANTIC CONSISTENCY IS PARAMOUNT**: The rewritten text MUST closely match the visual content and semantic meaning of the related images
2. **Use specific, concrete descriptions**: If the image shows specific objects, colors, scenes, or actions, the text MUST explicitly mention them
   - Extract ALL visual elements: objects (e.g., "latte", "wooden table", "white cup"), colors (e.g., "white", "brown"), actions (e.g., "drinking", "holding"), scenes (e.g., "coffee shop", "outdoor")
   - Mention specific details visible in the image: patterns, textures, lighting, composition elements
3. **Keyword alignment**: Extract key visual elements from the image description and naturally incorporate them into the text
   - Add visual keywords that match what's shown in the image
   - Use concrete nouns and descriptive adjectives that correspond to image content
4. **MUST include ultra-specific details** (CRITICAL for quality improvement):
   - For food recommendations: specific dish names, restaurant names, addresses/locations, prices, purchase channels
   - For travel/exploration: specific attraction/shop names, detailed addresses or subway stations, opening hours, ticket prices, personal recommended experiences
   - For products/recommendations: complete product names and models, brand names, purchase channel types, price ranges or discount info
5. **Real experience details**: Describe specific scenarios and feelings, share small details from usage/experience, can mention experiences with friends/family
6. **Practicality first**: Xiaohongshu users love "dry goods" - provide actionable advice, can include Tips, precautions, pitfalls to avoid
7. **Preserve core meaning**: Keep the original intent and information, but express it in a way that better aligns with the image
8. **Natural Xiaohongshu style**: Conversational, engaging, relatable (but secondary to semantic alignment)
9. **Avoid vague descriptions**: Don't say "some shop" or "a dish" - must provide specific names. Don't just say "worth trying" - clearly state where to buy/experience it

**Example of good rewriting (for REFLECTION optimization):**
- Original: "è¿™å®¶å’–å•¡åº—å¾ˆæ£’"
- Image shows: A latte with heart latte art on wooden table, white ceramic cup, warm lighting
- Good rewrite: "è¿™å®¶å’–å•¡åº—çš„æ‹¿é“çœŸçš„è¶…æ£’ï¼æ¡Œä¸Šçš„å¿ƒå½¢æ‹‰èŠ±çœ‹èµ·æ¥å°±å¾ˆæ²»æ„ˆâ˜• ç™½è‰²é™¶ç“·æ¯é…ä¸Šæœ¨æ¡Œï¼Œæš–è‰²è°ƒçš„å…‰çº¿è®©æ•´ä¸ªæ°›å›´ç‰¹åˆ«æ¸©é¦¨"
- Bad rewrite: "å§å¦¹ä»¬ï¼è¿™å®¶è¶…èµçš„å’–å•¡åº—ä½ ä»¬ä¸€å®šè¦å»ï¼" (è¿‡åº¦ä¼˜åŒ–é£æ ¼ï¼Œå¿½ç•¥å›¾ç‰‡å†…å®¹ï¼Œæ²¡æœ‰è§†è§‰å…³é”®è¯)

**Key principle**: After image regeneration in reflection, the text MUST include ALL visual elements shown in the newly generated image to maximize CLIP score.

**Output Requirements:**
- Output ONLY the rewritten paragraph text, no explanations
- No prefixes like "æ”¹å†™åï¼š"
- Text should be directly replaceable in the original position"""

            resp = requests.post(
                f"{self.chat_base_url}/chat/completions",
                headers={"Authorization": f"Bearer {self.chat_api_key}"},
                json={
                    "model": self.chat_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.5,  # Lower temperature for more focused, semantically consistent rewriting
                    "max_tokens": 600  # Increased tokens to support detailed descriptions
                },
                timeout=30
            )
            
            if resp.status_code == 200:
                new_text = resp.json()["choices"][0]["message"]["content"].strip()
                
                # æ¸…ç†å¯èƒ½çš„å‰ç¼€
                prefixes = ["æ”¹å†™åï¼š", "æ”¹å†™å:", "ä¿®æ”¹åï¼š", "ä¿®æ”¹å:", "æ–°æ–‡ï¼š", "æ–°æ–‡:"]
                for prefix in prefixes:
                    if new_text.startswith(prefix):
                        new_text = new_text[len(prefix):].strip()
                
                return new_text
            else:
                print(f"         âš ï¸  APIé”™è¯¯: {resp.status_code}")
                return None
                
        except Exception as e:
            print(f"         âš ï¸  é‡æ–°ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _regenerate_images_with_suggestions(self, html_path, text_content, user_profile, output_dir, image_modifications, tags=None, rag_examples=None, reflection_iteration=None):
        """
        æ ¹æ®reflection_advisorçš„å»ºè®®é‡æ–°ç”Ÿæˆå›¾ç‰‡
        
        Args:
            html_path: å½“å‰HTMLæ–‡ä»¶è·¯å¾„
            text_content: å½“å‰æ–‡æœ¬å†…å®¹
            user_profile: ç”¨æˆ·ç”»åƒ
            output_dir: è¾“å‡ºç›®å½•
            image_modifications: reflection_advisoræä¾›çš„å›¾ç‰‡ä¿®æ”¹å»ºè®®åˆ—è¡¨
            tags: æ ‡ç­¾åˆ—è¡¨
            rag_examples: RAGæ ·ä¾‹åˆ—è¡¨
            reflection_iteration: reflectionè¿­ä»£æ¬¡æ•°ï¼ˆ0=ç¬¬ä¸€æ¬¡ï¼Œç”¨äºæ·»åŠ ç‰¹æ®Šé™åˆ¶ï¼‰
            
        Returns:
            æ–°å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            print(f"\nğŸ¨ æ ¹æ®Advisorå»ºè®®é‡æ–°ç”Ÿæˆå›¾ç‰‡...")
            
            # 1. è§£æå½“å‰HTMLè·å–å›¾ç‰‡ä¿¡æ¯
            from bs4 import BeautifulSoup
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            content_div = soup.find('div', class_='post-content')
            
            if not content_div:
                print("   âš ï¸  æ— æ³•æ‰¾åˆ°post-content div")
                return None
            
            # è·å–å½“å‰å›¾ç‰‡æ•°é‡
            current_images = content_div.find_all('div', class_='post-image')
            num_images = len(current_images)
            
            if num_images == 0:
                print("   âš ï¸  æ²¡æœ‰å›¾ç‰‡éœ€è¦é‡æ–°ç”Ÿæˆ")
                return None
            
            print(f"   ğŸ“¸ éœ€è¦é‡æ–°ç”Ÿæˆ {num_images} å¼ å›¾ç‰‡")
            
            # 2. è·å–å½“å‰captions
            current_captions = self._extract_captions_from_html(html_path)
            
            # 3. æ„å»ºRAGå‚è€ƒæ–‡æœ¬
            rag_reference = ""
            if rag_examples:
                rag_texts = [ex.get('content', '')[:200] for ex in rag_examples[:3]]
                rag_reference = f"""
**RAG Top-3 Examples (Reference for style and quality):**
{chr(10).join([f"- Example {i+1}: {text}" for i, text in enumerate(rag_texts)])}
"""
            
            # 4. ç”Ÿæˆæ–°å›¾ç‰‡ï¼ˆæ•´åˆadvisorå»ºè®®ï¼‰
            theme, focus_desc = self._get_focus_subject(text_content, user_profile)
            must_people = self._must_include_people(text_content, user_profile)
            must_pets = self._must_include_pets(text_content, user_profile)
            
            # æ„å»ºèšç„¦æŒ‡å¼•
            if must_people:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
âš ï¸ **å¿…é¡»åŒ…å«äººç‰©**ï¼ˆå¦‚ç©¿æ­å±•ç¤ºã€å¥èº«ã€è‡ªæ‹ç­‰åœºæ™¯ï¼‰
èšç„¦å¯¹è±¡ï¼šäººç‰©å±•ç¤ºï¼Œé…åˆ {focus_desc.lower()}
"""
            elif must_pets:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
âš ï¸ **å¿…é¡»åŒ…å«å® ç‰©**
èšç„¦å¯¹è±¡ï¼šå® ç‰©ç‰¹å†™å’Œæ—¥å¸¸
"""
            else:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
èšç„¦å¯¹è±¡ï¼š{focus_desc}
"""
            
            new_image_paths = []
            
            for i in range(num_images):
                image_path = os.path.join(output_dir, f"personalized_post_{i+1}_reflection.png")
                
                # è·å–å¯¹åº”å›¾ç‰‡çš„ä¿®æ”¹å»ºè®®
                modification = None
                for mod in image_modifications:
                    if mod.get('position') == f'image_{i}':
                        modification = mod
                        break
                
                # æ„å»ºåŒ…å«å»ºè®®çš„prompt
                modification_guidance = ""
                if modification:
                    current_issue = modification.get('current_issue', '')
                    suggested_changes = modification.get('suggested_changes', '')
                    modification_guidance = f"""
**Advisor Image Modification Suggestions (CRITICAL - follow these exactly):**
- Current Issue: {current_issue}
- Suggested Changes: {suggested_changes}
"""
                
                # è·å–å½“å‰captionï¼ˆç”¨äºå‚è€ƒï¼‰
                caption_ref = ""
                caption_zh = ""
                caption_en = ""
                if i < len(current_captions) and current_captions[i]:
                    if isinstance(current_captions[i], dict):
                        caption_zh = current_captions[i].get('zh', '')
                        caption_en = current_captions[i].get('en', '')
                        caption_ref = f"Current Caption: {caption_zh} ({caption_en})"
                    else:
                        caption_ref = f"Current Caption: {current_captions[i]}"
                        caption_zh = str(current_captions[i])
                
                tags_section = ""
                if tags:
                    tags_str = ", ".join(tags)
                    tags_section = f"""
**Tags (Primary Reference):**
{tags_str}
"""
                
                # ç¬¬ä¸€æ¬¡reflectionçš„ç‰¹æ®Šè¦æ±‚ï¼šçªå‡ºcaptionä½œä¸ºä¸»ä½“
                caption_emphasis = ""
                if reflection_iteration == 0 and caption_zh:
                    caption_emphasis = f"""
âš ï¸ **CRITICAL FOR FIRST REFLECTION - CAPTION AS MAIN SUBJECT:**
- **The caption "{caption_zh}" MUST be the PRIMARY and DOMINANT subject in the image**
- The caption object/item should occupy the CENTER and FOREGROUND of the composition
- Other elements (background, context, etc.) should be SECONDARY and support the caption subject
- The image should clearly and prominently show what the caption describes
- Composition priority: Caption subject (70%+) > Context elements (30%-)
- This ensures maximum CLIP score matching between image and caption
"""
                
                prompt_text = f"""
Generate image {i+1}/{num_images} for a Xiaohongshu post (Reflection - regenerate based on context, caption, and advisor suggestions).

{tags_section}

**Text Context (PRIMARY Reference):**
{text_content[:800]}

{caption_ref}

{caption_emphasis}

{modification_guidance}

**User Profile:**
{user_profile[:300]}

{rag_reference}

{focus_guidance}

**CRITICAL Requirements:**
1. **PRIMARY References**: Context text + Caption (most important), RAG examples + user_profile (secondary)
2. **Follow Advisor Suggestions**: Implement the suggested changes exactly as specified
3. **Caption Alignment**: The image must match the current caption description
4. **Natural Xiaohongshu photo style**: Realistic, life-like, social media aesthetic
5. **Clear composition**: Main subject should be prominent and clearly visible
6. **ABSOLUTELY NO TEXT OVERLAYS**: Strictly prohibit any text on the image, especially Chinese characters! The image must be pure visual content without any text overlays

**REMEMBER**: This is reflection optimization - regenerate images based on context+caption (PRIMARY) and advisor suggestions to improve image-caption matching.
"""
                
                # è°ƒç”¨å›¾ç‰‡ç”ŸæˆAPIï¼ˆå¤ç”¨generate_imagesä¸­çš„é€»è¾‘ï¼‰
                success = False
                max_retries = 3
                reference_image = None  # ç”¨äºä¿æŒä¸€è‡´æ€§
                
                for attempt in range(1, max_retries + 1):
                    try:
                        if attempt > 1:
                            print(f"   ğŸ”„ å›¾ç‰‡ {i+1} é‡è¯• {attempt}/{max_retries}...")
                        else:
                            print(f"   ğŸ¨ ç”Ÿæˆå›¾ç‰‡ {i+1}/{num_images}...")
                        
                        # è°ƒç”¨Gemini APIç”Ÿæˆå›¾ç‰‡ï¼ˆä½¿ç”¨REST APIï¼Œä¸generate_imagesä¸€è‡´ï¼‰
                        # æ„å»ºè¯·æ±‚å†…å®¹
                        parts = [{"text": prompt_text}]
                        
                        # å¦‚æœæœ‰å‚è€ƒå›¾ç‰‡ä¸”ä¸æ˜¯ç¬¬ä¸€å¼ ï¼Œæ·»åŠ å‚è€ƒ
                        if reference_image and i > 0:
                            parts.insert(0, {
                                "inlineData": {
                                    "mimeType": "image/png",
                                    "data": reference_image
                                }
                            })
                        
                        resp = requests.post(
                            f"{self.generate_base_url}/models/gemini-2.5-flash-image-preview:generateContent",
                            headers={"Content-Type": "application/json", "Authorization": f"Bearer {self.generate_api_key}"},
                            json={
                                "contents": [{"parts": parts}],
                                "generationConfig": {"temperature": 0.9}
                            },
                            timeout=60
                        )
                        
                        if resp.status_code == 200:
                            result_parts = resp.json().get("candidates", [{}])[0].get("content", {}).get("parts", [])
                            for part in result_parts:
                                if "inlineData" in part:
                                    # ä¿å­˜å›¾ç‰‡
                                    image_data = base64.b64decode(part["inlineData"]["data"])
                                    with open(image_path, "wb") as f:
                                        f.write(image_data)
                                    
                                    if os.path.exists(image_path):
                                        new_image_paths.append(image_path)
                                        success = True
                                        print(f"      âœ… å›¾ç‰‡ {i+1} ç”ŸæˆæˆåŠŸ")
                                        
                                        # ä¿å­˜ä¸ºä¸‹ä¸€å¼ çš„å‚è€ƒ
                                        reference_image = part["inlineData"]["data"]
                                        break
                        else:
                            print(f"      âš ï¸  APIè¿”å›é”™è¯¯: {resp.status_code}")
                            if resp.text:
                                error_msg = resp.text[:200]
                                print(f"         é”™è¯¯ä¿¡æ¯: {error_msg}")
                        
                        if success:
                            break
                            
                    except Exception as e:
                        print(f"      âš ï¸  å°è¯• {attempt} å¤±è´¥: {e}")
                        if attempt == max_retries:
                            print(f"      âŒ å›¾ç‰‡ {i+1} ç”Ÿæˆå¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰")
                
                if not success:
                    print(f"   âš ï¸  å›¾ç‰‡ {i+1} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
            
            if new_image_paths:
                print(f"   âœ… æˆåŠŸç”Ÿæˆ {len(new_image_paths)} å¼ å›¾ç‰‡")
                return new_image_paths
            else:
                print(f"   âŒ æ‰€æœ‰å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                return None
                
        except Exception as e:
            print(f"   âŒ é‡æ–°ç”Ÿæˆå›¾ç‰‡è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _regenerate_images_with_new_captions(self, html_path, text_content, user_profile, output_dir, new_captions, tags=None, rag_examples=None):
        """
        æ ¹æ®æ–°ç”Ÿæˆçš„captionå’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡é‡æ–°ç”Ÿæˆå›¾ç‰‡ï¼ˆç”¨äºç¬¬ä¸‰æ¬¡reflectionï¼‰
        
        Args:
            html_path: å½“å‰HTMLæ–‡ä»¶è·¯å¾„
            text_content: å½“å‰æ–‡æœ¬å†…å®¹
            user_profile: ç”¨æˆ·ç”»åƒ
            output_dir: è¾“å‡ºç›®å½•
            new_captions: æ–°ç”Ÿæˆçš„captionåˆ—è¡¨ï¼Œæ¯ä¸ªæ˜¯dict: {{"zh": "ä¸­æ–‡", "en": "English"}}
            tags: æ ‡ç­¾åˆ—è¡¨
            rag_examples: RAGæ ·ä¾‹åˆ—è¡¨
            
        Returns:
            æ–°å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            print(f"\nğŸ¨ æ ¹æ®æ–°captionå’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡é‡æ–°ç”Ÿæˆå›¾ç‰‡...")
            
            # 1. è§£æå½“å‰HTMLè·å–å›¾ç‰‡ä¿¡æ¯
            from bs4 import BeautifulSoup
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            content_div = soup.find('div', class_='post-content')
            
            if not content_div:
                print("   âš ï¸  æ— æ³•æ‰¾åˆ°post-content div")
                return None
            
            # è·å–å½“å‰å›¾ç‰‡æ•°é‡
            current_images = content_div.find_all('div', class_='post-image')
            num_images = len(current_images)
            
            if num_images == 0:
                print("   âš ï¸  æ²¡æœ‰å›¾ç‰‡éœ€è¦é‡æ–°ç”Ÿæˆ")
                return None
            
            print(f"   ğŸ“¸ éœ€è¦é‡æ–°ç”Ÿæˆ {num_images} å¼ å›¾ç‰‡")
            
            # 2. æ„å»ºRAGå‚è€ƒæ–‡æœ¬
            rag_reference = ""
            if rag_examples:
                rag_texts = [ex.get('content', '')[:200] for ex in rag_examples[:3]]
                rag_reference = f"""
**RAG Top-3 Examples (Reference for style and quality):**
{chr(10).join([f"- Example {i+1}: {text}" for i, text in enumerate(rag_texts)])}
"""
            
            # 3. ç”Ÿæˆæ–°å›¾ç‰‡ï¼ˆæ ¹æ®æ–°captionå’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡ï¼‰
            theme, focus_desc = self._get_focus_subject(text_content, user_profile)
            must_people = self._must_include_people(text_content, user_profile)
            must_pets = self._must_include_pets(text_content, user_profile)
            
            # æ„å»ºèšç„¦æŒ‡å¼•
            if must_people:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
âš ï¸ **å¿…é¡»åŒ…å«äººç‰©**ï¼ˆå¦‚ç©¿æ­å±•ç¤ºã€å¥èº«ã€è‡ªæ‹ç­‰åœºæ™¯ï¼‰
èšç„¦å¯¹è±¡ï¼šäººç‰©å±•ç¤ºï¼Œé…åˆ {focus_desc.lower()}
"""
            elif must_pets:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
âš ï¸ **å¿…é¡»åŒ…å«å® ç‰©**
èšç„¦å¯¹è±¡ï¼šå® ç‰©ç‰¹å†™å’Œæ—¥å¸¸
"""
            else:
                focus_guidance = f"""
ğŸ“ ä¸»é¢˜ï¼š{theme}
èšç„¦å¯¹è±¡ï¼š{focus_desc}
"""
            
            new_image_paths = []
            reference_image = None  # ç”¨äºä¿æŒä¸€è‡´æ€§
            
            for i in range(num_images):
                image_path = os.path.join(output_dir, f"personalized_post_{i+1}_reflection3.png")
                
                # è·å–å¯¹åº”çš„æ–°caption
                caption_dict = None
                if i < len(new_captions):
                    caption_dict = new_captions[i]
                
                caption_zh = caption_dict.get('zh', '') if caption_dict else ''
                caption_en = caption_dict.get('en', '') if caption_dict else ''
                
                tags_section = ""
                if tags:
                    tags_str = ", ".join(tags)
                    tags_section = f"""
**Tags:**
{tags_str}
"""
                
                # æ„å»ºpromptï¼šå¼ºè°ƒcaptionä½œä¸ºä¸»ä½“ï¼Œéƒ¨åˆ†ä¸Šä¸‹æ–‡ä½œä¸ºè¾…åŠ©
                prompt_text = f"""
Generate image {i+1}/{num_images} for a Xiaohongshu post (3rd Reflection - regenerate based on NEW caption and partial context).

**NEW Caption (PRIMARY - MUST be the main subject):**
- Chinese: {caption_zh}
- English: {caption_en}

âš ï¸ **CRITICAL: The caption "{caption_en}" MUST be the PRIMARY and DOMINANT subject in the image**
- The caption object/item should occupy the CENTER and FOREGROUND of the composition
- Other elements should be SECONDARY and support the caption subject
- Composition priority: Caption subject (70%+) > Context elements (30%-)

**Partial Text Context (SECONDARY - for style and atmosphere only):**
{text_content[:400]}

{tags_section}

**User Profile:**
{user_profile[:300]}

{rag_reference}

{focus_guidance}

**CRITICAL Requirements:**
1. **Caption as Main Subject**: The caption "{caption_en}" MUST be the dominant visual element
2. **Context for Style**: Use partial context only for style, atmosphere, and background elements
3. **Natural Xiaohongshu photo style**: Realistic, life-like, social media aesthetic
4. **Clear composition**: Caption subject should be prominent and clearly visible
5. **ABSOLUTELY NO TEXT OVERLAYS**: Strictly prohibit any text on the image, especially Chinese characters! The image must be pure visual content without any text overlays

**REMEMBER**: This is 3rd reflection - regenerate images with caption as PRIMARY subject, partial context as SECONDARY reference.
"""
                
                # è°ƒç”¨å›¾ç‰‡ç”ŸæˆAPI
                success = False
                max_retries = 3
                
                for attempt in range(1, max_retries + 1):
                    try:
                        if attempt > 1:
                            print(f"   ğŸ”„ å›¾ç‰‡ {i+1} é‡è¯• {attempt}/{max_retries}...")
                        else:
                            print(f"   ğŸ¨ ç”Ÿæˆå›¾ç‰‡ {i+1}/{num_images}...")
                        
                        # è°ƒç”¨Gemini APIç”Ÿæˆå›¾ç‰‡ï¼ˆä½¿ç”¨REST APIï¼‰
                        parts = [{"text": prompt_text}]
                        
                        # å¦‚æœæœ‰å‚è€ƒå›¾ç‰‡ä¸”ä¸æ˜¯ç¬¬ä¸€å¼ ï¼Œæ·»åŠ å‚è€ƒ
                        if reference_image and i > 0:
                            parts.insert(0, {
                                "inlineData": {
                                    "mimeType": "image/png",
                                    "data": reference_image
                                }
                            })
                        
                        resp = requests.post(
                            f"{self.generate_base_url}/models/gemini-2.5-flash-image-preview:generateContent",
                            headers={"Content-Type": "application/json", "Authorization": f"Bearer {self.generate_api_key}"},
                            json={
                                "contents": [{"parts": parts}],
                                "generationConfig": {"temperature": 0.9}
                            },
                            timeout=60
                        )
                        
                        if resp.status_code == 200:
                            result_parts = resp.json().get("candidates", [{}])[0].get("content", {}).get("parts", [])
                            for part in result_parts:
                                if "inlineData" in part:
                                    # ä¿å­˜å›¾ç‰‡
                                    image_data = base64.b64decode(part["inlineData"]["data"])
                                    with open(image_path, "wb") as f:
                                        f.write(image_data)
                                    
                                    if os.path.exists(image_path):
                                        new_image_paths.append(image_path)
                                        success = True
                                        print(f"      âœ… å›¾ç‰‡ {i+1} ç”ŸæˆæˆåŠŸ")
                                        
                                        # ä¿å­˜ä¸ºä¸‹ä¸€å¼ çš„å‚è€ƒ
                                        reference_image = part["inlineData"]["data"]
                                        break
                        else:
                            print(f"      âš ï¸  APIè¿”å›é”™è¯¯: {resp.status_code}")
                            if resp.text:
                                error_msg = resp.text[:200]
                                print(f"         é”™è¯¯ä¿¡æ¯: {error_msg}")
                        
                        if success:
                            break
                            
                    except Exception as e:
                        print(f"      âš ï¸  å°è¯• {attempt} å¤±è´¥: {e}")
                        if attempt == max_retries:
                            print(f"      âŒ å›¾ç‰‡ {i+1} ç”Ÿæˆå¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰")
                
                if not success:
                    print(f"   âš ï¸  å›¾ç‰‡ {i+1} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
            
            if new_image_paths:
                print(f"   âœ… æˆåŠŸç”Ÿæˆ {len(new_image_paths)} å¼ å›¾ç‰‡")
                return new_image_paths
            else:
                print(f"   âŒ æ‰€æœ‰å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                return None
                
        except Exception as e:
            print(f"   âŒ é‡æ–°ç”Ÿæˆå›¾ç‰‡è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _replace_images_in_html(self, html_path, new_image_paths, iteration):
        """
        æ›¿æ¢HTMLä¸­çš„å›¾ç‰‡è·¯å¾„ï¼ˆä¿ç•™åŸæœ‰captionï¼‰
        
        Args:
            html_path: å½“å‰HTMLæ–‡ä»¶è·¯å¾„
            new_image_paths: æ–°å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            
        Returns:
            æ–°HTMLæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            print(f"\n   ğŸ”„ æ›´æ–°HTMLä¸­çš„å›¾ç‰‡å¼•ç”¨ï¼ˆä¿ç•™captionï¼‰...")
            
            # è¯»å–HTML
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            content_div = soup.find('div', class_='post-content')
            
            if not content_div:
                print("   âš ï¸  æ— æ³•æ‰¾åˆ°post-content div")
                return None
            
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡div
            image_divs = content_div.find_all('div', class_='post-image')
            
            if len(image_divs) != len(new_image_paths):
                print(f"   âš ï¸  å›¾ç‰‡æ•°é‡ä¸åŒ¹é…ï¼šHTMLä¸­{len(image_divs)}å¼ ï¼Œæ–°å›¾ç‰‡{len(new_image_paths)}å¼ ")
                return None
            
            # æ›¿æ¢æ¯å¼ å›¾ç‰‡çš„srcï¼ˆä¿ç•™captionï¼‰
            for idx, (img_div, new_img_path) in enumerate(zip(image_divs, new_image_paths)):
                img_tag = img_div.find('img')
                if img_tag:
                    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
                    new_img_filename = os.path.basename(new_img_path)
                    img_tag['src'] = new_img_filename
                    # captionä¿æŒä¸å˜
                    caption_div = img_div.find('div', class_='image-caption')
                    caption_text = caption_div.get_text(strip=True) if caption_div else f"å›¾ {idx+1}"
                    print(f"      âœ… å›¾ç‰‡ {idx+1}: {new_img_filename} (caption: {caption_text[:20]}...)")
            
            # ä¿å­˜ä¸ºæ–°ç‰ˆæœ¬
            output_dir = Path(html_path).parent
            new_html_path = output_dir / f"image_text_v{iteration+1}.html"
            
            with open(new_html_path, 'w', encoding='utf-8') as f:
                f.write(str(soup))
            
            print(f"   âœ… æ–°ç‰ˆæœ¬HTMLå·²ä¿å­˜: {new_html_path.name}")
            return str(new_html_path)
            
        except Exception as e:
            print(f"   âŒ HTMLæ›´æ–°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _apply_reflection_suggestions(self, html_path, suggestions, iteration):
        """
        åº”ç”¨Reflectionå»ºè®®ï¼Œç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬çš„HTML
        
        Args:
            html_path: åŸHTMLæ–‡ä»¶è·¯å¾„
            suggestions: Reflectionå»ºè®®ï¼ˆåŒ…å«text_changeså’Œimage_captionsï¼‰
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆ1-3ï¼‰
            
        Returns:
            æ–°HTMLæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            print(f"\nğŸ”§ åº”ç”¨Reflectionå»ºè®®ï¼ˆç¬¬{iteration}æ¬¡ä¼˜åŒ–ï¼‰...")
            
            # è¯»å–åŸHTML
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            content_div = soup.find('div', class_='post-content')
            
            if not content_div:
                print("âš ï¸  æ— æ³•æ‰¾åˆ°post-content div")
                return None
            
            # 1. åº”ç”¨å›¾ç‰‡captionä¿®æ”¹
            if suggestions.get('image_captions'):
                print(f"   ğŸ“¸ æ›´æ–° {len(suggestions['image_captions'])} ä¸ªå›¾ç‰‡caption...")
                for caption_suggestion in suggestions['image_captions']:
                    position = caption_suggestion.get('position', '')  # å¦‚ "image_0"
                    new_caption = caption_suggestion.get('caption', '')  # å¦‚ "å›¾1: ç®€çŸ­æè¿°"
                    
                    if position.startswith('image_'):
                        try:
                            image_index = int(position.split('_')[1])
                            # æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡caption
                            image_divs = content_div.find_all('div', class_='post-image')
                            if image_index < len(image_divs):
                                caption_div = image_divs[image_index].find('div', class_='image-caption')
                                if caption_div:
                                    # å¤„ç†æ–°çš„dictæ ¼å¼æˆ–æ—§çš„å­—ç¬¦ä¸²æ ¼å¼
                                    if isinstance(new_caption, dict):
                                        caption_zh = new_caption.get("zh", "")
                                        caption_en = new_caption.get("en", "")
                                        formatted_caption = f"å›¾{image_index + 1}: {caption_zh}"
                                        # è®¾ç½®data-caption-enå±æ€§ï¼ˆç”¨äºCLIPè®¡ç®—ï¼‰
                                        caption_div['data-caption-en'] = caption_en
                                        caption_div.string = formatted_caption
                                        print(f"      âœ… {position}: {formatted_caption} ({caption_en})")
                                    else:
                                        # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå‡è®¾æ˜¯è‹±æ–‡
                                        if not new_caption.startswith("å›¾"):
                                            formatted_caption = f"å›¾{image_index + 1}: {new_caption}"
                                        else:
                                            formatted_caption = new_caption
                                        caption_div['data-caption-en'] = new_caption
                                        caption_div.string = formatted_caption
                                        print(f"      âœ… {position}: {formatted_caption}")
                        except Exception as e:
                            print(f"      âš ï¸  æ›´æ–°captionå¤±è´¥ ({position}): {e}")
            
            # 2. åº”ç”¨æ–‡æœ¬ä¿®æ”¹ï¼ˆè°ƒç”¨AIé‡æ–°ç”Ÿæˆæ”¹è¿›åçš„æ®µè½ï¼‰
            if suggestions.get('text_changes'):
                # è¿‡æ»¤æ‰linkç›¸å…³çš„å»ºè®®ï¼ˆç”¨æˆ·è¦æ±‚å…ˆä¸ç®¡linkï¼‰
                text_changes = [c for c in suggestions['text_changes'] if not c.get('position', '').startswith('link_')]
                
                # å‡†å¤‡å›¾ç‰‡captionä¿¡æ¯ï¼ˆç”¨ä½œä¸Šä¸‹æ–‡ï¼‰
                image_analyses = suggestions.get('image_analyses', [])
                image_captions_map = {}
                for img_analysis in image_analyses:
                    img_index = img_analysis.get('image_index', -1)
                    if img_index >= 0:
                        image_captions_map[img_index] = img_analysis.get('caption', '')
                
                if text_changes:
                    print(f"   âœï¸  åº”ç”¨ {len(text_changes)} æ¡æ–‡æœ¬ä¿®æ”¹å»ºè®®...")
                    
                    # æå–æ‰€æœ‰<p>æ ‡ç­¾å’Œ<div class="post-image">ï¼ˆç”¨äºå®šä½ç›¸é‚»å›¾ç‰‡ï¼‰
                    all_elements = []
                    for elem in content_div.find_all(['p', 'div'], recursive=False):
                        if elem.name == 'p':
                            all_elements.append(('text', elem))
                        elif elem.name == 'div' and 'post-image' in elem.get('class', []):
                            all_elements.append(('image', elem))
                    
                    # æ„å»ºtext_indexåˆ°elementçš„æ˜ å°„
                    text_elements = []
                    text_index_to_position = {}
                    text_counter = 0
                    for i, (elem_type, elem) in enumerate(all_elements):
                        if elem_type == 'text':
                            text_elements.append((elem, i))
                            text_index_to_position[text_counter] = i
                            text_counter += 1
                    
                    for change in text_changes:
                        position = change.get('position', '')  # å¦‚ "text_0"
                        issue = change.get('issue', '')
                        suggestion_text = change.get('suggestion', '')
                        
                        # è§£æpositionï¼ˆå¦‚ "text_0" -> index 0ï¼‰
                        if position.startswith('text_'):
                            try:
                                text_index = int(position.split('_')[1])
                                
                                if text_index < len(text_elements):
                                    old_paragraph, elem_position = text_elements[text_index]
                                    old_text = old_paragraph.get_text(strip=True)
                                    
                                    # æŸ¥æ‰¾ç›¸é‚»çš„å›¾ç‰‡captionï¼ˆä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
                                    image_context = ""
                                    # æŸ¥æ‰¾å‰åçš„å›¾ç‰‡
                                    for offset in [-1, 1, -2, 2]:
                                        check_pos = elem_position + offset
                                        if 0 <= check_pos < len(all_elements):
                                            elem_type, elem = all_elements[check_pos]
                                            if elem_type == 'image':
                                                # å°è¯•æ‰¾åˆ°è¿™ä¸ªå›¾ç‰‡çš„caption
                                                img_caption_div = elem.find('div', class_='image-caption')
                                                if img_caption_div:
                                                    caption_text = img_caption_div.get_text(strip=True)
                                                    if caption_text and caption_text != f"å›¾ {offset}":
                                                        image_context += f"- {caption_text}\n"
                                    
                                    # è°ƒç”¨AIé‡æ–°ç”Ÿæˆæ”¹è¿›åçš„æ®µè½ï¼ˆä¼ å…¥å›¾ç‰‡ä¸Šä¸‹æ–‡ï¼‰
                                    print(f"      ğŸ”„ {position}: æ­£åœ¨é‡æ–°ç”Ÿæˆ...")
                                    if image_context:
                                        print(f"         ğŸ“¸ ç›¸å…³å›¾ç‰‡: {image_context[:60]}...")
                                    
                                    new_text = self._regenerate_paragraph(
                                        old_text=old_text,
                                        issue=issue,
                                        suggestion=suggestion_text,
                                        image_context=image_context if image_context else ""
                                    )
                                    
                                    if new_text and new_text != old_text:
                                        # æ›¿æ¢æ®µè½å†…å®¹ï¼ˆä¿ç•™HTMLæ ‡ç­¾ç»“æ„ï¼‰
                                        old_paragraph.clear()
                                        # å¤„ç†markdownï¼ˆåŠ ç²—ç­‰ï¼‰
                                        rendered_text = self._render_markdown(new_text)
                                        from bs4 import BeautifulSoup as BS
                                        rendered_soup = BS(rendered_text, 'html.parser')
                                        for child in rendered_soup.children:
                                            old_paragraph.append(child)
                                        
                                        print(f"      âœ… {position}: å·²æ›´æ–°")
                                        print(f"         åŸæ–‡: {old_text[:50]}...")
                                        print(f"         æ–°æ–‡: {new_text[:50]}...")
                                    else:
                                        print(f"      âš ï¸  {position}: AIæœªç”Ÿæˆæ–°å†…å®¹æˆ–å†…å®¹ç›¸åŒï¼Œè·³è¿‡")
                                        
                            except Exception as e:
                                print(f"      âš ï¸  {position}: ä¿®æ”¹å¤±è´¥ - {e}")
                                import traceback
                                traceback.print_exc()
            
            # 3. ä¿å­˜ä¸ºæ–°ç‰ˆæœ¬ï¼ˆæˆ–è¦†ç›–ç°æœ‰ç‰ˆæœ¬ï¼‰
            output_dir = Path(html_path).parent
            target_version = f"_v{iteration+1}"  # v2, v3, v4
            new_html_path = output_dir / f"image_text{target_version}.html"
            
            # æ£€æŸ¥å½“å‰html_pathæ˜¯å¦å·²ç»æ˜¯ç›®æ ‡ç‰ˆæœ¬ï¼ˆä¾‹å¦‚å›¾ç‰‡é‡å»ºåï¼‰
            current_filename = Path(html_path).name
            if current_filename == f"image_text{target_version}.html":
                # å·²ç»æ˜¯ç›®æ ‡ç‰ˆæœ¬ï¼Œç›´æ¥è¦†ç›–
                print(f"   ğŸ”„ è¦†ç›–ç°æœ‰ç‰ˆæœ¬: {current_filename}ï¼ˆå›¾ç‰‡+æ–‡æœ¬/captionè”åˆä¼˜åŒ–ï¼‰")
            else:
                # åˆ›å»ºæ–°ç‰ˆæœ¬
                print(f"   âœ… åˆ›å»ºæ–°ç‰ˆæœ¬: {new_html_path.name}")
            
            with open(new_html_path, 'w', encoding='utf-8') as f:
                f.write(str(soup))
            
            return str(new_html_path)
            
        except Exception as e:
            print(f"   âŒ åº”ç”¨å»ºè®®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def __call__(self, user_profile, output_dir, user_profile_path=None):
        """Main execution flow - æ”¯æŒ RAG æ¨¡å¼
        
        Args:
            user_profile: User profile text or dict
            output_dir: Output directory
            user_profile_path: Path to user profile file (for RAG)
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # Extract product ID from output_dir (format: generated_it/{index}_{user_id}/)
        product_id = os.path.basename(os.path.normpath(output_dir))
        # Extract index if available (format: {index}_{user_id})
        product_index = None
        if '_' in product_id:
            try:
                product_index = int(product_id.split('_')[0])
            except ValueError:
                pass
        
        # Parse profile data if needed
        profile_data = None
        if user_profile_path and os.path.exists(user_profile_path):
            try:
                with open(user_profile_path, 'r', encoding='utf-8') as f:
                    profile_data = json.load(f)
                print(f"ğŸ“‹ Loaded profile from: {user_profile_path}")
            except Exception as e:
                print(f"âš ï¸ Failed to load profile data: {e}")
        
        # Get profile text
        if isinstance(user_profile, dict):
            profile_text = user_profile.get("profile_text", json.dumps(user_profile, ensure_ascii=False))
        else:
            profile_text = str(user_profile)
            if profile_data is None:
                profile_data = {"profile_text": profile_text}
        
        print("1. ç”Ÿæˆæ–‡æ¡ˆã€æ ‡ç­¾ä¸å¹³å°é“¾æ¥...")
        text, tags, links = self.generate_text(profile_text, 
                                               user_profile_path=user_profile_path,
                                               profile_data=profile_data)
        print(f"   - æå–åˆ° {len(tags)} ä¸ªæ ‡ç­¾: {tags}")
        print(f"   - æå–åˆ° {len(links)} ä¸ªæ¨èé“¾æ¥: {[l['platform'] for l in links]}")
        
        print("2. æå–captionï¼ˆåŸºäºæ–‡æ¡ˆï¼Œä¸åˆ†æå›¾ç‰‡ï¼‰...")
        # ğŸ¯ å…ˆåŸºäºæ–‡æ¡ˆæå–captionï¼Œè¿™æ ·åˆäº§å“åˆ†æ•°ä¸ä¼šå¤ªé«˜ï¼Œreflectionæ•ˆæœæ‰èƒ½ä½“ç°
        # è®¡ç®—éœ€è¦å¤šå°‘å¼ å›¾ç‰‡
        word_count = len(text.strip())
        num_images = 1 if word_count <= 300 else (2 if word_count <= 800 else 3)
        
        # ä¸€æ¬¡æ€§æå–æ‰€æœ‰å›¾ç‰‡çš„captionï¼Œç¡®ä¿å®ƒä»¬ä¸åŒ
        image_captions = self._extract_all_captions_from_text(text, num_images)
        for i, caption in enumerate(image_captions):
            if isinstance(caption, dict):
                print(f"   ğŸ“ å›¾ç‰‡{i+1} caption: {caption.get('zh', '')} ({caption.get('en', '')})")
            else:
                print(f"   ğŸ“ å›¾ç‰‡{i+1} caption: {caption}")
        
        print("3. ç”Ÿæˆé…å›¾...")
        # ç¬¬äºŒè½®åæ€æ—¶ä¼šä½¿ç”¨å®Œæ•´æ–‡æ¡ˆä¸Šä¸‹æ–‡é‡æ–°ç”Ÿæˆå›¾ç‰‡
        images = self.generate_images(profile_text, text, output_dir, tags=tags, image_captions=image_captions)
        
        # æ£€æŸ¥å›¾ç‰‡ç”Ÿæˆæƒ…å†µ
        initial_image_generation_failed = len(images) == 0
        if initial_image_generation_failed:
            print(f"âš ï¸  åˆå§‹å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼ˆ0å¼ å›¾ç‰‡ï¼‰")
            print(f"ğŸ’¡ ç­–ç•¥ï¼šå…ˆç”Ÿæˆçº¯æ–‡æœ¬HTMLï¼Œåœ¨Reflectionæ—¶å°è¯•é‡æ–°ç”Ÿæˆå›¾ç‰‡")
            image_captions = []  # ç¡®ä¿captionsä¸ºç©º
        else:
            print(f"âœ… æˆåŠŸç”Ÿæˆ {len(images)} å¼ å›¾ç‰‡")
            print(f"   ğŸ“ ä½¿ç”¨é¢„æå–çš„Captions: {image_captions}")
        
        print("4. ç”ŸæˆHTML...")
        html_path = os.path.join(output_dir, "image_text_v0.html")  # åˆå§‹ç‰ˆæœ¬ä¸ºv0
        self.generate_html_post(text, images, links, tags, html_path, image_captions=image_captions)
        print(f"âœ… å®Œæˆ: {html_path}")
        
        # 4. Reflectionæœºåˆ¶ï¼ˆè‡ªåŠ¨è¿­ä»£ä¼˜åŒ–ï¼‰
        reflection_history = []  # è®°å½•æ¯æ¬¡reflectionçš„ç»“æœ
        current_html_path = html_path
        final_version = "v0"  # åˆå§‹ç‰ˆæœ¬ä¸ºv0
        
        if self.reflection_enabled:
            print(f"\n{'='*80}")
            print(f"ğŸ”„ å¯åŠ¨Reflectionæœºåˆ¶ï¼ˆæœ€å¤š{self.max_reflection_iterations}æ¬¡è¿­ä»£ï¼‰")
            print(f"   é˜ˆå€¼: GroupScore â‰¥ {self.reflection_threshold}")
            print(f"{'='*80}")
            
            for iteration in range(self.max_reflection_iterations):
                print(f"\n{'â”€'*80}")
                # Display product ID/index in evaluation header
                if product_index is not None:
                    print(f"ğŸ“Š [{product_index}] ç¬¬{iteration+1}æ¬¡è¯„ä¼° (å½“å‰ç‰ˆæœ¬: v{iteration})")
                else:
                    print(f"ğŸ“Š [{product_id}] ç¬¬{iteration+1}æ¬¡è¯„ä¼° (å½“å‰ç‰ˆæœ¬: v{iteration})")
                print(f"{'â”€'*80}")
                
                try:
                    # 4.1 è®¡ç®—GroupScore
                    print(f"\n1ï¸âƒ£  è®¡ç®—GroupScore...")
                    eval_result = evaluate_file(
                        html_path=current_html_path,
                        evaluator=self.clip_evaluator,
                        use_combined=True,
                        verbose=False  # ä¸æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œä¿æŒè¾“å‡ºç®€æ´
                    )
                    
                    if not eval_result:
                        print(f"   âš ï¸  GroupScoreè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡reflection")
                        break
                    
                    # ğŸš¨ ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼ˆåˆå§‹ç”Ÿæˆå¤±è´¥ï¼‰ï¼Œç«‹å³å°è¯•ç”Ÿæˆ
                    if eval_result.num_pairs == 0:
                        print(f"   âš ï¸  æ£€æµ‹åˆ°æ²¡æœ‰å›¾ç‰‡ï¼ˆnum_pairs=0ï¼‰")
                        print(f"   ğŸ’¡ å°è¯•ç”Ÿæˆå›¾ç‰‡ä»¥ä¿®å¤é—®é¢˜...")
                        
                        try:
                            # å°è¯•ç”Ÿæˆå›¾ç‰‡
                            new_image_paths = self._regenerate_images_for_reflection(
                                html_path=current_html_path,
                                text_content=text,
                                user_profile=profile_text,
                                output_dir=output_dir,
                                iteration=iteration
                            )
                            
                            if new_image_paths and len(new_image_paths) > 0:
                                # é‡æ–°ç”ŸæˆHTMLï¼ˆæ·»åŠ å›¾ç‰‡ï¼‰
                                print(f"   ğŸ”„ é‡æ–°ç”ŸæˆHTMLï¼ˆæ·»åŠ å›¾ç‰‡ï¼‰...")
                                new_html_path = os.path.join(output_dir, f"image_text_v{iteration+1}.html")
                                # é‡æ–°ç”Ÿæˆå›¾ç‰‡æ—¶ä¿ç•™åŸæœ‰captionï¼ˆä»å½“å‰HTMLä¸­æå–ï¼‰
                                current_captions = self._extract_captions_from_html(html_path)
                                self.generate_html_post(text, new_image_paths, links, tags, new_html_path, image_captions=current_captions)
                                
                                # éªŒè¯æ–°ç‰ˆæœ¬
                                new_eval_result = evaluate_file(
                                    html_path=new_html_path,
                                    evaluator=self.clip_evaluator,
                                    use_combined=True,
                                    verbose=False
                                )
                                
                                if new_eval_result and new_eval_result.num_pairs > 0:
                                    groupscore = new_eval_result.group_score_mean
                                    print(f"   âœ… æˆåŠŸæ·»åŠ å›¾ç‰‡ï¼GroupScore (Mean): {groupscore:.4f}")
                                    current_html_path = new_html_path
                                    final_version = f"v{iteration+1}"
                                    
                                    # å¦‚æœè¾¾åˆ°é˜ˆå€¼ï¼Œè®°å½•å¹¶ç»“æŸ
                                    if groupscore >= self.reflection_threshold:
                                        print(f"   ğŸ‰ GroupScoreå·²è¾¾åˆ°é˜ˆå€¼ï¼")
                                        # è¾¾æ ‡æ—¶æ‰è®°å½•ï¼Œå› ä¸ºä¸ä¼šå†æœ‰ä¸‹ä¸€ä¸ªiteration
                                        reflection_history.append({
                                            "iteration": iteration + 1,
                                            "version": f"v{iteration+1}",
                                            "groupscore": groupscore,
                                            "html_path": new_html_path,
                                            "strategy": "image_generation_rescue"
                                        })
                                        break
                                    else:
                                        # æœªè¾¾æ ‡ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£
                                        # ä¸‹ä¸€ä¸ªiterationå¼€å§‹æ—¶ä¼šè‡ªåŠ¨è¯„ä¼°å’Œè®°å½•
                                        continue
                                else:
                                    print(f"   âš ï¸  æ·»åŠ å›¾ç‰‡åè¯„ä¼°å¤±è´¥ï¼Œåœæ­¢reflection")
                                    break
                            else:
                                print(f"   âŒ å›¾ç‰‡ç”Ÿæˆä»ç„¶å¤±è´¥ï¼Œæ— æ³•ç»§ç»­reflection")
                                break
                        except Exception as e:
                            print(f"   âŒ å›¾ç‰‡ç”Ÿæˆè¡¥æ•‘å¤±è´¥: {e}")
                            break
                    
                    # ä½¿ç”¨meanä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡
                    groupscore = eval_result.group_score_mean
                    print(f"   ğŸ“ˆ GroupScore (Mean): {groupscore:.4f}")
                    print(f"      (Harmonic: {eval_result.group_score_harmonic:.4f}, Min: {eval_result.group_score_min:.4f})")
                    
                    # è®°å½•æœ¬æ¬¡è¯„ä¼°ï¼ˆä½†è¦é¿å…é‡å¤è®°å½•ï¼‰
                    # æ£€æŸ¥historyä¸­æ˜¯å¦å·²æœ‰ç›¸åŒç‰ˆæœ¬çš„è®°å½•ï¼ˆå¯èƒ½åœ¨ä¸Šä¸€iterationçš„å›¾ç‰‡é‡å»ºä¸­è®°å½•è¿‡ï¼‰
                    current_version = f"v{iteration}"
                    already_recorded = any(
                        record['version'] == current_version 
                        for record in reflection_history
                    )
                    
                    if not already_recorded:
                        reflection_history.append({
                            "iteration": iteration + 1,
                            "version": current_version,
                            "groupscore": groupscore,
                            "html_path": current_html_path
                        })
                    else:
                        print(f"   â„¹ï¸  ç‰ˆæœ¬{current_version}å·²åœ¨historyä¸­ï¼Œè·³è¿‡é‡å¤è®°å½•")
                    
                    # 4.2 åˆ¤æ–­æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                    if groupscore >= self.reflection_threshold:
                        print(f"   âœ… GroupScoreè¾¾æ ‡ï¼æ— éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
                        final_version = f"v{iteration}"
                        break
                    
                    print(f"   âš ï¸  GroupScore ({groupscore:.4f}) < é˜ˆå€¼ ({self.reflection_threshold})")
                    print(f"   ğŸ”„ å¯åŠ¨ç¬¬{iteration+1}æ¬¡Reflection...")
                    
                    # 4.3 è§£æHTML
                    print(f"\n2ï¸âƒ£  è§£æHTML...")
                    parse_result = self.html_parser.parse_html_to_sequence(current_html_path)
                    html_sequence = parse_result["sequence_text"]
                    image_paths = parse_result["image_paths"]
                    print(f"   âœ… è§£æå®Œæˆ: {parse_result['stats']['texts']} æ–‡æœ¬, {parse_result['stats']['images']} å›¾ç‰‡")
                    
                    # 4.4 è·å–RAGæ ·ä¾‹ï¼ˆå¤ç”¨å·²åŠ è½½çš„examplesï¼‰
                    print(f"\n3ï¸âƒ£  å‡†å¤‡RAGæ ·ä¾‹...")
                    rag_examples = []
                    
                    if hasattr(self, '_rag_cache') and self._rag_cache:
                        # ä½¿ç”¨ç¼“å­˜çš„RAGæ ·ä¾‹
                        cache_key = list(self._rag_cache.keys())[0]
                        cached_examples = self._rag_cache[cache_key]
                        
                        # åªæå–æ–‡æœ¬æ ·ä¾‹ï¼ˆè¿‡æ»¤æ‰å›¾ç‰‡ï¼‰
                        text_examples = [ex for ex in cached_examples if ex.get("type") == "text"]
                        
                        for ex in text_examples[:3]:  # Top-3 æ–‡æœ¬æ ·ä¾‹
                            rag_examples.append({
                                "content": ex.get("content", ""),
                                "similarity": ex.get("similarity", 0)
                            })
                        print(f"   âœ… ä½¿ç”¨ {len(rag_examples)} ä¸ªç¼“å­˜çš„RAGæ–‡æœ¬æ ·ä¾‹")
                    else:
                        # RAGç¼“å­˜ä¸ºç©ºï¼Œå°è¯•å¤šç§æ–¹å¼è·å–
                        print(f"   âš ï¸  RAGç¼“å­˜ä¸ºç©ºï¼Œå°è¯•å…¶ä»–æ–¹å¼è·å–æ ·ä¾‹...")
                        
                        # æ–¹å¼1: å¦‚æœæœ‰profileè·¯å¾„ï¼Œé‡æ–°åŠ è½½
                        if user_profile_path and profile_data:
                            try:
                                user_id = self.extract_user_id_from_path(user_profile_path)
                                if user_id:
                                    top1_preference = self.extract_top1_preference(profile_data)
                                    examples = self.load_examples_with_rag(user_id, top1_preference, top_k=3)
                                    
                                    # æå–æ–‡æœ¬æ ·ä¾‹
                                    text_examples = [ex for ex in examples if ex.get("type") == "text"]
                                    for ex in text_examples[:3]:
                                        rag_examples.append({
                                            "content": ex.get("content", ""),
                                            "similarity": ex.get("similarity", 0)
                                        })
                                    print(f"   âœ… é‡æ–°åŠ è½½æˆåŠŸï¼Œè·å– {len(rag_examples)} ä¸ªRAGæ–‡æœ¬æ ·ä¾‹")
                            except Exception as e:
                                print(f"   âš ï¸  é‡æ–°åŠ è½½å¤±è´¥: {e}")
                        
                        # æ–¹å¼2: å¦‚æœæ–¹å¼1å¤±è´¥ï¼Œå°è¯•ä»fallbackç›®å½•åŠ è½½
                        if not rag_examples:
                            try:
                                fallback_examples = self._load_examples_fallback()
                                text_examples = [ex for ex in fallback_examples if ex.get("type") == "text"]
                                for ex in text_examples[:3]:
                                    rag_examples.append({
                                        "content": ex.get("content", ""),
                                        "similarity": 0.5  # é»˜è®¤ç›¸ä¼¼åº¦
                                    })
                                if rag_examples:
                                    print(f"   âœ… ä»fallbackç›®å½•åŠ è½½ {len(rag_examples)} ä¸ªæ ·ä¾‹")
                            except Exception as e:
                                print(f"   âš ï¸  FallbackåŠ è½½å¤±è´¥: {e}")
                        
                        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ ·ä¾‹ï¼Œå°±ä½¿ç”¨ç©ºåˆ—è¡¨ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰
                        if not rag_examples:
                            print(f"   â„¹ï¸  æ— å¯ç”¨RAGæ ·ä¾‹ï¼ŒReflectionå°†ä»…åŸºäºå½“å‰å†…å®¹å’Œå›¾ç‰‡åˆ†æ")
                    
                    # 4.5 è°ƒç”¨Reflection Advisor
                    print(f"\n4ï¸âƒ£  AI Reflectionåˆ†æ...")
                    
                    # åˆå§‹åŒ–å˜é‡
                    should_apply = False
                    suggestion_detail = {}
                    
                    # ğŸ¯ ç¬¬ä¸€æ¬¡Reflectionï¼šæ”¹å›¾ç‰‡ï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡+caption+RAG+user_profileï¼‰
                    if iteration == 0:
                        print(f"   ğŸ“ ç­–ç•¥ï¼šç¬¬1æ¬¡Reflection - æ ¹æ®ä¸Šä¸‹æ–‡+caption+RAG+user_profileé‡æ–°ç”Ÿæˆå›¾ç‰‡")
                        
                        # è·å–å½“å‰captions
                        current_captions = self._extract_captions_from_html(current_html_path)
                        
                        # è°ƒç”¨reflection_advisorè·å–å›¾ç‰‡ä¿®æ”¹å»ºè®®
                        advisor_result = self.reflection_advisor.evaluate_and_suggest(
                            groupscore=groupscore,
                            html_sequence=html_sequence,
                            rag_examples=rag_examples,
                            image_paths=image_paths,
                            threshold=self.reflection_threshold,
                            user_profile=profile_text,
                            reflection_iteration=0
                        )
                        
                        if advisor_result.get('should_modify') and advisor_result.get('suggestions'):
                            image_modifications = advisor_result.get('suggestions', {}).get('image_modifications', [])
                            
                            if image_modifications:
                                print(f"   âœ… è·å¾— {len(image_modifications)} æ¡å›¾ç‰‡ä¿®æ”¹å»ºè®®")
                                # æ ¹æ®å»ºè®®é‡æ–°ç”Ÿæˆå›¾ç‰‡ï¼ˆç¬¬ä¸€æ¬¡reflectionéœ€è¦çªå‡ºcaptionä¸»ä½“ï¼‰
                                new_image_paths = self._regenerate_images_with_suggestions(
                                    html_path=current_html_path,
                                    text_content=text,
                                    user_profile=profile_text,
                                    output_dir=output_dir,
                                    image_modifications=image_modifications,
                                    tags=tags,
                                    rag_examples=rag_examples,
                                    reflection_iteration=0  # æ ‡è¯†æ˜¯ç¬¬ä¸€æ¬¡reflection
                                )
                                
                                if new_image_paths:
                                    # æ›¿æ¢HTMLä¸­çš„å›¾ç‰‡ï¼ˆä¿ç•™captionï¼‰
                                    new_html_path = self._replace_images_in_html(
                                        html_path=current_html_path,
                                        new_image_paths=new_image_paths,
                                        iteration=iteration
                                    )
                                    
                                    if new_html_path:
                                        # éªŒè¯æ–°å›¾ç‰‡çš„æ•ˆæœ
                                        print(f"\n   ğŸ” éªŒè¯æ–°å›¾ç‰‡çš„æ•ˆæœ...")
                                        try:
                                            new_eval_result = evaluate_file(
                                                html_path=new_html_path,
                                                evaluator=self.clip_evaluator,
                                                use_combined=True,
                                                verbose=False
                                            )
                                            new_score = new_eval_result.group_score_mean
                                            score_improvement = new_score - groupscore
                                            
                                            print(f"      ğŸ“Š é‡æ–°ç”Ÿæˆå‰ (Mean): {groupscore:.4f}")
                                            print(f"      ğŸ“Š é‡æ–°ç”Ÿæˆå (Mean): {new_score:.4f}")
                                            print(f"      ğŸ“Š æå‡å¹…åº¦: {score_improvement:+.4f}")
                                            
                                            if new_score > groupscore or not self.reflection_strict_mode:
                                                print(f"      âœ… å›¾ç‰‡é‡æ–°ç”ŸæˆæˆåŠŸï¼")
                                                current_html_path = new_html_path
                                                groupscore = new_score
                                                final_version = f"v{iteration+1}"
                                                
                                                if new_score >= self.reflection_threshold:
                                                    print(f"      ğŸ‰ Scoreå·²è¾¾åˆ°é˜ˆå€¼ï¼")
                                                    reflection_history.append({
                                                        "iteration": iteration + 1,
                                                        "version": f"v{iteration+1}",
                                                        "groupscore": new_score,
                                                        "html_path": new_html_path,
                                                        "strategy": "image_regeneration_1st_reflection"
                                                    })
                                                    break
                                                else:
                                                    print(f"      âœ… ç¬¬1æ¬¡åæ€å®Œæˆï¼Œè¿›å…¥ä¸‹ä¸€iteration")
                                                    continue
                                            else:
                                                print(f"      âš ï¸  å›¾ç‰‡é‡æ–°ç”ŸæˆåScoreæœªæå‡ï¼Œä¿ç•™åŸå›¾ç‰‡")
                                                print(f"      âœ… ç¬¬1æ¬¡åæ€å®Œæˆï¼Œè¿›å…¥ä¸‹ä¸€iteration")
                                                continue
                                        except Exception as e:
                                            print(f"      âš ï¸  éªŒè¯å¤±è´¥: {e}")
                                            continue
                                    else:
                                        print(f"   âš ï¸  HTMLæ›´æ–°å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå›¾ç‰‡")
                                        continue
                                else:
                                    print(f"   âš ï¸  å›¾ç‰‡é‡æ–°ç”Ÿæˆå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå›¾ç‰‡")
                                    continue
                            else:
                                print(f"   âš ï¸  æœªè·å¾—å›¾ç‰‡ä¿®æ”¹å»ºè®®ï¼Œè·³è¿‡")
                                continue
                        else:
                            print(f"   â„¹ï¸  Advisorå»ºè®®æ— éœ€ä¿®æ”¹ï¼Œè·³è¿‡")
                            continue
                    
                    # ğŸ¯ ç¬¬äºŒæ¬¡Reflectionï¼šæ ¹æ®å›¾ç‰‡è°ƒæ•´captionï¼Œç¡®ä¿captionæè¿°çš„æ˜¯å›¾ç‰‡ä¸»ä½“
                    elif iteration == 1:
                        print(f"   ğŸ“ ç­–ç•¥ï¼šç¬¬2æ¬¡Reflection - æ ¹æ®å›¾ç‰‡ä¸»ä½“è°ƒæ•´caption")
                        print(f"   ğŸ¯ ç›®æ ‡ï¼šç¡®ä¿captionå‡†ç¡®æè¿°å›¾ç‰‡ä¸­çš„ä¸»è¦è§†è§‰å…ƒç´ ")
                        
                        # ç›´æ¥ä½¿ç”¨vision modelåˆ†æå›¾ç‰‡ï¼Œç”ŸæˆåŸºäºå›¾ç‰‡ä¸»ä½“çš„caption
                        new_captions = []
                        for idx, img_path in enumerate(image_paths):
                            if img_path and os.path.exists(img_path):
                                print(f"   ğŸ” åˆ†æå›¾ç‰‡ {idx+1}/{len(image_paths)}...")
                                # ä½¿ç”¨_extract_keywords_for_captionæ–¹æ³•ï¼ŒåŸºäºå›¾ç‰‡å†…å®¹ç”Ÿæˆcaption
                                new_caption = self._extract_keywords_for_caption(img_path, text_content=text)
                                new_captions.append({
                                    "position": f"image_{idx}",
                                    "caption": new_caption
                                })
                                if isinstance(new_caption, dict):
                                    print(f"      ğŸ“¸ å›¾ç‰‡{idx}: {new_caption.get('zh', '')} ({new_caption.get('en', '')})")
                                else:
                                    print(f"      ğŸ“¸ å›¾ç‰‡{idx}: {new_caption}")
                            else:
                                print(f"      âš ï¸  å›¾ç‰‡{idx}ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                        
                        if new_captions:
                            print(f"   âœ… æ ¹æ®å›¾ç‰‡ä¸»ä½“ç”Ÿæˆäº† {len(new_captions)} ä¸ªæ–°caption")
                            should_apply = True
                            suggestion_detail = {
                                "text_changes": [],
                                "image_captions": new_captions
                            }
                        else:
                            print(f"   âš ï¸  æ— æ³•ç”Ÿæˆcaptionï¼Œè·³è¿‡")
                            continue
                    
                    # ğŸ¯ ç¬¬ä¸‰æ¬¡åŠåç»­Reflectionï¼šå…ˆé‡æ–°ç”Ÿæˆç®€çŸ­captionï¼Œå†æ ¹æ®captionåŠéƒ¨åˆ†ä¸Šä¸‹æ–‡ç”Ÿæˆå›¾ç‰‡
                    # iteration == 2: ç¬¬3æ¬¡åæ€ï¼ˆç‰¹å®šç­–ç•¥ï¼‰
                    # iteration >= 3: ç¬¬4æ¬¡åŠä»¥åï¼Œéƒ½ä½¿ç”¨ç¬¬3æ¬¡çš„ç­–ç•¥
                    elif iteration >= 2:
                        iteration_num = iteration + 1
                        print(f"   ğŸ“ ç­–ç•¥ï¼šç¬¬{iteration_num}æ¬¡Reflection - å…ˆç”Ÿæˆç®€çŸ­captionï¼Œå†æ ¹æ®caption+ä¸Šä¸‹æ–‡ç”Ÿæˆå›¾ç‰‡")
                        if iteration > 2:
                            print(f"   â„¹ï¸  ä½¿ç”¨ç¬¬3æ¬¡åæ€çš„ç­–ç•¥ï¼ˆé‡å¤æ‰§è¡Œç›´åˆ°è¾¾åˆ°é˜ˆå€¼ï¼‰")
                        
                        # Step 1: å…ˆæ ¹æ®å½“å‰å›¾ç‰‡ç”Ÿæˆæ–°çš„ç®€çŸ­caption
                        print(f"\n   ğŸ“ Step 1: æ ¹æ®å½“å‰å›¾ç‰‡ç”Ÿæˆæ–°çš„ç®€çŸ­caption...")
                        new_captions = []
                        for idx, img_path in enumerate(image_paths):
                            if img_path and os.path.exists(img_path):
                                print(f"   ğŸ” åˆ†æå›¾ç‰‡ {idx+1}/{len(image_paths)}...")
                                # ä½¿ç”¨_extract_keywords_for_captionæ–¹æ³•ï¼Œç”Ÿæˆç®€æ´çš„caption
                                new_caption = self._extract_keywords_for_caption(img_path, text_content=text)
                                new_captions.append({
                                    "position": f"image_{idx}",
                                    "caption": new_caption
                                })
                                if isinstance(new_caption, dict):
                                    print(f"      ğŸ“¸ å›¾ç‰‡{idx}: {new_caption.get('zh', '')} ({new_caption.get('en', '')})")
                                else:
                                    print(f"      ğŸ“¸ å›¾ç‰‡{idx}: {new_caption}")
                            else:
                                print(f"      âš ï¸  å›¾ç‰‡{idx}ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                        
                        if not new_captions:
                            print(f"   âš ï¸  æ— æ³•ç”Ÿæˆcaptionï¼Œè·³è¿‡")
                            continue
                        
                        print(f"   âœ… ç”Ÿæˆäº† {len(new_captions)} ä¸ªæ–°caption")
                        
                        # Step 2: æ ¹æ®æ–°ç”Ÿæˆçš„captionå’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡é‡æ–°ç”Ÿæˆå›¾ç‰‡
                        print(f"\n   ğŸ¨ Step 2: æ ¹æ®æ–°captionå’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡é‡æ–°ç”Ÿæˆå›¾ç‰‡...")
                        
                        # æå–captionçš„è‹±æ–‡ç‰ˆæœ¬ç”¨äºå›¾ç‰‡ç”Ÿæˆ
                        caption_guidance_list = []
                        for cap_item in new_captions:
                            cap = cap_item.get('caption')
                            if isinstance(cap, dict):
                                caption_en = cap.get('en', '')
                                caption_zh = cap.get('zh', '')
                            else:
                                caption_en = str(cap)
                                caption_zh = str(cap)
                            caption_guidance_list.append({
                                'en': caption_en,
                                'zh': caption_zh
                            })
                        
                        # é‡æ–°ç”Ÿæˆå›¾ç‰‡ï¼ˆæ ¹æ®æ–°captionå’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡ï¼‰
                        new_image_paths = self._regenerate_images_with_new_captions(
                            html_path=current_html_path,
                            text_content=text,
                            user_profile=profile_text,
                            output_dir=output_dir,
                            new_captions=caption_guidance_list,
                            tags=tags,
                            rag_examples=rag_examples
                        )
                        
                        if new_image_paths:
                            # æ›¿æ¢HTMLä¸­çš„å›¾ç‰‡
                            new_html_path = self._replace_images_in_html(
                                html_path=current_html_path,
                                new_image_paths=new_image_paths,
                                iteration=iteration
                            )
                            
                            if new_html_path:
                                current_html_path = new_html_path
                                print(f"   âœ… å›¾ç‰‡é‡æ–°ç”ŸæˆæˆåŠŸ")
                            else:
                                print(f"   âš ï¸  HTMLæ›´æ–°å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£")
                                # ä¸breakï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£
                                continue
                        else:
                            print(f"   âš ï¸  å›¾ç‰‡é‡æ–°ç”Ÿæˆå¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£")
                            # ä¸breakï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£
                            continue
                        
                        # Step 3: åº”ç”¨æ–°ç”Ÿæˆçš„captionï¼ˆåªæœ‰å›¾ç‰‡ç”ŸæˆæˆåŠŸæ‰ä¼šæ‰§è¡Œåˆ°è¿™é‡Œï¼‰
                        should_apply = True
                        suggestion_detail = {
                            "text_changes": [],
                            "image_captions": new_captions
                        }
                    
                    else:
                        # å…¶ä»–iterationï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘
                        print(f"   âš ï¸  æœªçŸ¥çš„iteration: {iteration}ï¼Œè·³è¿‡")
                        continue
                    
                    # æ³¨æ„ï¼šç¬¬ä¸‰æ¬¡reflectionæ—¶ï¼Œshould_applyå’Œsuggestion_detailå·²åœ¨ä¸Šé¢è®¾ç½®
                    if should_apply:
                        print(f"\n   ğŸ”§ åº”ç”¨æ”¹è¿›å»ºè®®...")
                        
                        num_image_captions = len(suggestion_detail.get('image_captions', []))
                        num_text_changes = len(suggestion_detail.get('text_changes', []))
                        
                        print(f"   - å›¾ç‰‡caption: {num_image_captions} æ¡")
                        print(f"   - æ–‡æœ¬ä¿®æ”¹: {num_text_changes} æ¡")
                        
                        if num_image_captions > 0 or num_text_changes > 0:
                            new_html_path = self._apply_reflection_suggestions(
                                current_html_path,
                                suggestion_detail,
                                iteration
                            )
                            
                            if new_html_path:
                                # å¦‚æœå¯ç”¨ä¸¥æ ¼æ¨¡å¼ï¼ŒéªŒè¯æ–°ç‰ˆæœ¬çš„scoreæ˜¯å¦æå‡
                                if self.reflection_strict_mode:
                                    print(f"\n      ğŸ” éªŒè¯æ”¹è¿›æ•ˆæœï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰...")
                                    try:
                                        new_eval_result = evaluate_file(
                                            html_path=new_html_path,
                                            evaluator=self.clip_evaluator,
                                            use_combined=True,
                                            verbose=False
                                        )
                                        new_score = new_eval_result.group_score_mean  # ä½¿ç”¨meanä½œä¸ºè¯„ä¼°æŒ‡æ ‡
                                        score_delta = new_score - groupscore
                                        
                                        print(f"      ğŸ“Š ä¿®æ”¹å‰ (Mean): {groupscore:.4f}")
                                        print(f"      ğŸ“Š ä¿®æ”¹å (Mean): {new_score:.4f}")
                                        print(f"      ğŸ“Š å˜åŒ–: {score_delta:+.4f}")
                                        
                                        if new_score >= groupscore:
                                            current_html_path = new_html_path
                                            final_version = f"v{iteration+1}"
                                            print(f"      âœ… Scoreæå‡æˆ–æŒå¹³ï¼Œæ¥å—ä¿®æ”¹: {Path(new_html_path).name}")
                                        else:
                                            print(f"      âŒ Scoreé™ä½ï¼Œæ‹’ç»ä¿®æ”¹ï¼ˆä¿ç•™v{iteration}ï¼‰")
                                            # ä¿ç•™å¤±è´¥çš„ç‰ˆæœ¬ï¼ˆä¸åˆ é™¤ï¼Œä¾¿äºè°ƒè¯•ï¼‰
                                            # ä½†ä¸æ›´æ–°current_html_pathï¼Œç»§ç»­ä½¿ç”¨ä¸Šä¸€ç‰ˆæœ¬
                                            # ç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£ï¼ˆå¯èƒ½å…¶ä»–å»ºè®®ä¼šæ›´å¥½ï¼‰
                                    except Exception as e:
                                        print(f"      âš ï¸  éªŒè¯å¤±è´¥: {e}ï¼Œé»˜è®¤æ¥å—ä¿®æ”¹")
                                        current_html_path = new_html_path
                                        final_version = f"v{iteration+1}"
                                else:
                                    # éä¸¥æ ¼æ¨¡å¼ï¼šç›´æ¥æ¥å—ä¿®æ”¹
                                    current_html_path = new_html_path
                                    final_version = f"v{iteration+1}"
                                    print(f"   âœ… å·²ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬: {Path(new_html_path).name}")
                            else:
                                print(f"   âš ï¸  åº”ç”¨å»ºè®®å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£")
                                # ä¸breakï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£ï¼ˆå¯èƒ½ä¸‹ä¸€æ¬¡ä¼šæˆåŠŸï¼‰
                                continue
                        else:
                            print(f"   âš ï¸  æ²¡æœ‰å…·ä½“çš„ä¿®æ”¹å†…å®¹ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£")
                            # ä¸breakï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£ï¼ˆå¯èƒ½ä¸‹ä¸€æ¬¡ä¼šæœ‰å†…å®¹ï¼‰
                            continue
                    else:
                        # åªæœ‰åœ¨ç¬¬3æ¬¡åŠä»¥åçš„è¿­ä»£ä¸­ï¼Œå¦‚æœæ²¡æœ‰ä¿®æ”¹å»ºè®®ï¼Œæ‰ç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£
                        # å‰3æ¬¡å¦‚æœæ²¡æœ‰ä¿®æ”¹å»ºè®®ï¼Œå¯èƒ½æ˜¯çœŸçš„æ²¡æœ‰éœ€è¦æ”¹è¿›çš„åœ°æ–¹
                        if iteration >= 2:
                            print(f"   â„¹ï¸  æ— ä¿®æ”¹å»ºè®®ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£ï¼ˆç¬¬{iteration+1}æ¬¡åæ€ï¼‰")
                            continue
                        else:
                            print(f"   â„¹ï¸  æ— ä¿®æ”¹å»ºè®®ï¼Œåœæ­¢reflection")
                            break
                    
                except Exception as e:
                    print(f"\n   âŒ Reflectionè¿‡ç¨‹å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    break
            
            # æ£€æŸ¥æœ€ç»ˆç‰ˆæœ¬æ˜¯å¦å·²è®°å½•åœ¨historyä¸­ï¼ˆå¯èƒ½æœ€åä¸€æ¬¡è¿­ä»£ç”Ÿæˆäº†æ–°ç‰ˆæœ¬ä½†æœªè¯„ä¼°ï¼‰
            final_version_recorded = any(
                record['version'] == final_version 
                for record in reflection_history
            )
            
            if not final_version_recorded and current_html_path:
                # æœ€ç»ˆç‰ˆæœ¬æœªè®°å½•ï¼Œéœ€è¦è¯„ä¼°å¹¶è®°å½•
                print(f"\nğŸ“Š è¯„ä¼°æœ€ç»ˆç‰ˆæœ¬ {final_version} çš„GroupScore...")
                try:
                    final_eval_result = evaluate_file(
                        html_path=current_html_path,
                        evaluator=self.clip_evaluator,
                        use_combined=True,
                        verbose=False
                    )
                    
                    if final_eval_result:
                        final_groupscore = final_eval_result.group_score_mean
                        print(f"   ğŸ“ˆ GroupScore (Mean): {final_groupscore:.4f}")
                        
                        # æ·»åŠ åˆ°history
                        reflection_history.append({
                            "iteration": len(reflection_history) + 1,
                            "version": final_version,
                            "groupscore": final_groupscore,
                            "html_path": current_html_path
                        })
                    else:
                        print(f"   âš ï¸  æœ€ç»ˆç‰ˆæœ¬è¯„ä¼°å¤±è´¥")
                except Exception as e:
                    print(f"   âš ï¸  è¯„ä¼°æœ€ç»ˆç‰ˆæœ¬æ—¶å‡ºé”™: {e}")
            
            # æ‰“å°Reflectionæ€»ç»“
            print(f"\n{'='*80}")
            print(f"ğŸ“‹ Reflectionæ€»ç»“")
            print(f"{'='*80}")
            print(f"æ€»è¿­ä»£æ¬¡æ•°: {len(reflection_history)}")
            print(f"æœ€ç»ˆç‰ˆæœ¬: {final_version}")
            
            if reflection_history:
                print(f"\nGroupScoreå˜åŒ–:")
                for record in reflection_history:
                    status = "âœ… è¾¾æ ‡" if record["groupscore"] >= self.reflection_threshold else "âš ï¸  æœªè¾¾æ ‡"
                    strategy_info = ""
                    if 'strategy' in record:
                        if record['strategy'] == 'image_regeneration':
                            strategy_info = " [å›¾ç‰‡é‡å»º]"
                        elif record['strategy'] == 'image_generation_rescue':
                            strategy_info = " [å›¾ç‰‡è¡¥æ•‘]"
                    if record.get('switched_to_best'):
                        strategy_info += " [åŸºäºæœ€ä½³ç‰ˆæœ¬]"
                    print(f"  {record['version']}: {record['groupscore']:.4f} {status}{strategy_info}")
            
            print(f"\næœ€ç»ˆHTML: {Path(current_html_path).name}")
            print(f"{'='*80}\n")
        
        return {
            "text": text,
            "images": images,
            "links": links,
            "tags": tags,
            "html_post": current_html_path,  # è¿”å›æœ€ç»ˆç‰ˆæœ¬çš„HTMLè·¯å¾„
            "reflection_history": reflection_history,  # è¿”å›reflectionå†å²
            "final_version": final_version
        }