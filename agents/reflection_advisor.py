"""
Reflection Advisor for ITProduct
ä¸ºå°çº¢ä¹¦å›¾æ–‡ç¬”è®°æä¾› AI é©±åŠ¨çš„åæ€å’Œæ”¹è¿›å»ºè®®

è¾“å…¥ï¼š
1. GroupScoreï¼ˆå›¾æ–‡ä¸€è‡´æ€§åˆ†æ•°ï¼‰
2. HTMLè§£æåºåˆ—ï¼ˆæ–‡æœ¬+å›¾ç‰‡+é“¾æ¥çš„æ’å¸ƒï¼‰
3. RAG Top-3 ä¼˜ç§€æ ·ä¾‹

è¾“å‡ºï¼š
- æ˜¯å¦éœ€è¦ä¿®æ”¹çš„åˆ¤æ–­
- æ–‡æœ¬ä¿®æ”¹å»ºè®®
- å›¾ç‰‡Captionå»ºè®®
"""

import os
import json
import base64
import requests
from pathlib import Path
from typing import Dict, List, Optional


class ReflectionAdvisor:
    """AIåæ€é¡¾é—® - åˆ†æå†…å®¹å¹¶æä¾›æ”¹è¿›å»ºè®®"""
    
    def __init__(self):
        # ä½¿ç”¨æœç´¢æ¨¡å‹è¿›è¡Œåæ€ï¼ˆæ”¯æŒè”ç½‘å’Œæ·±åº¦åˆ†æï¼‰
        self.api_key = os.getenv("SEARCH_API_KEY", "sk-dVaSXmTEMBh0Gygx49ResSvaONvErml5QV8McBAGkbPmX2mG")
        self.api_base = os.getenv("SEARCH_BASE_URL", "https://yunwu.ai/v1")
        self.model = os.getenv("SEARCH_MODEL", "gpt-5-all")
        
        # å¤šæ¨¡æ€æ¨¡å‹é…ç½®ï¼ˆç”¨äºå›¾ç‰‡captionç”Ÿæˆï¼‰
        self.vision_model = os.getenv("VISION_MODEL", "claude-sonnet-4-5-20250929")  # Claude 3.5 Sonnetå¤šæ¨¡æ€
        
        if not self.api_key:
            raise ValueError("SEARCH_API_KEY not found in environment variables")
    
    def evaluate_and_suggest(
        self,
        groupscore: float,
        html_sequence: str,
        rag_examples: List[Dict],
        image_paths: List[str] = None,
        threshold: float = 0.65,
        user_profile: Optional[str] = None,
        reflection_iteration: int = 0
    ) -> Dict:
        """
        ç»¼åˆè¯„ä¼°å†…å®¹å¹¶ç”Ÿæˆæ”¹è¿›å»ºè®®
        
        Args:
            groupscore: CLIP è¯„åˆ†ï¼ˆå›¾æ–‡ä¸€è‡´æ€§ï¼‰
            html_sequence: è§£æåçš„HTMLåºåˆ—æ–‡æœ¬
            rag_examples: RAGæ£€ç´¢çš„ä¼˜ç§€æ ·ä¾‹ï¼ˆTop-3ï¼‰
            image_paths: å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç”¨äºå¤šæ¨¡æ€åˆ†æï¼‰
            threshold: GroupScore é˜ˆå€¼ï¼ˆé»˜è®¤0.65ï¼‰
            user_profile: ç”¨æˆ·ç”»åƒï¼ˆå¯é€‰ï¼‰
            
        Returns:
            {
                "should_modify": bool,
                "reason": str,
                "suggestions": {
                    "text_changes": [...],
                    "image_captions": [...]
                },
                "image_analyses": [...],  # å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆçš„å›¾ç‰‡caption
                "overall_assessment": str
            }
        """
        
        print(f"\nğŸ¤– AI Reflection Advisor å¯åŠ¨...")
        print(f"   GroupScore: {groupscore:.4f} (é˜ˆå€¼: {threshold})")
        
        # 1. åˆæ­¥åˆ¤æ–­ï¼šGroupScore æ˜¯å¦ä½äºé˜ˆå€¼
        if groupscore >= threshold:
            print(f"   âœ… GroupScore è¾¾æ ‡ï¼Œæ— éœ€ Reflection")
            return {
                "should_modify": False,
                "reason": f"GroupScore ({groupscore:.4f}) å·²è¾¾åˆ°é˜ˆå€¼ ({threshold})",
                "suggestions": None,
                "overall_assessment": "å†…å®¹è´¨é‡è‰¯å¥½"
            }
        
        print(f"   âš ï¸  GroupScore ä½äºé˜ˆå€¼ï¼Œå¯åŠ¨æ·±åº¦åˆ†æ...")
        
        # 2. åˆ†æå›¾ç‰‡ï¼ˆå¦‚æœæœ‰å›¾ç‰‡ä¸”æä¾›äº†è·¯å¾„ï¼‰
        image_analyses = []
        if image_paths and len(image_paths) > 0:
            try:
                # æå–éƒ¨åˆ†æ–‡æœ¬ä½œä¸ºcontext
                context_text = html_sequence[:500] if html_sequence else ""
                image_analyses = self._analyze_images_with_vision(image_paths, context_text)
            except Exception as e:
                print(f"   âš ï¸  å›¾ç‰‡åˆ†æå¤±è´¥: {e}")
                image_analyses = []
        else:
            print(f"   â„¹ï¸  æ— å›¾ç‰‡éœ€è¦åˆ†æ")
        
        # 3. æ ¼å¼åŒ– RAG æ ·ä¾‹
        examples_text = self._format_rag_examples(rag_examples)
        
        # 4. æ ¼å¼åŒ–å›¾ç‰‡captionï¼ˆå¦‚æœæœ‰ï¼‰
        image_caption_text = ""
        if image_analyses:
            image_caption_text = "\n**å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆçš„å›¾ç‰‡Captionï¼š**\n"
            for caption_item in image_analyses:
                image_caption_text += f"- {caption_item['caption']}\n"
        
        # 5. æ„å»ºè¯„ä¼° Prompt
        evaluation_prompt = self._build_evaluation_prompt(
            groupscore=groupscore,
            html_sequence=html_sequence,
            examples_text=examples_text,
            threshold=threshold,
            user_profile=user_profile,
            image_caption=image_caption_text,
            reflection_iteration=reflection_iteration
        )
        
        # 6. è°ƒç”¨ AI è¿›è¡Œè¯„ä¼°å’Œå»ºè®®
        try:
            reflection_result = self._call_ai_for_reflection(evaluation_prompt)
            
            # 7. è§£æç»“æœ
            parsed_result = self._parse_reflection_result(reflection_result)
            
            # 8. å°†å›¾ç‰‡åˆ†æç»“æœæ•´åˆåˆ°è¿”å›ä¸­
            parsed_result["image_analyses"] = image_analyses
            
            print(f"\nğŸ“‹ AI è¯„ä¼°ç»“æœ:")
            print(f"   æ˜¯å¦éœ€è¦ä¿®æ”¹: {'æ˜¯' if parsed_result['should_modify'] else 'å¦'}")
            print(f"   ç†ç”±: {parsed_result['reason'][:100]}...")
            
            if parsed_result['should_modify'] and parsed_result['suggestions']:
                text_changes = parsed_result['suggestions'].get('text_changes', [])
                captions = parsed_result['suggestions'].get('image_captions', [])
                print(f"   æ–‡æœ¬ä¿®æ”¹å»ºè®®: {len(text_changes)} æ¡")
                print(f"   å›¾ç‰‡Captionå»ºè®®: {len(captions)} æ¡")
            
            return parsed_result
            
        except Exception as e:
            print(f"   âŒ AI è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # é™çº§ç­–ç•¥ï¼šå¦‚æœAIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤šæ¨¡æ€ç”Ÿæˆçš„captionï¼ˆå¦‚æœæœ‰ï¼‰
            fallback_captions = []
            if image_analyses:
                fallback_captions = [
                    {
                        "position": f"image_{caption_item['image_index']}",
                        "caption": caption_item['caption']
                    }
                    for caption_item in image_analyses
                ]
            
            return {
                "should_modify": True,
                "reason": f"GroupScore ({groupscore:.4f}) ä½äºé˜ˆå€¼ï¼ŒAIè¯„ä¼°å¤±è´¥ä½†å»ºè®®ä¿®æ”¹",
                "suggestions": {
                    "text_changes": [
                        {
                            "position": "æ•´ä½“",
                            "issue": "å›¾æ–‡ä¸€è‡´æ€§ä¸è¶³",
                            "suggestion": "å»ºè®®ä¼˜åŒ–æ–‡æ¡ˆä¸å›¾ç‰‡çš„åŒ¹é…åº¦"
                        }
                    ],
                    "image_captions": fallback_captions
                },
                "image_analyses": image_analyses,
                "overall_assessment": "éœ€è¦æ”¹è¿›ï¼ˆAIè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ + å¤šæ¨¡æ€å›¾ç‰‡åˆ†æï¼‰"
            }
    
    def _format_rag_examples(self, rag_examples: List[Dict]) -> str:
        """æ ¼å¼åŒ–RAGæ ·ä¾‹ä¸ºæ–‡æœ¬"""
        if not rag_examples:
            return "ï¼ˆæ— ä¼˜ç§€æ ·ä¾‹å‚è€ƒï¼‰"
        
        formatted = []
        for i, example in enumerate(rag_examples[:3], 1):  # æœ€å¤š3ä¸ª
            content = example.get('content', example.get('full_text', ''))
            similarity = example.get('similarity', 0)
            
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(content) > 500:
                content = content[:500] + "..."
            
            formatted.append(f"ã€ä¼˜ç§€æ ·ä¾‹ {i}ã€‘(ç›¸ä¼¼åº¦: {similarity:.3f})\n{content}")
        
        return "\n\n".join(formatted)
    
    def _build_evaluation_prompt(
        self,
        groupscore: float,
        html_sequence: str,
        examples_text: str,
        threshold: float,
        user_profile: Optional[str],
        image_caption: str = "",
        reflection_iteration: int = 0
    ) -> str:
        """æ„å»ºè¯„ä¼°Prompt"""
        
        # ç”¨æˆ·ç”»åƒéƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
        profile_section = ""
        if user_profile:
            profile_section = f"""
**ç”¨æˆ·ç”»åƒï¼š**
{user_profile[:300]}
"""
        
        prompt = f"""You are a Xiaohongshu content quality expert specializing in image-text semantic consistency optimization. Your primary goal is to MAXIMIZE the GroupScore (CLIP-based image-text matching) through strategic improvements.

## âš ï¸ CRITICAL: Caption-Only Evaluation

**GroupScore is calculated ONLY using image captions and images** (not full text paragraphs).
- CLIP model is trained on simple image-text pairs, not long paragraphs
- Only caption text is used for similarity calculation
- Full paragraph text is NOT used in GroupScore calculation

## Evaluation Materials

**1. Image-Text Consistency Score (GroupScore): {groupscore:.4f} / 1.0**
   - Threshold: {threshold}
   - Status: {'âš ï¸ Below threshold' if groupscore < threshold else 'âœ… Meets threshold'}
   - Explanation: CLIP model evaluates semantic similarity between images and captions ONLY
   - **Goal**: Achieve 0.1+ improvement (target score: {groupscore + 0.1:.4f})

**2. Current Content Sequence:**
```
{html_sequence}
```

**3. High-Quality Examples (RAG Top-3):**
```
{examples_text}
```

{image_caption}

{profile_section}

## Evaluation Task

Analyze this post and provide strategic improvements to BOOST the GroupScore by 0.1+.

**You must base your evaluation on FOUR key sources:**
1. **GroupScore** ({groupscore:.4f}): Current image-caption consistency score - identify what's missing
2. **HTML Sequence**: Current content structure and caption-image relationships
3. **RAG Top-3 Examples**: High-quality reference posts to learn from
4. **User Profile**: User preferences and interests to ensure content alignment

### Evaluation Focus (Priority Order):

1. **Caption-Image Alignment** (MOST CRITICAL for score improvement)
   - Do captions accurately describe what's visible in the images?
   - Are key objects, colors, scenes, or actions from images mentioned in captions?
   - Missing keywords: What visual elements are shown but not mentioned in captions?
   - Caption quality: Are captions concise (2-6 characters) with concrete nouns?

2. **Caption Precision**
   - Are captions too vague (e.g., "ç¾é£Ÿ") or too specific (e.g., "æŠ¹èŒ¶èŒ‰è‰æ‹¿é“é…ç™½è‰²é™¶ç“·æ¯")?
   - Should captions be more focused on core visual elements?

3. **Content Quality** (Secondary)
   - Is the text natural and engaging?
   - Any redundant or off-topic sections?

### Improvement Strategies (for maximum score boost):

{f"**ğŸ¯ REFLECTION ITERATION {reflection_iteration + 1} STRATEGY:**" if reflection_iteration < 3 else ""}
{f'''
**Iteration 1 (First Reflection) - Image Regeneration Focus:**
- **Primary Goal**: Regenerate images to better match context + captions
- **Key References**: Context text, current captions (PRIMARY), RAG-top3 examples, user_profile (SECONDARY)
- **Image Modification Suggestions**: Provide specific guidance on what visual elements should be changed/added/removed
- **Focus**: Improve image-caption alignment by regenerating images that better match the captions and context
- **Do NOT modify captions** - only suggest image changes

1. **Image Modification Suggestions** (PRIMARY for Iteration 1):
   - Analyze current images vs. captions and context
   - Identify visual elements that are missing or misaligned
   - Suggest specific changes: objects to add/remove, colors, composition, style, etc.
   - Reference: Use context text and captions as PRIMARY guide, RAG examples and user_profile as SECONDARY reference
   - Format: Provide detailed image modification guidance for each image

2. **Image Captions** (NOT modified in Iteration 1):
   - Keep existing captions unchanged
   - Only analyze if captions are appropriate for the suggested image changes
''' if reflection_iteration == 0 else ""}
{f'''
**Iteration 2 (Second Reflection) - Caption Adjustment Based on Images:**
- **Primary Goal**: Adjust captions based on the actual images to ensure captions describe the MAIN SUBJECT in images
- **Key References**: Current images (from Iteration 1) - analyze what's ACTUALLY visible
- **Focus**: Ensure captions accurately describe the PRIMARY visual element/subject in each image
- **Do NOT regenerate images** - only modify captions based on image analysis

1. **Image Captions** (PRIMARY for Iteration 2):
   - Analyze each image to identify the MAIN SUBJECT (what occupies the center/foreground)
   - Update captions to match the PRIMARY visual element in each image
   - Caption format: **å›¾1: [2-6 characters with core visual concept]**
   - Must describe the MAIN SUBJECT visible in the image, not secondary elements
   - Keep it concise: 2-6 characters, focus on the main visual theme
   - Example: If image shows "latte coffee" prominently â†’ caption should be "æ‹¿é“å’–å•¡" / "latte coffee"
''' if reflection_iteration == 1 else ""}
{f'''
**Iteration 3 (Third Reflection) - Caption-First Reconstruction:**
- **Primary Goal**: First generate concise captions, then regenerate images based on captions + partial context
- **Workflow**: 
  1. Generate new concise captions based on current images
  2. Regenerate images with captions as PRIMARY subject, partial context as SECONDARY reference
- **Key References**: 
  - Step 1: Current images (to extract main subjects for new captions)
  - Step 2: New captions (PRIMARY) + Partial context text (SECONDARY) + RAG-top3 + user_profile

1. **Image Captions** (Step 1 - Generate first):
   - Analyze current images to extract main subjects
   - Generate concise captions (2-6 characters) that describe the main visual element
   - Caption format: **å›¾1: [2-6 characters with core visual concept]**

2. **Image Regeneration** (Step 2 - Based on new captions):
   - Regenerate images with NEW captions as the PRIMARY and DOMINANT subject
   - Use partial context text only for style, atmosphere, and background (SECONDARY)
   - Composition: Caption subject (70%+) > Context elements (30%-)
   - This ensures maximum CLIP score matching
''' if reflection_iteration == 2 else ""}
{f'''
1. **Image Captions** (ONLY way to improve GroupScore):
   - Multimodal model has generated captions (see above)
   - You can refine these captions to be more semantically precise
   - Caption format: **å›¾1: [2-6 characters with core visual concept]**
   - Must include: Core concept, key objects (e.g., "æŠ¹èŒ¶é¥®å“", "å’–å•¡åº—", "å¥½ç‰©åˆ†äº«")
   - Keep it concise: 2-6 characters, focus on the main visual theme
''' if reflection_iteration >= 3 else ""}

## Return Format (MUST be valid JSON)

{f'''
```json
{{
  "should_modify": true/false,
  "reason": "Brief reason (1-2 sentences, focus on image-caption alignment impact)",
  "suggestions": {{
    "text_changes": [],
    "image_modifications": [
      {{
        "position": "image_0",
        "current_issue": "What's wrong with current image",
        "suggested_changes": "Detailed guidance on visual elements to change/add/remove (objects, colors, composition, style, etc.)",
        "reference_priority": "Context + Caption (PRIMARY), RAG-top3 + user_profile (SECONDARY)"
      }}
    ],
    "image_captions": []  // Keep empty for Iteration 1
  }},
  "overall_assessment": "Overall evaluation (1-2 sentences, estimate expected score improvement)"
}}
```
''' if reflection_iteration == 0 else ""}
{f'''
```json
{{
  "should_modify": true/false,
  "reason": "Brief reason (1-2 sentences, focus on caption-image alignment impact)",
  "suggestions": {{
    "text_changes": [],
    "image_captions": [
      {{
        "position": "image_0",
        "caption": "å›¾1: [2-6 characters, core visual concept, e.g., 'æŠ¹èŒ¶é¥®å“', 'å’–å•¡åº—', 'å¥½ç‰©åˆ†äº«']"
      }}
    ]
  }},
  "overall_assessment": "Overall evaluation (1-2 sentences, estimate expected score improvement)"
}}
```
''' if reflection_iteration == 1 else ""}
{f'''
```json
{{
  "should_modify": true/false,
  "reason": "Brief reason (1-2 sentences, focus on full reconstruction impact)",
  "suggestions": {{
    "text_changes": [],
    "image_modifications": [
      {{
        "position": "image_0",
        "current_issue": "What's wrong with current image",
        "suggested_changes": "Detailed guidance on visual elements to change/add/remove"
      }}
    ],
    "image_captions": [
      {{
        "position": "image_0",
        "caption": "å›¾1: [2-6 characters, core visual concept, e.g., 'æŠ¹èŒ¶é¥®å“', 'å’–å•¡åº—', 'å¥½ç‰©åˆ†äº«']"
      }}
    ]
  }},
  "overall_assessment": "Overall evaluation (1-2 sentences, estimate expected score improvement)"
}}
```
''' if reflection_iteration == 2 else ""}
{f'''
```json
{{
  "should_modify": true/false,
  "reason": "Brief reason (1-2 sentences, focus on caption-image alignment impact)",
  "suggestions": {{
    "text_changes": [],
    "image_captions": [
      {{
        "position": "image_0",
        "caption": "å›¾1: [2-6 characters, core visual concept, e.g., 'æŠ¹èŒ¶é¥®å“', 'å’–å•¡åº—', 'å¥½ç‰©åˆ†äº«']"
      }}
    ]
  }},
  "overall_assessment": "Overall evaluation (1-2 sentences, estimate expected score improvement)"
}}
```
''' if reflection_iteration >= 3 else ""}

**Critical Guidelines**:
- âš ï¸ **Caption-Only**: GroupScore is calculated ONLY using captions, NOT full text paragraphs
- If GroupScore â‰¥ {threshold}: Return should_modify: false ONLY if captions are already optimal
- If GroupScore < {threshold}: MUST provide caption improvements targeting 0.1+ score boost
- Captions: Must be concise (2-6 characters), focus on core visual concept
- Examples of good captions: "æŠ¹èŒ¶é¥®å“", "å’–å•¡åº—", "å¥½ç‰©åˆ†äº«", "æ—…è¡Œvlog"
- Examples of bad captions: "ç¾é£Ÿ" (too vague), "æŠ¹èŒ¶èŒ‰è‰æ‹¿é“é…ç™½è‰²é™¶ç“·æ¯" (too long)
- Expected impact: Prioritize caption changes with highest expected score improvement
- Output ONLY JSON, no other text

Begin evaluation:

"""
        
        return prompt
    
    def _call_ai_for_reflection(self, prompt: str) -> str:
        """è°ƒç”¨AIè¿›è¡Œåæ€"""
        
        try:
            response = requests.post(
                f"{self.api_base}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.3,  # è¾ƒä½æ¸©åº¦ï¼Œæ›´ç²¾ç¡®çš„åˆ†æ
                    "top_p": 0.9
                },
                timeout=60
            )
            
            if response.status_code != 200:
                raise Exception(f"API error {response.status_code}: {response.text[:200]}")
            
            result = response.json()
            
            if "choices" not in result:
                raise Exception(f"Unexpected response format: {result}")
            
            content = result["choices"][0]["message"]["content"].strip()
            
            return content
            
        except Exception as e:
            print(f"âš ï¸ AIè°ƒç”¨å¼‚å¸¸: {e}")
            raise
    
    def _analyze_images_with_vision(self, image_paths: List[str], context_text: str = "") -> List[Dict]:
        """
        ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ä¸ºå›¾ç‰‡ç”Ÿæˆcaption
        
        Args:
            image_paths: å›¾ç‰‡ç»å¯¹è·¯å¾„åˆ—è¡¨
            context_text: æ–‡ç« ä¸Šä¸‹æ–‡ï¼ˆå¸®åŠ©ç†è§£å›¾ç‰‡åœ¨æ–‡ç« ä¸­çš„ä½œç”¨ï¼‰
            
        Returns:
            [
                {
                    "image_index": 0,
                    "caption": "å›¾1: ç®€çŸ­æè¿°ï¼ˆ10-20å­—ï¼‰"
                },
                ...
            ]
        """
        print(f"\nğŸ” ä½¿ç”¨ {self.vision_model} ä¸º {len(image_paths)} å¼ å›¾ç‰‡ç”Ÿæˆcaption...")
        
        image_captions = []
        
        for idx, img_path in enumerate(image_paths):
            if not img_path or not Path(img_path).exists():
                print(f"   âš ï¸  è·³è¿‡å›¾ç‰‡ {idx}: æ–‡ä»¶ä¸å­˜åœ¨")
                image_captions.append({
                    "image_index": idx,
                    "caption": f"å›¾{idx+1}: [å›¾ç‰‡ç¼ºå¤±]"
                })
                continue
            
            try:
                # è¯»å–å›¾ç‰‡å¹¶ç¼–ç ä¸ºbase64
                with open(img_path, 'rb') as f:
                    img_data = f.read()
                
                # åˆ¤æ–­å›¾ç‰‡æ ¼å¼
                ext = Path(img_path).suffix.lower()
                mime_type = {
                    '.png': 'image/png',
                    '.jpg': 'image/jpeg',
                    '.jpeg': 'image/jpeg',
                    '.webp': 'image/webp',
                    '.gif': 'image/gif'
                }.get(ext, 'image/jpeg')
                
                img_base64 = base64.b64encode(img_data).decode('utf-8')
                
                # æ„å»ºvision promptï¼ˆç¬¬ä¸€äººç§°è§†è§’ï¼Œåƒä½œè€…ç»™å›¾ç‰‡åŠ çš„è¯´æ˜ï¼‰
                vision_prompt = f"""ä½ æ˜¯å°çº¢ä¹¦åšä¸»ï¼Œæ­£åœ¨ä¸ºè‡ªå·±çš„ç…§ç‰‡æ·»åŠ è¯´æ˜æ–‡å­—ã€‚ç”¨ç¬¬ä¸€äººç§°è§†è§’æè¿°è¿™å¼ å›¾ç‰‡ï¼Œåƒæ˜¯åœ¨ç»™æœ‹å‹ä»‹ç»ã€‚

**èƒŒæ™¯**ï¼šè¿™æ˜¯ä½ å°çº¢ä¹¦ç¬”è®°ä¸­çš„ç¬¬{idx+1}å¼ å›¾ç‰‡ã€‚

**ä½ çš„ç¬”è®°å†…å®¹ç‰‡æ®µ**ï¼š
{context_text[:300]}...

**Captionè¦æ±‚**ï¼š
1. **ç¬¬ä¸€äººç§°è§†è§’**ï¼šç”¨"æˆ‘"ã€"è¿™é‡Œ"ã€"ä»Šå¤©"ç­‰ï¼Œåƒæ˜¯ä½ åœ¨åˆ†äº«è‡ªå·±çš„ç…§ç‰‡
2. **å…·ä½“çš„è§†è§‰ç»†èŠ‚**ï¼šæåˆ°å…·ä½“çš„ç‰©å“ã€é¢œè‰²ã€åœºæ™¯ï¼ˆä¸è¦åªè¯´"å¾ˆç¾"ã€"å¾ˆæ£’"ï¼‰
3. **è´´è¿‘æ–‡ç« ä¸»é¢˜**ï¼šå‘¼åº”æ–‡ç« å†…å®¹ï¼Œä½“ç°ä½ åœ¨åšä»€ä¹ˆã€çœ‹åˆ°ä»€ä¹ˆ
4. **è‡ªç„¶å£è¯­åŒ–**ï¼šåƒå¯¹æœ‹å‹è¯´è¯ä¸€æ ·ï¼Œå¯ä»¥ç”¨"è¶…çº§"ã€"çœŸçš„"ç­‰è¯­æ°”è¯
5. **10-40å­—**ï¼šç®€çŸ­ä½†æœ‰ä¿¡æ¯é‡

**Captionæ ¼å¼**ï¼šå›¾{idx+1}: [ä½ çš„ç¬¬ä¸€äººç§°æè¿°]

**å¥½çš„ç¤ºä¾‹ï¼ˆç¬¬ä¸€äººç§°ã€å…·ä½“ã€æœ‰ç”»é¢æ„Ÿï¼‰**ï¼š
- å›¾1: æˆ‘ç«™åœ¨èŒ¶å¡ç›æ¹–çš„å¤©ç©ºä¹‹é•œå‰ï¼Œç™½è‰²ç›æ™¶åœ°é¢å€’æ˜ ç€è“å¤©ç™½äº‘ï¼Œè¶…ç¾çš„
- å›¾2: è¿™å°±æ˜¯æ•¦ç…Œè«é«˜çªŸçš„ä¹å±‚æ¥¼ï¼çº¢è‰²æœ¨è´¨å»ºç­‘é‡Œçš„å£ç”»è‰²å½©çœŸçš„å¥½é²œè‰³
- å›¾3: ä»Šå¤©åœ¨å±±å§†ä¹°äº†ä¸€å¤§è½¦ï¼Œå·¦è¾¹æ˜¯æ¸…æ´ç”¨å“ï¼Œå³è¾¹å†·æŸœè£…æ»¡äº†å†·å†»é£Ÿå“å’Œè‚‰
- å›¾4: è¿™æ¯æ‹¿é“çš„å¿ƒå½¢æ‹‰èŠ±æˆ‘çˆ±äº†ï¼æœ¨è´¨æ¡Œå­é…ç™½è‰²é™¶ç“·æ¯è¶…æœ‰è´¨æ„Ÿ
- å›¾5: ç©¿ä¸Šè¿™ä»¶æ–°ä¹°çš„æ¯›è¡£è¯•æ‹ä¸€å¼ ï¼Œç±³è‰²çš„å¾ˆæ¸©æŸ”æ˜¾ç™½

**ä¸å¥½çš„ç¤ºä¾‹ï¼ˆå¤ªå®¢è§‚ã€ç¬¬ä¸‰äººç§°ã€ç©ºæ³›ï¼‰**ï¼š
- å›¾1: èŒ¶å¡ç›æ¹–å¤©ç©ºä¹‹é•œå€’æ˜ äº‘å±‚ï¼ˆâŒ å¤ªå®¢è§‚ï¼Œåƒå¯¼æ¸¸è§£è¯´ï¼‰
- å›¾2: ç¾ä¸½çš„é£æ™¯ï¼ˆâŒ å¤ªç©ºæ³›ï¼Œæ²¡æœ‰å…·ä½“ä¿¡æ¯ï¼‰
- å›¾3: ä¸€ä¸ªäººåœ¨è´­ç‰©ï¼ˆâŒ æ²¡æœ‰ç»†èŠ‚ï¼Œä¸å¤Ÿç”ŸåŠ¨ï¼‰

**åªè¾“å‡ºcaptionæ–‡å­—**ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
                
                # è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼Œåªç”Ÿæˆcaptionï¼‰
                response = requests.post(
                    f"{self.api_base}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.vision_model,
                        "messages": [
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": vision_prompt
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:{mime_type};base64,{img_base64}"
                                        }
                                    }
                                ]
                            }
                        ],
                        "temperature": 0.3,  # Lower temperature for more precise, consistent captions
                        "max_tokens": 250  # Captionæ›´è¯¦ç»†ï¼Œéœ€è¦æ›´å¤štokens
                    },
                    timeout=60
                )
                
                if response.status_code != 200:
                    raise Exception(f"API error {response.status_code}: {response.text[:200]}")
                
                result = response.json()
                caption = result["choices"][0]["message"]["content"].strip()
                
                # æ¸…ç†å¯èƒ½çš„markdownæˆ–å¤šä½™æ ¼å¼
                caption = caption.replace("```", "").replace("**", "").strip()
                
                # å¦‚æœæ²¡æœ‰"å›¾X:"å‰ç¼€ï¼Œè‡ªåŠ¨æ·»åŠ 
                if not caption.startswith(f"å›¾{idx+1}"):
                    caption = f"å›¾{idx+1}: {caption}"
                
                image_captions.append({
                    "image_index": idx,
                    "caption": caption
                })
                
                print(f"   âœ… å›¾ç‰‡ {idx}: {caption}")
                
            except Exception as e:
                print(f"   âŒ å›¾ç‰‡ {idx} captionç”Ÿæˆå¤±è´¥: {e}")
                image_captions.append({
                    "image_index": idx,
                    "caption": f"å›¾{idx+1}: [ç”Ÿæˆå¤±è´¥]"
                })
        
        return image_captions
    
    def _parse_reflection_result(self, ai_response: str) -> Dict:
        """è§£æAIè¿”å›çš„JSONç»“æœ"""
        
        # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
        import re
        content = ai_response
        
        # ç§»é™¤ ```json å’Œ ```
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        # å°è¯•è§£æJSON
        try:
            result = json.loads(content)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ["should_modify", "reason", "overall_assessment"]
            for field in required_fields:
                if field not in result:
                    raise ValueError(f"Missing required field: {field}")
            
            # å¦‚æœéœ€è¦ä¿®æ”¹ï¼ŒéªŒè¯ suggestions
            if result["should_modify"]:
                if "suggestions" not in result:
                    result["suggestions"] = {
                        "text_changes": [],
                        "image_captions": []
                    }
                
                # ç¡®ä¿ suggestions æœ‰æ­£ç¡®çš„ç»“æ„
                if "text_changes" not in result["suggestions"]:
                    result["suggestions"]["text_changes"] = []
                if "image_captions" not in result["suggestions"]:
                    result["suggestions"]["image_captions"] = []
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å†…å®¹: {content[:500]}...")
            
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
            try:
                # ç§»é™¤å¯èƒ½çš„BOMå’Œæ§åˆ¶å­—ç¬¦
                content = content.encode('utf-8').decode('utf-8-sig').strip()
                content = ''.join(char for char in content if ord(char) >= 32 or char in '\n\r\t')
                
                # ä¿®å¤ trailing commas
                content = re.sub(r',\s*}', '}', content)
                content = re.sub(r',\s*]', ']', content)
                
                result = json.loads(content)
                print("âœ… JSONä¿®å¤æˆåŠŸ")
                return result
                
            except Exception as e2:
                print(f"âŒ JSONä¿®å¤å¤±è´¥: {e2}")
                raise ValueError(f"æ— æ³•è§£æAIè¿”å›çš„JSON: {e}")


def test_reflection_advisor():
    """æµ‹è¯• ReflectionAdvisor"""
    import sys
    from html_parser_for_reflection import HTMLParserForReflection
    
    print("="*80)
    print("ğŸ§ª æµ‹è¯• Reflection Advisor")
    print("="*80)
    
    # åˆ›å»ºå®ä¾‹
    try:
        advisor = ReflectionAdvisor()
    except ValueError as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿è®¾ç½®äº† SEARCH_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    # æµ‹è¯•æ–‡ä»¶
    html_path = "generated_it/29_5726fe0950c4b401f76283be/image_text.html"
    
    if not os.path.exists(html_path):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {html_path}")
        return
    
    print(f"\nğŸ“ æµ‹è¯•æ–‡ä»¶: {html_path}")
    
    # 1. è§£æHTML
    print("\n1ï¸âƒ£ è§£æHTML...")
    parser = HTMLParserForReflection()
    parse_result = parser.parse_html_to_sequence(html_path)
    html_sequence = parse_result["sequence_text"]
    image_paths = parse_result["image_paths"]
    
    print(f"   âœ… è§£æå®Œæˆ: {parse_result['stats']['texts']} æ–‡æœ¬, {parse_result['stats']['images']} å›¾ç‰‡, {parse_result['stats']['links']} é“¾æ¥")
    print(f"   ğŸ“¸ å›¾ç‰‡æ–‡ä»¶: {image_paths}")
    
    # 2. æ¨¡æ‹Ÿ GroupScore
    groupscore = 0.5923  # ä½äºé˜ˆå€¼çš„åˆ†æ•°
    print(f"\n2ï¸âƒ£ GroupScore: {groupscore:.4f}")
    
    # 3. æ¨¡æ‹Ÿ RAG æ ·ä¾‹
    rag_examples = [
        {
            "content": "å§å¦¹ä»¬ï¼ä»Šå¤©è¦åˆ†äº«ä¸€ä¸ªè¶…æ£’çš„å’–å•¡åº—â˜•ï¸\n\nğŸ“åº—åï¼šManner Coffee\nåœ°å€ï¼šæœé˜³å¤§æ‚¦åŸ2æ¥¼\n\nç¯å¢ƒè¶…çº§å¥½ï¼Œé€‚åˆæ‹ç…§ğŸ“· æ‹›ç‰Œæ‹¿é“çœŸçš„ç»äº†ï¼",
            "similarity": 0.85
        },
        {
            "content": "å‘¨æœ«å»äº†è¶Ÿè¥¿æ¹–ï¼Œé£æ™¯ç¾åˆ°çª’æ¯ğŸŒ…\n\nå¿…æ‰“å¡æ™¯ç‚¹ï¼š\n1. æ–­æ¡¥æ®‹é›ª\n2. é›·å³°å¡”\n3. è‹å ¤æ˜¥æ™“\n\nè®°å¾—ç©¿æ±‰æœæ‹ç…§ï¼Œè¶…å‡ºç‰‡ï¼",
            "similarity": 0.78
        }
    ]
    
    print(f"\n3ï¸âƒ£ RAGæ ·ä¾‹: {len(rag_examples)} ä¸ª")
    
    # 4. è°ƒç”¨ Reflection
    print(f"\n4ï¸âƒ£ è°ƒç”¨ AI Reflection...")
    print("-"*80)
    
    result = advisor.evaluate_and_suggest(
        groupscore=groupscore,
        html_sequence=html_sequence,
        rag_examples=rag_examples,
        image_paths=image_paths,
        threshold=0.65,
        user_profile="çƒ­çˆ±æ—…è¡Œå’Œç¾é£Ÿçš„å¹´è½»å¥³æ€§ç”¨æˆ·"
    )
    
    # 5. æ˜¾ç¤ºç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š Reflection ç»“æœ")
    print("="*80)
    
    print(f"\næ˜¯å¦éœ€è¦ä¿®æ”¹: {'âœ… æ˜¯' if result['should_modify'] else 'âŒ å¦'}")
    print(f"ç†ç”±: {result['reason']}")
    print(f"æ•´ä½“è¯„ä»·: {result['overall_assessment']}")
    
    if result['should_modify'] and result['suggestions']:
        suggestions = result['suggestions']
        
        if suggestions.get('text_changes'):
            print(f"\nğŸ“ æ–‡æœ¬ä¿®æ”¹å»ºè®® ({len(suggestions['text_changes'])} æ¡):")
            for i, change in enumerate(suggestions['text_changes'], 1):
                print(f"   {i}. ä½ç½®: {change.get('position', 'N/A')}")
                print(f"      é—®é¢˜: {change.get('issue', 'N/A')}")
                print(f"      å»ºè®®: {change.get('suggestion', 'N/A')}")
        
        if suggestions.get('image_captions'):
            print(f"\nğŸ–¼ï¸  å›¾ç‰‡Captionå»ºè®® ({len(suggestions['image_captions'])} æ¡):")
            for i, caption in enumerate(suggestions['image_captions'], 1):
                print(f"   {i}. {caption.get('position', 'N/A')}: {caption.get('caption', 'N/A')}")
    
    # æ˜¾ç¤ºå¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆçš„caption
    if result.get('image_analyses'):
        print(f"\nğŸ” å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆçš„Caption ({len(result['image_analyses'])} å¼ ):")
        for caption_item in result['image_analyses']:
            print(f"   ğŸ“¸ {caption_item['caption']}")
    
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("="*80)
    
    # ä¿å­˜ç»“æœ
    output_path = "reflection_test_result.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


# if __name__ == "__main__":
#     test_reflection_advisor()

